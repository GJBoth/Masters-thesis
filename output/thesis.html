<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <meta http-equiv="Content-Style-Type" content="text/css" />
        <meta name="generator" content="pandoc" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
                        <title>01_title_page</title>
        <style type="text/css">code{white-space: pre;}</style>
                                            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
                            <style>
            body {
                font-family: Georgia;
                max-width: 800px;
                margin: 0 auto;
                line-height: 30px;
                font-size: 18px;
                padding-left: 350px;
                padding-right: 50px;
                color: #111;
            }
            
            h1, h2, h3, h4, h5, h6 {
                font-family: Arial;
            }
            
            h1 {
                padding-top: 200px;
                line-height: 50px;
            }
            h2 {
                padding-top: 30px;
            }
            h3 {
                padding-top: 20px;
            }
            h4 {
                padding-top: 10px;
            }
            p {
                text-align: justify;
            }
            p a {
                word-wrap: break-word;
                white-space: pre;
            }
            code {
                word-wrap: break-word;
            }
            blockquote {
                border-left: 3px solid #eee;
                margin-left: 20px;
                padding-left: 20px;
            }
            
            ::selection {
                background-color: #E4E4E4;
            }
            
            table {
                width: 100%;
            }
            table caption {
                font-weight: bold;
            }
            table tr {
                padding: 0;
                margin: 0;
                background-color: #f0f0f0;
            }
            table tr.even {
                background-color: #fafafa;
            }
            table td {
                margin: 0;
                padding: 3px 5px;
            }
            
            p span.added {
                color: green;
                background-color: #FFF3C5;
            }
            p span.removed {
                color: red;
                background-color: #FFF3C5;
            }
            
            #title-page {
                padding: 80px 0;
            }
            
            #TOC {
                position: fixed;
                left: 0;
                top: 0;
                overflow-y: scroll;
                height: 100%;
                background: #fafafa;
                max-width: 300px;
                font-family: Arial;
                font-size: 15px;
                line-height: 30px;
            }
            ::-webkit-scrollbar {
                width: 8px;
            }
            ::-webkit-scrollbar-track {
                background-color: #ECECEC;
            }
            ::-webkit-scrollbar-thumb {
                background-color: #B0B0B0;
                border-radius: 8px;
            }
            #TOC > ul {
                padding-right: 10px;
            }
            #TOC ul {
                list-style: none;
                padding-left: 20px;
            }
            #TOC ul li a {
                text-decoration: none;
                color: #364149;
                text-overflow: ellipsis;
                display: block;
                white-space: nowrap;
                overflow: hidden;
            }
            #TOC ul li a:hover {
                color: #008cff;
            }
            
            .figure {
                text-align: center;
            }
            .figure p {
                text-align: center;
                font-style: italic;
            }
            .figure img {
                  width: 100%;
            }
            </style>
                <script src="js/jquery.js"></script>
        <script src="js/diff.js"></script>
        <script src="js/main.js"></script>
    </head>
    <body>
                <!--
                -->
        <div id="title-page">
            <h1>This is the title of the thesis</h1>
            <h2>Firstname Surname</h2>
        </div>
                    <div id="TOC">
                <ul>
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
                <li><a href="#quantitative-work-on-the-golgi-so-far"><span class="toc-section-number">1.1</span> Quantitative work on the Golgi so far</a></li>
                <li><a href="#rush-system"><span class="toc-section-number">1.2</span> RUSH system</a><ul>
                <li><a href="#man-ii"><span class="toc-section-number">1.2.1</span> Man II</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#introduction-1"><span class="toc-section-number">2</span> Introduction</a></li>
                <li><a href="#data-processing-pipeline"><span class="toc-section-number">3</span> Data processing pipeline</a><ul>
                <li><a href="#step-1-segmentation"><span class="toc-section-number">3.1</span> Step 1: Segmentation</a><ul>
                <li><a href="#voronoi-diagram"><span class="toc-section-number">3.1.1</span> Voronoi diagram</a></li>
                <li><a href="#intensity"><span class="toc-section-number">3.1.2</span> Intensity</a></li>
                </ul></li>
                <li><a href="#step-2---denoising"><span class="toc-section-number">3.2</span> Step 2 - Denoising</a><ul>
                <li><a href="#wavelet-filter">Wavelet filter</a></li>
                <li><a href="#proper-orthogonal-decomposition">Proper orthogonal decomposition</a></li>
                <li><a href="#wavinpod">WavinPOD</a></li>
                </ul></li>
                <li><a href="#step-3---derivatives"><span class="toc-section-number">3.3</span> Step 3 - Derivatives</a></li>
                <li><a href="#step-4---fitting"><span class="toc-section-number">3.4</span> Step 4 - Fitting</a></li>
                </ul></li>
                <li><a href="#results-data-analysis"><span class="toc-section-number">4</span> Results data analysis</a><ul>
                <li><a href="#general-analysis"><span class="toc-section-number">4.1</span> General analysis</a></li>
                <li><a href="#analysis-of-time-derivatives"><span class="toc-section-number">4.2</span> Analysis of time derivatives</a></li>
                <li><a href="#analysis-of-ls-fit"><span class="toc-section-number">4.3</span> Analysis of LS-fit</a><ul>
                <li><a href="#diffusion"><span class="toc-section-number">4.3.1</span> Diffusion</a></li>
                <li><a href="#advection"><span class="toc-section-number">4.3.2</span> Advection</a></li>
                </ul></li>
                <li><a href="#analysis-of-constrained-ls-fit"><span class="toc-section-number">4.4</span> Analysis of constrained LS-fit</a></li>
                </ul></li>
                <li><a href="#physics-informed-neural-networks"><span class="toc-section-number">5</span> Physics Informed Neural Networks</a><ul>
                <li><a href="#neural-networks"><span class="toc-section-number">5.1</span> Neural Networks</a><ul>
                <li><a href="#architecture"><span class="toc-section-number">5.1.1</span> Architecture</a></li>
                <li><a href="#training"><span class="toc-section-number">5.1.2</span> Training</a></li>
                </ul></li>
                <li><a href="#physics-informed-neural-networks-1"><span class="toc-section-number">5.2</span> Physics Informed Neural Networks</a><ul>
                <li><a href="#the-concept"><span class="toc-section-number">5.2.1</span> The concept</a></li>
                <li><a href="#fitting-and-prediction">Fitting and prediction</a></li>
                <li><a href="#pinns-in-practice"><span class="toc-section-number">5.2.2</span> PINNs in practice</a></li>
                </ul></li>
                <li><a href="#conclusion"><span class="toc-section-number">5.3</span> Conclusion</a><ul>
                <li><a href="#weak-points-and-how-to-improve"><span class="toc-section-number">5.3.1</span> Weak points and how to improve</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#conclusion-1"><span class="toc-section-number">6</span> Conclusion</a></li>
                <li><a href="#appendix-1-some-extra-stuff">Appendix 1: Some extra stuff</a></li>
                <li><a href="#references">References</a></li>
                </ul>
            </div>
                                <!-- 
This is the Latex-heavy title page. 
People outside UCL may want to remove the header logo 
and add the centred logo
-->

<h1 id="abstract" class="unnumbered">Abstract</h1>
<!-- This is the abstract -->
<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam et turpis gravida, lacinia ante sit amet, sollicitudin erat. Aliquam efficitur vehicula leo sed condimentum. Phasellus lobortis eros vitae rutrum egestas. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Donec at urna imperdiet, vulputate orci eu, sollicitudin leo. Donec nec dui sagittis, malesuada erat eget, vulputate tellus. Nam ullamcorper efficitur iaculis. Mauris eu vehicula nibh. In lectus turpis, tempor at felis a, egestas fermentum massa.</p>




<h1 id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<h2 id="quantitative-work-on-the-golgi-so-far"><span class="header-section-number">1.1</span> Quantitative work on the Golgi so far</h2>
<h2 id="rush-system"><span class="header-section-number">1.2</span> RUSH system</h2>
<h3 id="man-ii"><span class="header-section-number">1.2.1</span> Man II</h3>
<h1 id="introduction-1"><span class="header-section-number">2</span> Introduction</h1>
<h1 id="data-processing-pipeline"><span class="header-section-number">3</span> Data processing pipeline</h1>
<p>In this chapter I present the work done on processing the rush movies. Several preprocssing steps hav been undertaken to improve the quality of the fit, and we present all here. Roughly, we can divide the process in four steps:</p>
<ol type="1">
<li>Segmentation and creation of masks</li>
<li>Denoising of movies</li>
<li>Calculation of spatial and temporal derivatives</li>
<li>The actual fitting</li>
</ol>
<p>Below we describe each step separately.</p>
<h2 id="step-1-segmentation"><span class="header-section-number">3.1</span> Step 1: Segmentation</h2>
<p>The images obtained from the rush experiments often contain multiple cells. Furthermore, we can also segment the image into roughly three different types: 1) the background, where nothing of interest happens. No cells are present here, 2) the cytoplasm, which is the area where we want to fit our model and 3) the Golgi itself, where we do not necessarily want to fit. Unfortunately, no bright field images were available, making segmentation significantly harder, as no clear cell boundary can be observed. Further complicating the story is the large dynamic range of the movies due to the fluorescence concentrating in the Golgi. The following procedures we present have been developed to deal with these problems. Note that they are empirical methods, i.e. there’s no theoretical background as to why they <em>should</em> work. However, in practice they do and I haven’t found any other method which was able to.</p>
<h3 id="voronoi-diagram"><span class="header-section-number">3.1.1</span> Voronoi diagram</h3>
<p>This method is based on a technique called Voronoi tesselation and doesn’t depend on ny measure of the intensity. It was developed after noting that since the cargo is spread throughout the ER in the first few frames and as the ER is roughly circumnuclear, we can use this to determine the centre of the cell (roughly). Voronoi tesselation then allows us to divide the frame into areas with just one point per area, i.e. one cell per area (theoretically). More precise, given <span class="math inline">\(n\)</span> coordinates, voronoi tesselation divides the given area into <span class="math inline">\(n\)</span> pieces, where every point in a piece is closest to one coordinate. In practice this means for us that each point in a cell area is closest to its the given cell centre. Figure <strong>ref</strong> shows this. Each calculated cell centre is a red point and the lines depict the borders between each voronoi cell. Assuming the cells don’t move too much, they don’t cross the cells and thus we apply the voronoi diagram calculated in the first few frames to the entire movie.</p>
<h3 id="intensity"><span class="header-section-number">3.1.2</span> Intensity</h3>
<p>For the fitting however we wish to make a slightly better approach than a voronoi diagram. As stated, we can’t find the exact delineation of the cell, but looking at the intensity, we can see an ‘area’ of interest, separating background from the cell. Since the Golgi is quite bright in het last 200 or so frames, we consider only the intensity for the Golgi, while for the cytoplasm we consider both the intensity and its time derivative. Thus we have two analog but different processes. For the Golgi we do the following:</p>
<ol type="1">
<li>Renormalize the concentration <span class="math inline">\(C\)</span> between 0 and 1.</li>
<li>Sum all frames. One then obtains an image such as figure <strong>ref</strong> <span class="math display">\[x
\sum_{frames}C(x,y,t)
\]</span></li>
<li>This image is thresholded, either through an otsu threshold or a manual one, until the mask roughly matches what we want. Note that extreme precision isn’t required, since we just want the rough area. THis results in figure <strong>ref</strong></li>
</ol>
<p>For the cytoplasm we follow the same procedure only now we take the log of sum of the product of the intensity and its time derivative:</p>
<p><span class="math display">\[
\log\left(\sum_{frames}C(x,y,t)\cdot\partial_tC(x,y,t)\right)
\]</span> We thus obtain a complete mask for the movie as shown in figure <strong>ref</strong></p>
<h2 id="step-2---denoising"><span class="header-section-number">3.2</span> Step 2 - Denoising</h2>
<p>In order to accurately calculate the derivatives and generally improve the quality of fitting, we wish to denoise and smooth the obtained movies. Denoising and smoothing is a subject about which many books have been written and there are hundreds of approaches. One oft-used technique is to Fourier transform the signal, cutoff all coefficients above a cutoff frequency and retransform back into the real domain. Next, a Savitzky-Golay filter can be used to finally smooth the result. However, a big issue with all these methods is their non locality. Since our movies have different scales, this is a big problem. Furthermore, they often smooth out sharp peaks. After evaluating several methods, I have settled on a relatively new method presented in <strong>ref</strong>.</p>
<p>The so-called WavinPOD method combines two well-known filtering techniques, known as wavelet filtering and Proper Orthogonal Decomposition. Below we explain each separately. Our explanation is adapted from <strong>ref</strong> and <strong>ref</strong>.</p>
<h3 id="wavelet-filter" class="unnumbered">Wavelet filter</h3>
<p>A wavelet filter is not really the appropriate name, as its more of a transform.</p>
<p><strong>More about wavelet transform</strong></p>
<h3 id="proper-orthogonal-decomposition" class="unnumbered">Proper orthogonal decomposition</h3>
<p>Proper orthogonal decomposition is a technique similar to what is known as Principal component Analysis in statistics and falls into the general category of model reduction techniques. It’s often used in flow problems to extract coherent structures from turbulent flows. Simply put, in POD we wish to express data as a sum of orthogonal functions, where the basis is determined from the data, i.e. we don’t impose something as a fourier basis, etc..</p>
<p><span class="math display">\[
f(x,t)=\sum_k g(x)h(t)
\]</span></p>
<p>Basically we’re trying to find the eigenfunctions of the data. Full explanation in paper <strong>ref</strong> Each eigenfunction comes with a eigenvalue, which can be interpreted as the energy of a mode. The higher the eigenvalue, the more important the mode is to the entire signal. To reduce the dimension of the data, we pick a cutoff <span class="math inline">\(k_{max}\)</span> and only use the modes <span class="math inline">\(k&lt;k_{max}\)</span>. Several methods are used to determine the cutoff, but often used is the knee-technique. When we plot the log10 spectrum in figure <strong>ref</strong>, one can often observe a ‘knee’. Usually this point is taken as the cutoff.</p>
<h3 id="wavinpod" class="unnumbered">WavinPOD</h3>
<p>WavinPOD combines these two techniques in the following way. First, we decompose our problem with a POD transformation. This yields a set of temporal and spatial modes. We select the most energetic modes and wavelet filter these, before transforming them back to the real domain. As shown in <strong>ref</strong>, combining these techniques has an advantage over others.</p>
<p>In our case, we select the number of modes to be used by hand (30 in the case of MANII) and apply a 3-level db4 wavelet. We use a slightly higher than necessary level to increase smoothness. In the figure below we show the result for both a pixel in time and one time snapshot. Note that the result is significantly smoother, but that smaller details have been preserved.</p>
<h2 id="step-3---derivatives"><span class="header-section-number">3.3</span> Step 3 - Derivatives</h2>
<p>Taking spatial and temporal derivatives of these images is not an entirely trivial operation due to the discreteness of the system. More specifically, taking numerical derivatives of data is extremely hard to do properly and becomes even harder in the presence of noise. Next to basic finite difference methods, one can for example use a linear-least-squares fitted polynomial, smoothing spline or a so-called tikhonov-regularizer <strong>ref needed</strong>. Each method comes with its strengths and weaknesses, but one particularly nasty thing for our context is that they don’t scale well to higher dimensions and quickly become computationally expensive.</p>
<p>Another issue related to discretization is the size of the grid w.r.t. the size of the features. To see this, we plot a 2D-gaussian with <span class="math inline">\(\sigma=1\)</span> in figure <strong>ref</strong>.</p>
<p>As expected, the derivative is normal to the isolines of the object. Now consider the discretized version of the object. Taking the naive spatial derivate w.r.t. to each direction means only considering a single row or column of and taking the derivative in that direction. Figure <strong>ref</strong> shows the result of this operation. An artifact is clearly visible: instead of a nice uniform derivative, we see a ‘cross’. This effect is a cause of the discretization grid being too large for some smaller, often bright, objects.</p>
<p>To remedy this, one can for example artificially upscale the grid, interpolate the values inbetween, and take the derivates from this grid. This is not ideal however, since the upscaling requires a large amount of memory and is computationally expensive. Another solution which is common in image processing is applying a <em>kernel operator</em>. The advantage of a kernel operator is that it is extremely computationally cheap, as it involves convolving the original picture with a differentiation kernel. The differentiation kernel is an approximate version of a finite difference scheme. We use and show here the Sobel filter, which is the most commonly used one.</p>
<p>In a simple finite central difference scheme, we set</p>
<p><span class="math display">\[
\frac{dx}{dt}\approx\frac{x_{i+1}-x_{i-1}}{2h}
\]</span></p>
<p>where <span class="math inline">\(h\)</span> is the distance between two points. In terms of a kernel operator, this would look like (the <span class="math inline">\(h\)</span> drops out as the distance in terms of pixels is 1):</p>
<p><span class="math display">\[\frac{1}{2}\cdot
\begin{bmatrix}
1 &amp; 0 &amp; -1
\end{bmatrix}
\]</span></p>
<p>And applying it by convoluting it to a matrix gives the x-derivative:</p>
<p><span class="math display">\[
\partial_xA\approx A*\begin{bmatrix}
1 &amp; 0 &amp; -1
\end{bmatrix}
\]</span></p>
<p>and analogous for the y-direction. However, as we’ve seen, looking at just a single row introduces cross-like artifacts. To remedy this, we wish to include diagonal pixels as well. However, the distance between the diagonal pixels and the center pixel is not 1 but <span class="math inline">\(\sqrt{2}\)</span> and furthermore we need to decompose is it into <span class="math inline">\(\hat{x}\)</span> and <span class="math inline">\(\hat{y}\)</span>, introducing another factor <span class="math inline">\(\sqrt{2}\)</span>. Thus, one obtains the classis <span class="math inline">\(3\times3\)</span> Sobel filter <strong>ref</strong>:</p>
<p><span class="math display">\[
\mathbf G_x=\frac{1}{8}\cdot
\begin{bmatrix}
1 &amp; 0 &amp; -1\\
2 &amp; 0 &amp; -2\\
1 &amp; 0 &amp; -1
\end{bmatrix}
\mathbf G_y=\frac{1}{8}\cdot
\begin{bmatrix}
1 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; -2 &amp; -1
\end{bmatrix}
\]</span></p>
<p>Although not extremely accurate, the Sobel filter seems to do the tricks for us. Several other versions such as Scharr or Prewitt exist, offering several benefits such as rotational symmetry, but we have not pursued these. They just change the coefficients. Although we have shown a <span class="math inline">\(3\times3\)</span> filter here, the filter can take into account higher order schemes such as a <span class="math inline">\(5\times5\)</span> or <span class="math inline">\(7\times7\)</span>. The major benefit of the spatial derivatives as a convolution operator is its computational efficiency: convolutional operations are performed parallel and are extremely fast.</p>
<p>For the time derivative, we apply a second order accurate central derivative scheme, while for the spatial derivatives (both first and second order) we apply the <span class="math inline">\(5\times5\)</span> Sobel filter. We analyze these in the next chapter .</p>
<h2 id="step-4---fitting"><span class="header-section-number">3.4</span> Step 4 - Fitting</h2>
<p>Now that we have gathered all our data we can use it to fit. We construct a model (advection-diffusion) and then use a ‘simple’ least-squares fit to obtain an estimate of the diffusion and advection coefficients. We note two extra issues. First, a diffusion coefficient defined positive, i.e. a negative diffusion coefficient is unphysical. We thus make two different fits in the next chapter as a check: one with unconstrained variables, and one where we force <span class="math inline">\(D&gt;0\)</span>.</p>
<p>Secondly, it’s highly plausible that the diffusion coefficient and advection are position and time dependent. One could construct the full model for this and obtain both the coefficients and their derivatives, but its highly unlikely that this will lead to consistent results and it would still need to happen locally. We thus perform a ‘moving-window-fit’, where we set the width of the time and position window around a central pixel and assume that the diffusion and velocity are smooth and constant enough in that window to ensure a decent fit. In this, it’s quite similar to a technique known in computer vision as optical flow.</p>
<h1 id="results-data-analysis"><span class="header-section-number">4</span> Results data analysis</h1>
<p>Here we present the results from our analysis on the RUSH experiments. We only show the results of MANII because this is the only thing we studied.</p>
<h2 id="general-analysis"><span class="header-section-number">4.1</span> General analysis</h2>
<h2 id="analysis-of-time-derivatives"><span class="header-section-number">4.2</span> Analysis of time derivatives</h2>
<h2 id="analysis-of-ls-fit"><span class="header-section-number">4.3</span> Analysis of LS-fit</h2>
<h3 id="diffusion"><span class="header-section-number">4.3.1</span> Diffusion</h3>
<h3 id="advection"><span class="header-section-number">4.3.2</span> Advection</h3>
<h2 id="analysis-of-constrained-ls-fit"><span class="header-section-number">4.4</span> Analysis of constrained LS-fit</h2>
<h1 id="physics-informed-neural-networks"><span class="header-section-number">5</span> Physics Informed Neural Networks</h1>
<p>In the previous chapters we showed the difficulties in fitting a model in the form of a partial differential equation to spatio-temporal data. The method we developed was a classical numerical approach, separating the problem into several substeps such as denoising, smoothing and numerical differentiating. In the last few years machine learning has been slowly making its way into physics. Very recently, a technique generally referred to as Physics Informed Neural Networks (PINNs) have shown great promise as both tools for simulation and model fitting (<span class="citation" data-cites="karpatne_physics-guided_2017">1</span>, <span class="citation" data-cites="sharma_weakly-supervised_2018">2</span>, <span class="citation" data-cites="pun_physically-informed_2018">3</span>, <span class="citation" data-cites="raissi_physics_2017">4</span>, <span class="citation" data-cites="raissi_physics_2017-1">5</span> ). In this chapter, I will evaluate the use of this te</p>
<p>the difficulty in finding a general fit for a model to spatiotemporal data. Very recently, a set of papers introduced a new technique called Physics Informed neural networks. The set of papers show very powerful and promising results. I’ve evaluated this technique for use in fitting our model to the RUSH data. Since for many Neural networks are a new technique, this chapter is also a gentle introduction into Neural networks themself. We have three parts:</p>
<ul>
<li>Neural Networks</li>
<li>Physics Informed Neural networks</li>
<li>Conclusion</li>
</ul>
<p>In neural networks we introduce the ideas and some mathematics behind neural networks in Physics informed neural networks we showcase the concept using a simple toy model and in conclusion we present the evaluation of the technique. Since Neural networks are completely new concept to most, we use a light tone.</p>
<h2 id="neural-networks"><span class="header-section-number">5.1</span> Neural Networks</h2>
<p>Neural networks. Inspired by biological neural networks, but not the same!!! Started with the perceptron in the early 60’s, but only one layer so nothing cool. Back propagarion rediscovered in the 80’s, now recognized how to efficiently train a network, but we needed the advancements in the late 00’s in GPU’s for large scale NN. Ever since great advancements in engineering and slowly starting to seep into science now as well. Two main flavours: supervised and non-supervised. In non-supervised we dont tell the goal to the network, in supervised we do. We’ll only focus on the last one. We start with some simple introduction to architecture and how to train them<span class="citation" data-cites="raissi_physics_2017"><sup>4</sup></span>.</p>
<h3 id="architecture"><span class="header-section-number">5.1.1</span> Architecture</h3>
<p>The basic building block of each type of neural network is the same: the neuron. Inspired, but not the same as a biological neural network, the neuron basically has several inputs and transforms them into a single output. This roughly a two step process. Immediatly going to matrix notation, given an input vector <span class="math inline">\(\mathbf{x}\)</span>, the neuron multiplies the input vector by a weight matrix and adding a bias. This gives us the <em>weighted input</em> z:</p>
<p><span id="eq:weighted_input"><span class="math display">\[
z = W\mathbf{x}+b
\qquad(1)\]</span></span></p>
<p>The weighted input eq. 1 is then transformed by the neuron <em>activation function <span class="math inline">\(\sigma\)</span></em> to give the output of the neuron <span class="math inline">\(a\)</span>, also known as the activation:</p>
<p><span class="math display">\[
a = \sigma(z) = \sigma(W\mathbf{x}+b)
\]</span></p>
<p>The role of the activation function is to introduce non-linearity into the system. Many research is ongoing into different activation, but one of the most used is the <span class="math inline">\(tanh(x)\)</span> function, due to its bounded output between -1 and 1 and easy derivative. Several other functions with more favourable properties such as lower computational cost are available as well.</p>
<p>Multiple neurons working in parallle constitute a layer, while multiple layers in series forms a neural network. We always have an input and input layer, and the layers inbetween are known as hidden layers. Networks with more than 1 hidden layer are known as deep neural networks. In the simplest neural network, all neurons in a layer are connected to all the neurons in the next layer. Such a network is shown in figure… We can then rewrite function … in a matrix form. We state <span class="math inline">\(a^l\)</span> is the activation of layer <span class="math inline">\(l\)</span>:</p>
<p><span class="math display">\[
a^l = \sigma(z^l) = \sigma(W^la^{l-1}+b^l)
\]</span></p>
<p>The weights <span class="math inline">\(W^l\)</span> are now the ‘strength’ of each neurons layer to the next. It turns out that such a network is what is known as a <em>universal function approximator</em>, meaning that with enough layers and neurons, a NN is able to approximate <strong>any continous function</strong>. Now that we’ve set up the network, we turn to training it.</p>
<h3 id="training"><span class="header-section-number">5.1.2</span> Training</h3>
<p>As the name machine learning implies, we teach a machine to perform a certain task, i.e. contrary to normal algorithms, we do not tell the machine how to do something. In the case of supervised learning, we have a set of labeled data. This means that we have some inputs which we know should lead to a desired output. The task of training then falls to adjusting the weights and biases untill we get the desired output. A measure to relate how far the given output is from the desired output is given by the <em>cost function</em>. Many different cost functions are available, each one useful for a specific task, but one of the most basic and simple ones is the mean squared error (MSE):</p>
<p><span class="math display">\[
cost_{MSE} = \frac{1}{2n}\sum_n|y-y_{pred}|^2
\]</span> where <span class="math inline">\(n\)</span> represents the sum over each training sample. Training the network thus becomes minimizing the cost function with respect to weight and bias. In general this a problem with local minima, but a solution may be found using gradient descent techniques.</p>
<h4 id="gradient-descent" class="unnumbered">Gradient descent</h4>
<p>Consider a function <span class="math inline">\(f(\mathbf{x})\)</span>, we which we wish to minimize with respect to <span class="math inline">\(\mathbf{x}\)</span>. One starts with an estimate of <span class="math inline">\(\mathbf{x}_0\)</span>, which we iteratively refine</p>
<p><span class="math display">\[
\mathbf{x}_{n+1} = \mathbf{x}_{n}-\gamma\nabla f(\mathbf{x}_n)
\]</span></p>
<p>untill the gradient vanishes. This position is where f is minimized w.r.t <span class="math inline">\(\mathbf{x}\)</span>.<span class="math inline">\(\gamma\)</span> is known as the learning rate and sets the step rate. More advanced techniques can set a variable learning rate etc, but the basis remains similar.</p>
<h4 id="back-propagation-and-automatic-differentiation" class="unnumbered">Back propagation and automatic differentiation</h4>
<p>In the case of neural networks we wish to minimize the cost w.r.t. to each weight <span class="math inline">\(w^l_{jk}\)</span> and bias <span class="math inline">\(b^l_j\)</span>. To do this computationally is not trivial, and most of the NN field uses one algorithm: back propagation. We give a short introduction below. We want to know basically two different things:</p>
<p><span class="math display">\[
\frac{\partial C}{\partial w^l_{jk}}, 
\frac{\partial C}{\partial b^l_{j}}
\]</span> the change of the cost w.r.t to each weight and bias. Let’s define the error neuron <span class="math inline">\(j\)</span> in layer <span class="math inline">\(l\)</span> as <span class="math inline">\(\delta^l_j=\partial C/\partial z^l_j\)</span>. We can rewrite this using the chain rule as:</p>
<p><span class="math display">\[
\delta^l_j = \sum_k \frac{\partial C}{\partial a^l_{jk}}\frac{\partial a^l_{jk}}{\partial z^l_{j}} 
\]</span></p>
<p>However, the second term is always zero except when <span class="math inline">\(j=k\)</span>, so we drop the sum. We also see that the second term is equal to <span class="math inline">\(\sigma&#39;(z^l_j)\)</span>. For the last layer <span class="math inline">\(L\)</span>, the first term turns unto the derivative of the cost function, finally giving us:</p>
<p><span class="math display">\[
\delta^L_j =  |a^L_j-y_j|\sigma&#39;(z^L_j)
\]</span></p>
<p>This expression gives us the error in the output layer in terms of its weighted input. This in turn is a function of previous inputs and errors and we thus need to find an expression relating the error in layer <span class="math inline">\(l\)</span> with the error in an layer <span class="math inline">\(l+1\)</span>. Since we have an expression for the error in the last layer, we calculate the errors going down the layers (from <span class="math inline">\(L\)</span> to <span class="math inline">\(0\)</span>), hence the name backpropagation. Again using the chain rule gives: <span class="math display">\[
\delta^l_j = \sum_k \frac{\partial C}{\partial z^{l+1}_{jk}}\frac{\partial z^{l+1}_{jk}}{\partial z^l_{j}} = \sum_k \delta^{l+1}_k\frac{\partial z^{l+1}_{jk}}{\partial z^l_{j}}
\]</span> Using the definitions of <span class="math inline">\(z^l_{jk}\)</span>, we finally after substitution obtain:</p>
<p><span class="math display">\[
\delta^l_j = \sum_k\delta^{l+1}_kw^{l+1}_{kj}\sigma&#39;(z^l_j)
\]</span> Using these two equations, we can calculate the error in C due to each neuron. Finally, to calculate the <span class="math inline">\(\partial C/\partial w^l_{jk}\)</span> and<br />
<span class="math inline">\(\partial C/\partial b^l_{j}\)</span>, we relate these to the error, again via the chain rule:</p>
<p><span class="math display">\[
\frac{\partial C}{\partial b^l_{j}}\frac{\partial b^l_{j}}{\partial z^l_{j}}=\frac{\partial C}{\partial b^l_{j}}=\delta^l_j
\]</span> <span class="math display">\[
\sum_k\frac{\partial C}{\partial w^l_{jk}}\frac{\partial w^l_{jk}}{\partial z^l_{j}}=\delta^l_j\to \frac{\partial C}{\partial w^l_{jk}}=a^{l-1}_{j}\delta^l_j
\]</span> These four equations together make up the the backpropagation algorithm. The complete optimization of the network goes as follows: 1. Complete a forward pass, i.e., calculate the expected outcomes with the current weights and biases. 2. Calculate the error and back propagate it to obtain the gradients in the weights and biases. 3. Adjust the weights using optimizer (e.g. gradient descent) 4. Return to step 1 and redo the cycle untill convergence.</p>
<p>Officialy, back propagation is a special case of a technique known as automatic differentiation, which is third type of differentiation, next to numeric and symbolic. It also for machine precision calculation of derivatives by writing it as a chain of simple operations combined with the chain rule, similar to backpropagation. Note that:</p>
<p><span class="math display">\[
\delta^0_j = \frac{\partial C}{\partial x_j}\frac{\partial x_j}{\partial z^0_j}
\]</span> so that: <span class="math display">\[
\frac{\partial C}{\partial x_j} = a^0_j \delta^0_j 
\]</span> Thus when learning, we also have access to high precision derivatives with regard to each coordinate!</p>
<h2 id="physics-informed-neural-networks-1"><span class="header-section-number">5.2</span> Physics Informed Neural Networks</h2>
<p>On the face of things, physics and neural networks seem to satisfy two completely different goals. Whereas physics tries to build (simplified) models, neural networks try to learn a general modelless mapping from the inputs to the outputs. However, recently some approaches have emerged which fuse these seemingly opposite goals in order to do two different things:</p>
<ol type="1">
<li>Use a neural network to simulate/numerically solve equations.</li>
<li>Use a neural network to ‘fit’ a model to spatiotemporal data and even infer a coefficient field.</li>
</ol>
<p>Initial results have shown very promising: using neural networks to numerically solve models doesn’t require any advanced meshing and carefull handling of shocks, whereas the ability to infer coefficient fields from spatiotemporal data hasn’t been shown at all to my knowledge.</p>
<h3 id="the-concept"><span class="header-section-number">5.2.1</span> The concept</h3>
<p>Consider a set of 1+1D spatiotemporal data, consisting of some property <span class="math inline">\(u_i\)</span> at coordinates <span class="math inline">\((x_i,t_i)\)</span>. As stated before, a neural network learns the mapping <span class="math inline">\(x,t\to u\)</span> because it is a universal function approximator through minimizing the mean squared error:</p>
<p><span class="math display">\[
MSE = \sum_i(u_i^{in}-u^{pred}_i)^2
\]</span></p>
<p>Now assume that we know that <span class="math inline">\(u(x,t)\)</span> is governed by some process which can be modeled as a partial differential equation. We can write (almost) every PDE as:</p>
<p><span class="math display">\[
\partial_t u = f(1, u, u_x, u_xx, u^2, ...)
\]</span></p>
<p>where <span class="math inline">\(f\)</span> is some sort of function of <span class="math inline">\(u\)</span> or its spatial derivatives. Now we rewrite is as:</p>
<p><span class="math display">\[
g = -\partial_t u + f(1, u, u_x, u_xx, u^2, ...)
\]</span></p>
<p>To satisfy the model, the function <span class="math inline">\(g\)</span> always has to go to zero. The idea behind Physics Informed Neural Networks (PINNs) is that we can include this function as an extra cost to be minimized:</p>
<p><span class="math display">\[
cost = \sum_i(u_i^{in}-u^{pred}_i)^2 + \sum_ig_i^2 = MSE + PI
\]</span></p>
<p>Since to satisfy the model we need <span class="math inline">\(g\to0\)</span>, by adding our second ‘Physic-informed’ term, we effectively penalize solutions not satisfying the physics we put in: our new term acts as a regularizer. More concretely, this term has two effects:</p>
<ol type="1">
<li><strong>It prevents overfitting:</strong> Neural Networks can be prone to overfitting. This term prevents that by penalizing variations not described by the physics.</li>
<li><strong>It makes the NN more data-efficient:</strong> we can get good results with not much data.</li>
<li><strong>It allows fitting and prediction:</strong> we can fit and predict based to the terms in <span class="math inline">\(f\)</span>.</li>
</ol>
<p>The first point is rather technical and interesting for more in-depth, so we’ll leave it for now. The second point is very interesting from an experimental point of view. By presupposing some structure in the data in the form of a model, we need significantly less data to get the same result. It’s also here that we see the no free lunch theorem: a price has to be paid. By encoding a model into the neural network, we lose the freedom of the neural network to map <span class="math inline">\(x,t\)</span> <span class="math inline">\(u\)</span> using any function. For us physicists however, this is actually a blessing but for applications where equationless modeling is usefull this is terrible. Note that we also circumvent the issue of numerical derivatives, since we can use the network provided automatic derivatives at machine precision. This combination provides the power of PINNs - a one-two punch consisting of physics regularization and automatic derivatives.</p>
<h3 id="fitting-and-prediction" class="unnumbered">Fitting and prediction</h3>
<p>Point three is however where PINNs really shine. How can the extra term we’ve included have these effects? To see this, consider the classics physics exercise of calculating the trajectory of a launched object. In this case we know the function <span class="math inline">\(g\)</span>, i.e. <span class="math inline">\(\ddot{y}=-g\)</span>. A classical numerical solver would take small steps in time, updating the position and speed of the object each step according to <span class="math inline">\(g\)</span>. A PINN however uses a completely different approach. Given the initial state of the neural network, it calculates a first trajectory but then keeps adjusting the weights of the network until the cost is minimized, i.e. <span class="math inline">\(g\)</span> goes to zero everywhere. In other words: the Neural network keeps launching the object and adjusting its internal weights until the physics are satisified at every step of the trajectory. The classical numerical approach goes for one correct try; the neural network just keeps trying until it converges on the correct solution. This approach allows us at the same time to circumvent complex discretization schemes and issues such as solutions blowing up. Such an approach is possible because of the backpropagation algorithm, which allows us to calculate the part of each neuron in the total cost and in which direction to change the weights and biases to minimize the cost.</p>
<p>Interestingly, we can also use this framework to fit models to spatiotemporal data by letting the coefficent of each term be a variable w.r.t to which we minimize too. More concretely, where before the cost was a function of the weights and biases, <span class="math inline">\(cost=f(w^l_{jk}, b^l_k)\)</span>, we now let it be a function of the coefficients <span class="math inline">\(\lambda\)</span> of the PDE as well: <span class="math inline">\(cost=f(w^l_{jk}, b^l_k, \lambda)\)</span>. This is shown for all kinds of systems in the papers of <strong>[ref]</strong>. In theory however, it should also be possible to infer coefficient <em>fields</em>, both spatially and temporally varying. We can achieve this by a multi-output Neural network. Instead of outputting just <span class="math inline">\(c\)</span>, we also output the coefficient at that spatiotemporal point, as shown in figure <strong>[ref]</strong>. We investigate this claim in the next section.</p>
<h3 id="pinns-in-practice"><span class="header-section-number">5.2.2</span> PINNs in practice</h3>
<p>In this section we wish to evaluate the use of PINN’s for our RUSH data. We start with a toy problem: a simple diffusive process. This has already been shown in the papers by Raissi, but it’s just to show the reader. We then show the similar problem but with two different diffusion constants. This has not been shown by Raissi and we show here thats is possible to infer a coefficient field from the data using PINNS.</p>
<p>We use the following toy problem: a 1D box, started with initial condition:</p>
<p><span class="math display">\[
c(x, 0) = e^{-\frac{(x-0.5)^2}{0.02}}
\]</span></p>
<p>and a diffusive process:</p>
<p><span class="math display">\[
\frac{\partial c}{\partial t} = \nabla \cdot[D(x)\nabla c]
\]</span></p>
<p>on the spatial domain (0,1) with boundary conditions in equation eq. 2</p>
<p><span id="eq:boundvalue"><span class="math display">\[
c(0,t) = c(1,t) = 0
\qquad(2)\]</span></span></p>
<p>i.e. perectly absorbing boundary conditions. We used mathematica to solve these equations. The code is the appendix.</p>
<h4 id="constant-d"><span class="header-section-number">5.2.2.1</span> Constant D</h4>
<h4 id="noiseless" class="unnumbered">Noiseless</h4>
<p>Now consider the problem with a constant diffusion of <span class="math inline">\(D_0 = 0.01\)</span>. We simulate the data on a domain <span class="math inline">\(x:[0,1]\)</span> and <span class="math inline">\(t:[0,0.5]\)</span> we use a spatial resolution of 0.01, giving the number of points 101 by 51, giving a total number of data points of 5151. Since the fitting is the training, we do not need to separate the data in a training and validation set. Figure ref shows the input data and the what the network predicts. After training, the network predicts (almost exactly) what we want it to and the average error is less than 1. Ofcourse, more interesting is the inferred diffusion parameter. With a value of 0.10034, the error is roughly 0.3. This is very good, but ofcourse our data is without noise. Note however that even still is extremely good and that in their papers Raissi shows far more complex equations such as the complex Schrodinger one. Also note that the error is mostly located at areas with low signal. This is a consistent problem and must be taken into account. Powerful as they are, neural networks seem to struggle with this fig. <strong>¿fig:Dfield?</strong> .</p>
<figure>
<img src="source/figures/png/error_constantD.png" alt="Figure 1: Left panel: something. Right panel: something else" id="fig:error_constantD" /><figcaption>Figure 1: Left panel: something. Right panel: something else</figcaption>
</figure>
<p>Looks really nice.</p>
<h4 id="noisy" class="unnumbered">Noisy</h4>
<p>fig. <strong>¿fig:Dfield?</strong> It becomes more interesting once we add noise. We take exactly the same problem, but now add 5% gaussian distributed white noise (e.g. <span class="math inline">\(0.05std(c)\)</span>) and let the neural network do it’s thing again. The network is now doing two things at once: it’s <em>denoising</em> the data by stating that the underlying data is of a certain model while finding the optimal model parameters. Again figure <strong>ref</strong> shows our original data and the fit.</p>
<p>The data correctly infers the ground truth. The inferred diffusion coefficient is 0.10017, an error of <span class="math inline">\(0.17\)</span>. I just want to state that this is almost ridiculous. We have roughly 5000 points with 5 error and the network is able to infer the coefficent within 1. We also havent optimized the network in any way: it’s the most basic full connected layers with tanh activation function and we just used enough layers and neurons. It’s also a very general technique: it works for whatever PDE and data multidimensional data. We also fit the model directly to the data; circumventing the need to know boundary conditions, initial conditions etc… We can now proceed to more advanced setup.</p>
<figure>
<img src="source/figures/png/error_constantD_noisy.png" alt="Figure 2: Left panel: something. Right panel: something else" id="fig:error_constantD_noisy" /><figcaption>Figure 2: Left panel: something. Right panel: something else</figcaption>
</figure>
<h4 id="varying-d"><span class="header-section-number">5.2.2.2</span> Varying D</h4>
<p>As stated, it should be possible to infer varying coefficient fields. Instead of using a single output network, we implement of two output neural network; outputting both the coefficient and the concentration. Note that this is slightly different from what is done in this paper <strong>ref</strong>. They infer the pressure field, but this is a separate added term; its not multiplied by a spatially varying term.</p>
<h4 id="non-varying" class="unnumbered">Non-varying</h4>
<p>We first demonstrate this using the same data as before: so although we allow a spatially varying field, the underlying data only has a single diffusion coefficient. The coefficient is then learned through training the network and ‘corrected’ by PI-loss function. Figure <strong>ref</strong> shows our results.</p>
<p>test reference<span class="citation" data-cites="raissi_physics_2017"><sup>4</sup></span></p>
<p>and another one</p>
<h4 id="varying" class="unnumbered">Varying</h4>
<h4 id="real-cell"><span class="header-section-number">5.2.2.3</span> Real cell</h4>
<h2 id="conclusion"><span class="header-section-number">5.3</span> Conclusion</h2>
<h3 id="weak-points-and-how-to-improve"><span class="header-section-number">5.3.1</span> Weak points and how to improve</h3>
<h1 id="conclusion-1"><span class="header-section-number">6</span> Conclusion</h1>
<h1 id="appendix-1-some-extra-stuff" class="unnumbered">Appendix 1: Some extra stuff</h1>
<!-- 
This could be a list of papers by the author for example 
-->
<p>Add appendix 1 here. Vivamus hendrerit rhoncus interdum. Sed ullamcorper et augue at porta. Suspendisse facilisis imperdiet urna, eu pellentesque purus suscipit in. Integer dignissim mattis ex aliquam blandit. Curabitur lobortis quam varius turpis ultrices egestas.</p>

<!-- 
Do not edit this page.

References are automatically generated from the BibTex file (References.bib)

...which you should create using your reference manager.
-->
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-karpatne_physics-guided_2017">
<p>1. Karpatne, A., Watkins, W., Read, J. &amp; Kumar, V. Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling. <em>arXiv:1710.11431 [physics, stat]</em> (2017).</p>
</div>
<div id="ref-sharma_weakly-supervised_2018">
<p>2. Sharma, R., Farimani, A. B., Gomes, J., Eastman, P. &amp; Pande, V. Weakly-Supervised Deep Learning of Heat Transport via Physics Informed Loss. <em>arXiv:1807.11374 [cs, stat]</em> (2018).</p>
</div>
<div id="ref-pun_physically-informed_2018">
<p>3. Pun, G. P. P., Batra, R., Ramprasad, R. &amp; Mishin, Y. Physically-informed artificial neural networks for atomistic modeling of materials. <em>arXiv:1808.01696 [cond-mat]</em> (2018).</p>
</div>
<div id="ref-raissi_physics_2017">
<p>4. Raissi, M., Perdikaris, P. &amp; Karniadakis, G. E. Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations. <em>arXiv:1711.10566 [cs, math, stat]</em> (2017).</p>
</div>
<div id="ref-raissi_physics_2017-1">
<p>5. Raissi, M., Perdikaris, P. &amp; Karniadakis, G. E. Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations. <em>arXiv:1711.10561 [cs, math, stat]</em> (2017).</p>
</div>
</div>
            </body>
</html>

