\documentclass[12pt,a4paper,]{Dissertate}


\usepackage{layouts}

% Overwrite \begin{figure}[htbp] with \begin{figure}[H]
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}


% fix for pandoc 1.14
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% TP: hack to truncate list of figures/tables.
\usepackage{truncate}
\usepackage{caption}
\usepackage{tocloft}
% TP: end hack

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

\date{}
\makeatletter
\@ifpackageloaded{subfig}{}{\usepackage{subfig}}
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\captionsetup[subfloat]{margin=0.5em}
\AtBeginDocument{%
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\AtBeginDocument{%
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother

\begin{document}

\begin{titlepage}
​    \begin{center}

    % Delete the following line
    % to remove the UCL header logo
    %\ThisULCornerWallPaper{1.0}{style/univ_logo.eps}
        
        \vspace*{2.5cm}
        
        \huge
        Quantifying the Golgi
        
        \vspace{1.5cm}
        
        \Large
        Gert-Jan Both
    
        \vspace{1.5cm}
    
        %\normalsize
        %A thesis presented for the degree of\\
        %Doctor of Philosophy
        
        \vfill
        
        \normalsize
        Supervised by:\\
        P. Sens\\
        C. Storm
    
        \vspace{0.8cm}
    
        % Uncomment the following line
        % to add a centered university logo
        % \includegraphics[width=0.4\textwidth]{style/univ_logo.eps}
        
        \normalsize
        Technical university of Eindhoven\\
        January-November 2018
    
        % Except where otherwise noted, content in this thesis is licensed under a Creative Commons Attribution 4.0 License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Copyright 2015,Tom Pollard.
    
    \end{center}
\end{titlepage}

\hypertarget{abstract}{%
\chapter*{Abstract}\label{abstract}}
\addcontentsline{toc}{chapter}{Abstract}

Title: Quantifying the Golgi

Abstract: The Golgi apparatus is a key component in intracellular
trafficking, maturing and directing proteins essential to the cell.
Despite years of research, a model coupling Golgi size and function to
the cells' transport properties is lacking. In this thesis we develop
such a model, describing the Golgi as an active droplet. New
experimental data sheds more insight in the spatial organization of the
trafficking and I have also devised two new methods relying on image
gradients and neural networks to analyze this data and confront it with
our model.

\pagenumbering{roman}
\setcounter{page}{1}
\newpage
\pagenumbering{gobble}

\tableofcontents

\newpage

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

The cell uses needs thousands of proteins and lipids to function and
many of these are produced in the Endoplasmic Reticulum (ER), an
organelle found in eukariotic cells. Exiting the ER, the cargo is
transported throughout and outside the cell, completing what is called
the \emph{secretory pathway}. Key component in this intracellular
trafficking is the Golgi apparatus, an organelle responsible for
biochemically maturing proteins and lipids exiting the ER and directing
them to the right location. Intense research over the last years has
identified key players in the secretory pathway (1, 2) but an integrated
model coupling Golgi size and function to the intracellular transport is
lacking (3, 4). In this thesis, we seek exactly such a model.

\hypertarget{the-secretory-pathway-biology-101-for-physicists}{%
\section{The secretory pathway: biology 101 for
physicists}\label{the-secretory-pathway-biology-101-for-physicists}}

Proteins produced in the ER exit the organelle at specific places known
as ER Exit Sites - ERES. At the ERES, cargo is packaged into a lipid
bilayer and this package, known as a vesicle, buds off into the
cytoplasm\textsuperscript{5}. As ERES are located throughout the cell,
the vesicles need to be transported to the Golgi apparatus. In general,
intracellular transport has two modes: diffusive and
active\textsuperscript{6}. In the active mode, molecular motors pull the
vesicles across microtubules. Microtubules (MTs) are tubular polymers
which form a network throughout the cell (in fact, they are part of the
cytoskeleton) and act as the backbone for intracellular transport. They
nucleate and organise around objects known as MicroTubular Organisation
Centers (MTOCs). The primary MTOC is the centrosome, an organelle
located next to the nucleus, but strong evidence exists that the Golgi
apparatus acts a nucleation center
too\textsuperscript{7},\textsuperscript{8}.

MTs are polarized, having two distinct ((+) and (-)) ends, and use
different molecular motors (dynesin and kynesin respectively) for
transport towards each end\textsuperscript{9}. Research has shown that
cargo constantly switches between these two transport directions in a
random fashion, making active transport in the cell a stochastic process
which can be described by a tug-of-war model\textsuperscript{10}. It's
also possible for the cargo to completely detach from the MT. The cargo
will then move through the cytoplasm in a diffusive way, until it
reattaches to a microtubule.

The appearance of the Golgi is strongly dependent on the cell type. In
plants for example, the Golgi is distributed throughout the cell in
separate but fully functional subunits\textsuperscript{11} known as
stacks, whereas in mammals all these stacks are localized in a single
organelle, the \emph{Golgi Ribbon}\textsuperscript{7}. Each Golgi stack
consists of a number of stacked compartments with a disk-like shape
called cisternae. These are membrane enclosed objects with enzymes
responsible for biochemically altering proteins. The golgi has distinct
entry and exit faces known respectively as the cis and trans face, with
the cisternae being labeled analogously. At the cis-face vesicles fuse
with the golgi, releasing their cargo into a compartment, while their
lipid bilayer becomes part of the golgi membrane. At the trans-face, the
cargo is encapsulated again in a lipid bilayer made from the golgi
membrane and is transported to its destination. The Golgi thus acts as a
sort of post-office of the cell, receiving cargo, repackaging and
sending it to the right destination\textsuperscript{12}.

\begin{figure}
\hypertarget{fig:golgimodels}{%
\centering
\includegraphics[width=0.8\textwidth]{source/figures/png/golgimodels.png}
\caption{\textbf{Left panel}: the cisternal maturation model.
\textbf{Right panel}: the vesicle-shuttle model. Image taken from
13}
\label{fig:golgimodels}
}
\end{figure}

Exactly how this sorting and maturing inside the Golgi happens is
debated\textsuperscript{14}. The two main competing models are known
respectively as the \emph{cisternal maturation} and \emph{vesicle
shuttle}. In figure \ref{fig:golgimodels} we show the structure of
the Golgi and and a schematic view of each model. In the cisternal
maturation model (see the left panel of fig.\ref{fig:golgimodels}) ,
incoming vesicles form a cis-compartment which then matures as whole to
a medial and finally trans-compartment. Vesicles are budded off and any
excess membrane is trafficked back to the cis face. In the vescile
shuttle model, the cisternae are static entities with a defined task and
cargo is moved from one compartment to the next by vesicles. This is
shown in the right panel. After budding off from the trans face, the
cargo is transported to either some location in the cell or secreted.

\hypertarget{quantitative-models-of-the-golgi}{%
\subsection{Quantitative models of the
Golgi}\label{quantitative-models-of-the-golgi}}

Although the Golgi has been intensively studied by biologists for many
years, very few quantitative work has appeared. Our research only turned
up a single attempt by Hirschberg et al 15 in which they use a kinetic
model to describe the trafficking of a virus (VSVG) from the ER through
the Golgi to the plasma membrane. We reprint their main result in figure
fig.\ref{fig:ratemodel}.

\begin{figure}
\hypertarget{fig:ratemodel}{%
\centering
\includegraphics{source/figures/png/kineticmodel.png}
\caption{\textbf{Left panel}: First order rate model fitted to
experimental data by Hirschberg et al. \textbf{Right panel}: Inferred
concentration in ER, Golgi and PM using the fitted parameters from the
left panel and their model. Image taken from 15}\label{fig:ratemodel}
}
\end{figure}

Hirschberg et al model the secretory pathway by dividing it into three
reservoirs: each one representing respectively the ER, Golgi and the PM.
The compartments are connected by a first-order rate equation, i.e.
\(d \phi_{golgi}/dt=k_{ER \to Golgi}\phi_{ER}\) and they show (see
figure fig.\ref{fig:ratemodel}) that such an equation is sufficient to
model both total concentration in the cell as well as the concentration
in the Golgi. Although this model fits the experimental data well, it's
a phenomological model, reducing many processes to a single rate
parameter \(k\) and neglecting any spatial dependence. It is thus unable
to provide any information on how the intracellular transport of the
vesicles couples to the function and size of the Golgi.

\hypertarget{this-thesis}{%
\section{This thesis}\label{this-thesis}}

In this thesis we seek to build a model which couples intracellular
transport to the Golgi function and size. We hypothesize that we can
describe the Golgi as an \emph{active, phase separated droplet}. We also
confront our model with experimental data, obtained by the group of
Frank Perez at Institut Curie. This group developed a new technique
called RUSH\textsuperscript{16} which allows precise timing of the
release of proteins from the ER and study the intracellular transport
using fluorescence microscopy. In the next sections, we justify the
description of the Golgi as an active phase-separated droplet and how we
intend to perform the data analysis of the experimental data.

\hypertarget{golgi-as-an-active-phase-separated-droplet}{%
\subsection{Golgi as an active phase-separated
droplet}\label{golgi-as-an-active-phase-separated-droplet}}

Many biological processes require a relatively high concentration of
some protein or lipid to occur. This can be reached by physically
separating this protein inside a membrane, but the cell has several
membraneless organelles. Such an organelle thus needs a different
process to reach such concentrations. The prime candidate is
liquid-liquid phase separation and indeed it is able to correctly
describe several phenomena such as P-granules\textsuperscript{17} and
centrosome growth\textsuperscript{18}.

The strongest clue for describing the Golgi comes from its biogenesis:
upon removal, the Golgi is able to form \emph{de novo}. Ronchi et
al\textsuperscript{8} study golgi biogenesis in detail and found several
phases of growth. First, vesicles are released from the ER until after
some time stack-like structures are formed, including a cisternae-like
topology. In the second phase, the stacks are transported by the
microtubules to thelocation of the would-be golgi, where the stacks are
fused into a single golgi ribbon. The first phase resembles phase
transition: once the solution reaches some critical density, phase
separation occurs and a dense droplet is formed. One crucial detail
we've skipped over is that the golgi isn't a membraneless organelle. The
cargo entering and exiting the Golgi isnt membraneless either; while
these membranes ensure phase separation of the cargo, if we interpret
the vesicles as the liquid to separate, phase separation occurs if the
vesicles combine into a single dense droplet - the Golgi. A droplet
arising from such a description would be a passive droplet however,
forever growing. Contrarily, the golgi constantly takes up vesicles with
immature proteins while vesicles with mature proteins bud off. Including
a term to mimic this process makes the droplet \emph{active}. We can
thus model a membrane-delimited organelle using tools normally used for
membraneless ones.

\hypertarget{biological-image-analysis}{%
\subsection{Biological image analysis}\label{biological-image-analysis}}

Image analysis is a lively subject in cell biology, with many different
methods and techniques being developed constantly. The last years image
analysis is being developed with the specific goal of quantification in
mind\textsuperscript{19}, specifically as an insight into intracellular
transport (20, 21, 22). To analyze intracellular transport or flow in a
cell, single particle tracking (SPT) is a powerful method. SPT uses
small fluorescent beads or particles suspended in a flow which can be
tracked by a computer. The obtained trajectories can then be analyzed to
obtain information about the transport. The RUSH method doesn't make use
of such beads or particles, so that these techniques cannot be applied.
Alternative methods mainly utilize some sort of correlation spectroscopy
(23, 24, 25). One can derive a general relation between the correlation
of the noise and transport properties such as the diffusion constant.
These techniques require a signal roughly constant in time, which
unfortunately is not a given for the RUSH experimental data. Thus, none
of the techniques we found are applicable to the RUSH data.

All these methods are a specific solution to a general problem: how do
we fit a model to a spatiotemporal dataset? Since most of the times a
model is written in the form a partial differential equation (i.e.
\(df/dt = \alpha df/dx+\beta d^2f/dx^2+...\)), while the data is
\(f(x,t)\), this is not a trivial problem. In this case, we're actually
asking what set of parameters in our model (such as a diffusion
coefficient) best describes the temporal evolution of a dataset. We have
developed and evaluated two different methods for doing this. Our first
method attacks the problem rather directly, by calculating spatial and
temporal derivatives directly from the data using something known as
image gradients. Our second alternative method is based on a recently
invented method based on neural networks\textsuperscript{26}. We'll show
that by encoding physics into the neural network, we're not only able to
infer the optimal parameters, but even an optimal parameter
\emph{field}.

\hypertarget{structure-and-main-questions}{%
\subsection{Structure and main
questions}\label{structure-and-main-questions}}

The rest of this thesis is divided into two parts. In the first part we
show the two model fitting methods we have developed and apply them to
the RUSH experimental data. The second part discusses the model we have
developed for the Golgi. In a chapter-by-chapter breakdown, we have the
following:

\begin{itemize}
\tightlist
\item
  \textbf{Part I - Model fitting and data analysis}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Chapter 2} introduces the framework we have developed for
    model fitting spatiotemporal data using image gradients.
  \item
    \textbf{Chapter 3} applies the method developed in chapter 2 to
    experimental data.
  \item
    \textbf{Chapter 4} shows an alternative method for model fitting
    based on neural networks.
  \end{itemize}
\item
  \textbf{Part II - Model for Golgi }

  \begin{itemize}
  \tightlist
  \item
    \textbf{Chapter 5} introduces the Cahn-Hilliard equation, which
    describes phase separation, an approximation of it known as
    effective droplet theory and describes our model.
  \item
    \textbf{Chapter 6} contains the predictions the model developed in
    chapter 5 and investigates the biological implications.
  \end{itemize}
\item
  \textbf{Chapter 7-} is the concluding chapter and summarizes all the
  findings from the previous chapters.
\end{itemize}

\hypertarget{model-fitting}{%
\chapter{Model fitting}\label{model-fitting}}

In this chapter we introduce the method we have developed for fitting a
model in the form of a PDE to spatiotemporal data. We start with a
section describing the general idea and subsequent sections elaborate on
each step. The method principaly works for any type of data and model,
but was developed originally to analyze data from the RUSH experiments.
We have chosen to illustrate the effects of each step with RUSH
experimental data instead of synthetic data.

\hypertarget{the-concept}{%
\section{The concept}\label{the-concept}}

Assume we have acces to experimental data of some process \(f(x,t)\).
Parallely, we have also developed a model describing this process, but
it is the form of a PDE: \begin{equation}
\partial_t f(x,t) = \lambda_1 \nabla^2f(x,t)+\lambda_2\nabla f(x,t) +\lambda_3 f(x,t) +\lambda_4
\label{eq:PDE}\end{equation} We now wish to investigate if this model
fits the data \(f(x,t)\) and find the optimal value of coefficients
\(\lambda_i\). To do so, we consider each spatial term in \(f(x,t)\) in
eq.\ref{eq:PDE} as some variable \(x_i\) but \(\partial_t f\) as \(y\),
so that we can rewrite it as: \[
y = \lambda_1 x_1+\lambda_2x_2 +\lambda_3 x_3 +\lambda_4
\]

If we thus can find the variables \(x_i\) and \(y\), we can perform some
fitting procedure such as least squares to obtain the coefficients
\(\lambda_i\). In other words, if we can calculate the spatial and
temporal derivatives of our data, we can fit the model. Although the
concept seems trivial, its implementation is all but. Data is rarely
noiseless and obtaining accurate derivatives from noisy data is
notoriously hard, but forms the heart of our method. It's also possible
to have distinct models in different areas of the data, so that we need
to segment the data. Furthermore, the coefficients \(\lambda_i\) might
not be constant but could be space- and time- dependent. The process of
fitting the data thus has several steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Denoising and smoothing
\item
  Calculating derivatives
\item
  Segmenting
\item
  Fitting
\end{enumerate}

In the next sections, we describe each step separately. Note that the
method we present here has been developed empirically: there's no
theoretical background as to why this particular combination should work
optimally. Instead, it's been developed while analyzing the data,
adapting each step on the go. However, we have found several parallel to
another method from machine vision known as optical
flow\textsuperscript{27}.

\hypertarget{step-1---smoothing-and-denoising}{%
\section{Step 1 - Smoothing and
denoising}\label{step-1---smoothing-and-denoising}}

The first step in our pipeline is to denoise and smooth the data. The
smoothing is necessary for accurately calculating the derivatives.
Denoising still is a very active area of research (especially in life
sciences) and dozens of different methods exist\textsuperscript{30}. For
example, one could Fourier transform the signal and use a high pass
filter, but this would also get rid of small and sharp features. After
evaluating several methods, we have settled on the so-called `WavInPOD'
method, introduced by 31 in 2016. In 32 they show that this methods
outperforms several other advanced methods. WavInPOD combines Proper
Orthogonal Decomposition (POD) with Wavelet filtering (Wav). Both
subjects are vast (especially Wavelet transform) and we're only
interested in the result of the technique, so we only present a short
introduction here, adapted from 31.

POD is a so-called dimensionality reduction technique and is very
closely related to the more well-known Principal Component Analysis
(PCA) in statistics. In physics it's ofen used to analyze turbulent
flows\textsuperscript{33}. In POD we wish to describe a function as a
sum over its variables:

\[
f(x,t)=\sum_n^r \alpha_n(x)\phi_n(t)
\]

where \(\alpha_n\) and \(\phi_n\) are called respectively the spatial
and temporal modes. Associated with each mode \(n\) is an energy-like
quantity \(E_n\). Modes with a higher `energy' \(E_n\) contribute more
to the signal \(f\) than modes with a lower energy and we can thus
approximate the signal by selecting the \(k\) modes with the highest
energy. A typical log10 energy spectrum has a `knee' in the values, as
shown in figure fig.\ref{fig:eigen}. Modes with an energy below the
knee are noise, and modes above signal.

\includegraphics{source/figures/pdf/eigenspectrum.pdf}

The wavelet transform is very similar to the Fourier transform, but uses
wavelets as its basis. A fourier transform gives the frequency domain
with infinite precision, but tells nothing about the locality of the
frequencies (.i.e when each frequency is present in a signal). By using
a wavelet (a wave whose amplitude is only non-zero for a finite time),
we sacrifice precision in the frequency domain but gain information on
the locality instead. Performing a wavelet transform transforms the
signal into the sum of an approximation and its details and we can
filter this analogous to a fourier filter. Due to its locality however,
noise is filtered out, by sharpness is retained.

WavinPOD combines these two techniques by applying wavelet filtering to
the POD modes. In detail, one first decomposes the problem with a POD
transformation. The energy spectrum of this transformation is shown in
figure fig.\ref{fig:eigen} and we select a cutoff of 27. All retained
modes are wavelet filtered and are then retransformed to give the
denoised and smoothed signal. In figure fig.\ref{fig:filtered} we show
the results of the smoothing in the time and spatial domain. In the left
panel we show the signal of a single pixel in time, while we plot a line
of pixels in a single frame in the right panel. The red lines denote the
original (unfiltered) signal, the blue line the effect of just applying
a POD and the black one the result of the WavInPOD technique. Note that
the effect of the wavelet filtering is to smooth the signal
significantly and in comparing the original data to the filtered data
that we've retained the sharpness of the features whilst obtaining a
smooth signal.

\begin{figure}
\hypertarget{fig:filtered}{%
\centering
\includegraphics{source/figures/pdf/filtered.pdf}
\caption{Effect of POD with a cutoff of 27 and wavelet filtering with a
level 3 db4 wavelet. Left panel shows the result in the time domain,
right panel in the spatial domain. Lines have been offset for
clarity.}\label{fig:filtered}
}
\end{figure}

\hypertarget{step-2---derivatives}{%
\section{Step 2 - Derivatives}\label{step-2---derivatives}}

After having denoised the images, we calculate the spatial and temporal
derivatives. Obtaining correct numerical derivatives is hard and becomes
much more so in the presence of noise\textsuperscript{35}. Next to a
finite-difference scheme, one can for example (locally) fit a polynomial
and take its derivative or use a so-called
tikhonov-regularizer\textsuperscript{36}. The computational cost of
these methods is high however and they don't scale well to dimensions
higher than one. For our spatial derivatives these methods are thus not
available. In fact, obtaining the gradient of a 2D discrete grid has
another subtlety which we need to adress.

Naively, one could obtain the gradient of a 2D grid by taking the
derivative using a finite difference scheme with respect to the first
and second axis. If there are features on the scale of the
discretization (\(\sim\) few pixels), such an operation will lead to
artifacts and underestimate the gradient. These issues have long been
known and several techniques have been developed to accurately calculate
the gradient of an `image'. The most-used image-gradient technique is
the so-called Sobel operator. It belongs to a set of operations known as
\emph{kernel operators}. Kernel operators are expressed as a matrix and
by convolving this matrix with the matrix on which the operation is to
be performed, we obtain the result of the operator. We show this for the
Sobel operator.

Consider a basic central finite difference scheme:

\[
\frac{df}{dx}\approx\frac{f(x_{i+1})-f(x_{i-1})}{2h}
\]

where \(h\) is defined as \(x_{i+1}-x_{i}\). In terms of a kernel
operator, we rewrite this as:

\[\frac{1}{2}\cdot
\begin{bmatrix}
1 & 0 & -1
\end{bmatrix}
\] where we have set \(h=1\), as the distance between pixels is one by
definition. By convolving this matrix with the matrix \(A\) we obtain
the derivative of \(A\): \[
\partial_xA\approx A*\frac{1}{2}\begin{bmatrix}
1 & 0 & -1
\end{bmatrix}
\] \includegraphics{source/figures/pdf/derivative.pdf}

As stated, this operation is inaccurate and introduces artifacts. To
improve this, we wish to include the pixels on the diagonal of the pixel
we're performing the operation on as well (see figure
fig.\ref{fig:sobel}). The distance between the diagonal pixels and the
center pixel is not 1 but \(\sqrt{2}\) and the diagonal gradient also
needs to be decomposed into \(\hat{x}\) and \(\hat{y}\), introducing
another factor \(\sqrt{2}\). The kernel thus obtained is the classic
\(3\times3\) Sobel filter: \[
\mathbf G_x=\frac{1}{8}\cdot
\begin{bmatrix}
1 & 0 & -1\\
2 & 0 & -2\\
1 & 0 & -1
\end{bmatrix}
\]

Increasing the size of the Sobel filter increases its accuracy and we've
implemented a 5x5 operator. Implementing the derivative operation as a
kernel method is also beneficial from a computational standpoint, as
convolutional operations are very efficient. The Sobel filter is usually
applied to an image and hence is often said to calculate the
image-gradient, but due to its separability is possibe to scale this
method to an arbitrary number of dimensions.

\hypertarget{step-3---segmentation}{%
\section{Step 3 - Segmentation}\label{step-3---segmentation}}

In the case of the RUSH data, obtained images and movies often contain
multiple cells. Each of these cells can be further segmented into two
more areas of interest: the cytoplasm, which is were we want to fit our
model and the Golgi apparatus. We wish to make a mask which allows us to
separate the cells from the backgroud and themselves and divide each
cell into cytoplasm or Golgi. Figure fig.\ref{fig:threeframes} shows
four typical frames in the MANII transport cycle. Note that no sharp
edges can be observed, especially once the MANII localizes in the Golgi.
No bright field images were available as well, together making use of
techniques such as described in 37 unavailable. We have thus developed
two methods which allow us to segment the image and the cells, based on
Voronoi diagrams and the intensity.

\hypertarget{voronoi-diagram}{%
\subsection{Voronoi diagram}\label{voronoi-diagram}}

Consider again the frame on the left of figure
fig.\ref{fig:threeframes}. Note that in early frames such as this one,
the cargo (i.e.~fluorescence) is spread circumnuclear. Applying a simple
intensity based segmentation gives us a number of separate areas, which
\emph{very} roughly correspond to a cell. We can then pinpoint each
cells' respective center. Given \(n\) points, Voronoi tesselation
divides the frame into \(n\) areas, where point \(i\) is the closest
point for each position in area \(A_i\). The hidden assumption here is
thus that each pixel belongs to the cell center it's closest too.
Although this is a very big assumption, in practice we've found this to
be reasonable. Furthermore, one can add `empty' points to make the
diagram match observations. Assuming small movements of the cell, this
isn't an issue either for this technique, as we are assigning an area to
each cell instead of very precisely bounding it. This also allows us to
calculate the Voronoi diagram in the early frames and apply the
segmentation to the entire movie. The result of this segmentation for
MANII is shown in figure fig.\ref{fig:voronoi}. Each cell centre is
denoted by a dot, while the lines denote the border between each voronoi
cell.

\begin{figure}
\hypertarget{fig:voronoi}{%
\centering
\includegraphics{source/figures/pdf/Voronoi.pdf}
\caption{The obtained mask. Red dots are cell centers, dashed lines
infinite edges and solid lines finite edges.}\label{fig:voronoi}
}
\end{figure}

\hypertarget{intensity}{%
\subsection{Intensity}\label{intensity}}

The Voronoi technique works very well for an area-based approach, but
for analyzing our fitting data we would like a more precise mask -
although we still don't require pixel-level accuracy. From the movies,
the Golgi is clearly visible and we can separate the cytoplasm from the
background, with a big `gray' area inbetween. We thus turn to an
intensity based approach. We have developed the following approach for
localizing the Golgi:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Normalize the intensity \(I\) between 0 and 1.
\item
  Sum all the frames in time: \(\sum_n I(x, y, t_n)\). A typical result
  is shown in figure fig.\ref{fig:mask} .
\item
  Threshold the image to obtain the mask. This is either done
  automatically through an Otsu threshold or by manually adjusting the
  threshold until desired result.
\item
  The mask is postprocessed by filling any potential holes inside the
  mask.
\end{enumerate}

This procedure was unable to reliably separate the background from the
cytoplasm. Noting that while the cytoplasm might not have the intensity
as the golgi, its time derivative should be higher than the rest of the
areas. We replace step two by
\(\log_{10}\left(\sum_nI(x,y,t_n)\cdot\partial_tI(x,y,t_n)\right)\),
where the time derivative has been normalized between 0 and 1. Figure
fig.\ref{fig:mask} shows our final results. The upper two panels show
the images obtained after performing the summing operation for the Golgi
and cytoplasm respectively, while the lower left panel shows the final
mask obtained after thresholding these two images. For comparison, we
plot frame to compare the mask to.

\begin{figure}
\hypertarget{fig:mask}{%
\centering
\includegraphics{source/figures/pdf/segmenting.pdf}
\caption{Four panels showing the different stages of making the mask.
From segmenting the upper two panels we determine the golgi and active
area, leading to the mask in the lower left. Compare the to the lower
right.}\label{fig:mask}
}
\end{figure}

\hypertarget{step-4---fitting}{%
\section{Step 4 - Fitting}\label{step-4---fitting}}

The final step in our method is to fit the our model to the data. By
calculating the derivatives, we have reduced our PDE-model to a generic
model, which allows us to use virtually any fitting method. For
simplicity, we use least-squares, but one could use a Bayesian method to
obtain not only the fit, but also the uncertainty.

Equation eq.\ref{eq:PDE} assumes a model with constant coefficients. In
reality, coefficients will be spatially and even temporally varying. To
circumvent this issue, we assume the coefficients can be assumed to be
locally constant. We thus assume that for a small area we can fit the
model using constant coefficients. We perform this operation for every
datapoint in a sliding-window manner, as shown in figure
fig.\ref{fig:slidingwindow}, thus ending up with a coefficient field.

\begin{figure}
\hypertarget{fig:slidingwindow}{%
\centering
\includegraphics{source/figures/pdf/slidingwindow.pdf}
\caption{Schematic overview of the sliding window technique. The solid
black line encompasses an area around its blue coloured central pixel
and the fit output is assigned to that pixel. We then move the window
(dashed black line) and perform the fit for the orange coloured
pixel.}\label{fig:slidingwindow}
}
\end{figure}

In the next chapter we apply this method to the RUSH experimental data.

\hypertarget{data-analysis}{%
\chapter{Data analysis}\label{data-analysis}}

In this chapter we apply the method developed in the previous chapter to
experimental data obtained using the RUSH technique. Our first section
introduces the RUSH technique and discusses our model for the
intracellular transport. In the following section we discuss the
experimental data, investigate the fluorescence curves of several areas
of interest. and gain more insight into the data by studying its time
derivative. We then present a linear least squares fit and show that
this can lead to unphysical results. We end with a short section of both
recommendations for the experimentalists as well as a number of ways to
improve the method.

\hypertarget{experimental-data}{%
\section{Experimental data}\label{experimental-data}}

The transport of vesicles from the ERES to the Golgi is both diffusive
and directive and a technique known as RUSH (Retention Using Selective
Hooks) has recently been developed\textsuperscript{16} in the team of
Frank Perez at Institut Curie to study this trafficking. RUSH allows
precise timing of the release of proteins from the ER and can be used to
follow the secretory pathway from the ER to the Golgi and even
post-golgi using fluorescent live-cell imaging. Several other methods
have been developed (\textbf{???}--golgi\_1997,
\textbf{???}--golgi\_2018), but lack the non-toxicity, timing and
versatility of the RUSH technique.

\begin{figure}
\hypertarget{fig:RUSH}{%
\centering
\includegraphics{source/figures/png/RUSH.png}
\caption{Schematic overview of the RUSH system. Image taken from
16}\label{fig:RUSH}
}
\end{figure}

Figure fig.\ref{fig:RUSH} shows the principle of the RUSH system.
Inside the ER, a core streptavidin is fused to it using a hook protein.
Another protein known as a streptavidin-binding-protein (SBP) binds to
streptavidin, but connected to the SBP are also the protein to be
transported (`reporter') and a fluorescent protein. Upon the addition of
biotin, the SBP is released from the streptavidin as the biotin binds to
it. The SBP-reporter-fluorescent complex then exits the ER and can be
followed the entire secretory pathway with fluorescence microscopy.

The RUSH technique can be used for many different proteins, but in this
thesis we mainly focus on the \(\alpha\)-mannosidase-II, generally
referred to as ManII. The ManII protein resides in the Golgi apparatus
and thus upon reaching it will remain there. This means that the data we
obtain will only contain transport \emph{towards} the golgi, greatly
simplifying the analysis as we won't have to post and pre-golgi traffic.
Figure fig.\ref{fig:manII} shows two frames in a typical RUSH
experiment of ManII trafficking. The left panel shows an image obtained
just after the addition of the biotion, so that most of the cargo is
still retained in the ER. A later frame is shown on the right: we can
observe the localization of fluorescence in the Golgi, while there's
still fluorescence in the rest of the cells.

\begin{figure}
\hypertarget{fig:manII}{%
\centering
\includegraphics{source/figures/pdf/frames.pdf}
\caption{Two frames of the ManII transport images using the RUSH
technique.}\label{fig:manII}
}
\end{figure}

\hypertarget{model}{%
\subsection{Model}\label{model}}

Vesicles exiting the ERES are transported towards the ER over the
microtubules. This is a stochastic process with the proteins detaching
from and (re-) attaching to the microtubules randomly, while the
vesicles move diffusely once detached. Several models have been
developed to describe such intracellular transport processes (6, 38),
many in the light of virus trafficking (39, 40, 41). In general, these
models assume a two population model, with one population being cargo
attached to a microtubule and another cargo freely diffusing in the
cytoplasm. If one assumes that the timescale for attaching and detaching
from the microtubules is much smaller than the transport timescale, the
two populations can be assumed to be in equilibrium. In this assumption,
known as a quasi-steady-state reduction, the two population model
reduces to a Fokker-Planck equation. As the Fokker-Planck equation is
functionally equivalent to an advection-diffusion equation, we
hypothesize that we can model protein transport using an
advection-diffusion equation: \begin{equation}
\partial_t c = D\nabla^2c-v\nabla c
\label{eq:adeq}\end{equation} where \(c\) is the concentration of the
cargo, \(D\) a diffusion coefficient and \(v\) an advection velocity.
Equation eq.\ref{eq:adeq} is thus the model we fit our data to. Note
that the fluorescence images obtained from the RUSH experiment return an
intensity \(I\) and not a concentration \(c\), and hence we make the
assumption \(c \propto I\).

\hypertarget{initial-analysis}{%
\section{Initial analysis}\label{initial-analysis}}

We first study the evolution of the fluorescence in two ways, plotting
both the mean fluorescence for each frame and the mean fluorescence in
the golgi region. To get rid of the background in our statistics, we
normalize the fluorescence between before computing the mean. The left
panel of frame \#fig:fluorescence shows the average fluorescence of each
frame and shows a significant drop of almost \(30\%\) between the
initial and final frame. We observe a strong initial drop and a slower
decay after roughly frame 100, which can probably be attributed to
photobleaching. The transition between the two decays roughly matches
the saturation of the fluorescence in the Golgi (see the right panel),
casting strong doubts on our assumption that \(c\propto I\).
Compensating for this is possible (see 15), but requires significant
effort on the experimentalists' part and if the difference between two
subsequent frames is small the effect on our fit is neglegible.

.\includegraphics{source/figures/pdf/general_fluorescence.pdf}

In the right panel we show the fluorescence in the Golgi ROI for each of
the three cells. Specifically, we plot the mean intensity in each ROI,
normalized on the maximum intensity and compensated for the loss of
fluorescence as shown in the left panel. Interestingly, we observe that
all three curves roughly follow a similar pattern. Although one of the
cells (blue line) seems to have some sort of delay, the fluorescence
seems to increase in a linear way up to frame 100, when the fluorescence
saturates. The purple cell shows a significant drop at frame 200, but
since the ManII protein remains in the Golgi, this is not caused by any
type of intracellular transport and thus not of interest to us. The
linear increase and common pattern suggests that the transport
properties are not concentration dependent at these concentrations.

To perform our fit in the next section, we have also determined the time
derivative of the dataset, \(\partial_tI\). We plot this for four
different frames in figure fig.\ref{fig:timederiv} .

\begin{figure}
\hypertarget{fig:timederiv}{%
\centering
\includegraphics{source/figures/pdf/time_deriv.pdf}
\caption{The determined time derivative four different frames of the
ManII RUSH experiments.}\label{fig:timederiv}
}
\end{figure}

Areas where the derivative is positive (thus were the concentration
increases) are coloured red, while areas where the concentration
decreases are coloured blue. As expected, the Golgi shows up in each
cell as a bright red object. Note however that we also observe red areas
towards the edges of the cell. As the concentration close to the Golgi
decreases, the red area moves outwards and slowly takes over the blue
area. This could be caused by ERES acting as a in that area, but given
that ERES are located throughout the cell it seems more likely such a
pattern would be caused by diffusion.

\hypertarget{analysis-of-ls-fit}{%
\section{Analysis of LS-fit}\label{analysis-of-ls-fit}}

In this section we analyse results of the least squares fit. We've used
a 7x7 window in the spatial domain to perform the sliding window
operation, fitting each frame of the movie independently. We analyse the
diffusion, advection and the error of our fit. These fits are movies and
we're hence unable to print them - please find our these movies at our
GitHub. In figure fig.\ref{fig:diff_ls} we show two typical inferred
diffusion fields in the upper row, just after addition of the biotin
(frame 4) and halfway to the complete saturation of the Golgi (frame
40).

\begin{figure}
\hypertarget{fig:diff_ls}{%
\centering
\includegraphics{source/figures/pdf/Diff.pdf}
\caption{Analysis of the inferred diffusion field. The upper row shows
the inferred field at two frames, while the lower row shows the
distribution of values and the fraction of physical values as a function
of time.}\label{fig:diff_ls}
}
\end{figure}

Another problem is that we observe many areas with a negative
-unphysical- diffusion coefficient. In the lower left panel we plot the
distribution of values. Analysis shows that roughly \(40\%\) of the
inferred field has a negative diffusion coefficient. In the lower right
panel we have calculated this fraction as a function of time. It shows
that, save for a few initial frames, this fraction is not (strongly)
time-dependent. Results are slightly skewed though, since many
coefficients are negative but extremely close to zero (e.g
\(-10^{-4}\)). Negative diffusion coefficients correspond to clustering,
but could also be the result of an incorrect fit. We investigate this in
depth after studying the advection profiles, which we show in figure
fig.\ref{fig:advection} . In the four panels we show the inferred
velocity in the \(\hat{x}\) and \(\hat{y}\) direction, the magnitude of
the velocity and the angle.

\begin{figure}
\hypertarget{fig:advection}{%
\centering
\includegraphics{source/figures/pdf/Fit_LSconstrainedadvection.pdf}
\caption{Analysis of the velocity fields. The upper rows show
respectively \(v_x\) and \(v_y\), wwhile the lower row shows the
magnitude and angle.}\label{fig:advection}
}
\end{figure}

Similar to the diffusion, we observe patterns both in time and space
bigger than our fitting window, meaning that the fit isn't completely
random. On the other hand, we are not able to discern any specific flow
from the figures in fig.\ref{fig:advection}. To gain more insight into
our fit, we analyze a single pixel in time. Figure
fig.\ref{fig:timepixel} shows the diffusion and advection velocities as
a function of time. We've plotted a scaled and translated signal of that
pixel in a black dashed line. This pixel is initially constant and then
decreases to noise level. Note that initially, while the signal is
constant, the diffusion constant is negative. Once the signal starts
decreasing, or, in other words, cargo starts flowing from that pixel, we
see a physical diffusion constant and non-zero velocities. Once the
signal returns to around noise-level at pixel 50, the inferred
velocities and diffusion constant seem to become random around 0. In
other words, our method seems to work when cargo is flowing, but
struggling when the signal is either constant or at noise level. We
observe similar behaviour in other pixels, so we contribute (most of)
the unphysical diffusion values to constant and noise-level signal.
Performing the fit with the diffusion constant constrained to be larger
than zero lead to the negative coefficients in the free fit to being set
to zero.

\begin{figure}
\hypertarget{fig:timepixel}{%
\centering
\includegraphics{source/figures/pdf/Fit_LSgeneral_fit.pdf}
\caption{Diffusion and advection velocities of a single pixel in time.
We've plotted (scaled and translated) signal as a black dashed line to
find any correlation.}\label{fig:timepixel}
}
\end{figure}

This doesn't completely explain the magnitude of the coefficients
though, which is significantly lower than expected. One possibility is
the `mixing' of the transport fluorescence with the fluorescence of the
ER. After the addition of biotin, the fluorescent cargo gets released,
but still has a finite residence time in the ER. Since the obtained
images are projected over an axis, changes in fluorescence we observe
can be both due to intracellular transport as well as processes inside
the ER. If these processes have different timescales, this can strongly
affect the inferred coefficients.

Another possibility is that we've assumed that a concentration of
fluorescent particles leads to some sort of `mean-field' fluorescence
which we can describe by an advection-diffusion equation. Once the size
of the particles becomes on the order of the pixel size, this assumption
breaks down. In the case of the ManII trafficking, the pixels are
roughly two to three times the size of a vesicle, meaning that we are at
the limits of the `mean-field' assumption.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

We've applied the method developed in the previous chapter to the RUSH
trafficking data of the ManII protein. Although our fit shows patterns
in the coefficient fields larger than our fitting window, pointing at
some underlying pattern, we're unable to make any conclusions about our
model. It's been shown that the model seems to perform well when the
data is transient, i.e.~when cargo is actually flowing. However, for
most of the time the data is either constant or at noise level,
preventing us from making any conclusions.

Improvements to our method fall roughly into two categories. The first
category concerns improvements to the calculation of the derivatives.
We've implemented a rather basic 5x5 Sobel filter, but implementing more
advanced methods which would result in more accurate derivatives would
probably make the biggest improvement. The second category would be
improve the fitting procedure. The most obvious candidate is
implementing some sort of Bayesian method which would return not just
the most probable coefficient, but the entire probability distribution.

\hypertarget{physics-informed-neural-networks}{%
\chapter{Physics Informed Neural
Networks}\label{physics-informed-neural-networks}}

In the previous chapters we showed the difficulties in fitting a model
in the form of a partial differential equation to spatio-temporal data.
The method we developed was a classical numerical approach, separating
the problem into several substeps such as denoising, smoothing and
numerical differentiating. In the last few years machine learning has
been slowly making its way into physics. Very recently, a technique
generally referred to as Physics Informed Neural Networks (PINNs) have
shown great promise as both tools for simulation and model fitting (42,
43, 44, 26, 45). In this chapter, I will evaluate the use of this
technique to fit the model to the RUSH data. I've divided the chapter
into three parts:

\begin{itemize}
\tightlist
\item
  \textbf{Neural Networks} - This part will cover the basics of neural
  networks: their inner workings, how to train them and other general
  features.
\item
  \textbf{Physics Informed Neural networks} - In this second part we
  introduce the concept behind PINNs, use it to solve a toy problem and
  apply it to our RUSH data.
\item
  \textbf{Conclusion} - Finally we summarize the results and
  observations from the previous sections.
\end{itemize}

\hypertarget{neural-networks}{%
\section{Neural Networks}\label{neural-networks}}

Artificial Neural Networks (ANNs) are networks inspired by biological
neural networks. Contrary to other ways of computing, ANNs are not
specifically programmed for a task - instead, ANNs are \emph{trained}
using a set of data. Research on artificial neural networks started in
the '40s but never gained any critical mass, as no efficient training
algorithm was known. Once an efficient training algorithm was found in
1975 by Werbos, interest resurged but it wasn't until the late '00s that
deep learning started gaining widespread traction. The use of GPU's
allowed ANNs to be efficiently trained and widely deployed at reasonable
cost.

The advancements in machine learning in general and especially neural
networks in the last ten years have yielded a wealth of techniques and
approaches. In supervised learning, the network is given pre-labeled
data so that it is trained by learning the mapping from the given inputs
to the given outputs. Other types such as supervised learning, where the
network needs to learns to discriminate between unlabeled data, and
reinforcement learning don't have any obvious use for PINNs yet and I've
thus chosen to omit them. In the next sections, I'll present the
mathematics of an ANN and show how they are trained using the so-called
\emph{backpropagation} algorithm.

\hypertarget{architecture}{%
\subsection{Architecture}\label{architecture}}

\emph{An excellent introduction is given by Michael Nielsen in his
freely available book ``Neural networks and deep learning.'' The
following section has been strongly inspired by his presentation.}

At the basis of each neural network lies the neuron. It transforms
several inputs non-linearly into an output and we can use several
neurons in parallel to create a \emph{layer}. In turn, we several layers
in series make up a network. The layers in the middle of the network are
known as \emph{hidden layers}, as shown in figure
fig.\ref{fig:neuralnetwork}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth,height=\textheight]{source/figures/pdf/neuralnetwork.pdf}
\caption{Schematic view of a neural network.}
\end{figure}

In the schematic shown in fig.\ref{fig:neuralnetwork}, each neuron is
connected to every neuron of the previous and next layer. This is known
as a \emph{fully connected} layer. Using only this type of layers, we've
created a feed-forward network and it has been proven that a single
hidden layer with enough neurons is a \emph{universal function
approximator}, i.e.~a neural network can represent any continuous
function using enough neurons.

As stated, a neuron takes several inputs and transforms them into an
output. This is a two step process, where in the first step the neuron
multiplies the input vector \(\mathbf{x}\) with a weight vector \(w\)
and adds a bias \(b\):

\begin{equation}
z = w\mathbf{x}+b
\label{eq:weighted_input}\end{equation}

\(z\) is called the weighted input and is transformed in the second step
by the neuron \emph{activation function \(\sigma\)}. This in turn gives
the output of the neuron \(a\), also known as the activation:

\begin{equation}
a = \sigma(z) = \sigma(w\mathbf{x}+b)
\label{eq:activation}\end{equation}

The role of the activation function is to introduce non-linearity into
the system. The classical and often used activation function is the
\(tanh\), as it is bounded between +1 and -1. Since we're working with
multiple layers, it is useful to rewrite function
eq.\ref{eq:activation} in terms of the activation \(a^l\) of layer
\(l\):

\[
a^l = \sigma(z^l) = \sigma(w^la^{l-1}+b^l)
\]

where \(w^l\) and \(b^l\) are respectively the weight matrix and bias of
layer \(l\).

\hypertarget{training}{%
\subsection{Training}\label{training}}

In supervised learning the task of training a machine means adjusting
the weights and biases until the neural network predictions match the
desired outputs. We thus need some sort of metric to define this
`distance' between prediction and desired output. Training the network
than means minimizing the metric with respect to the weights and biases
of the network. This metric is known as the cost function
\(\mathcal{L}\) and the most used form is a mean squared error:

\begin{equation}
\mathcal{L} = \frac{1}{2n}\sum_i|y_i-a^L_i|^2
\label{eq:MSE}\end{equation}

where \(n\) is the number of samples, \(y_i\) the desired output of
sample \(i\) and \(a^L_i\) the activation of the last function - the
prediction of the network. Minimizing this is not trivial, as the
problem can have many local minima. A solution can be found however
using gradient descent techniques.

Gradient descent techniques are based on the fact that given an initial
position, the fastest way to reach the minimum from that position is by
following the steepest gradient. Thus, given a function
\(f(\mathbf{x})\) to minimize w.r.t to \(\mathbf{x}\), we guess an
initial position \(x_n\) and iteratively change until it convergences:

\[
\mathbf{x}_{n+1} = \mathbf{x}_{n}-\gamma\nabla f(\mathbf{x}_n)
\]

where \(\gamma\) is known as the learning rate. If a global minimum
exists, this technique will converge on it. More advanced versions of
this technique exist which are able to deal with local minima as well,
since convexity of the cost function is not at all guaranteed.

Making use of gradient descent requires knowledge of the derivatives of
the cost function w.r.t to the variables to be optimized. In the case of
neural networks, we thus need to know the derivative w.r.t to each
weight and bias. A naive finite difference scheme would quickly grow
computationally untractable for even shallow networks. A solution to
this problem was found by Werbos in the form of the backpropagation
algorithm. Despite many years of ongoing research, it is still the go-to
algorithm for each neural network implementation.

\hypertarget{back-propagation-and-automatic-differentiation}{%
\subsubsection*{Back propagation and automatic
differentiation}\label{back-propagation-and-automatic-differentiation}}
\addcontentsline{toc}{subsubsection}{Back propagation and automatic
differentiation}

As we wish to minimize the cost function w.r.t. to each weight \(w\) and
bias \(b\) using gradient descent, we need to find the derivative of the
cost function w.r.t to each. Our argument simplifies if we move away
from vector notation and introduce \(w^l_{jk}\), the weight of the
\(j\)-th neuron in layer \(l-1\) to neuron \(k\) in layer \(l\) and
\(b^l_j\), the bias of the neuron \(j\) in the \(l\)-th layer. We
introduce the error of neuron \(j\) in layer \(l\) as:

\[
\delta^l_j=\frac{\partial C}{\partial z^l_j}
\]

We can rewrite this using the chain rule as:

\[
\delta^l_j = \sum_k \frac{\partial C}{\partial a^l_{jk}}\frac{\partial a^l_{jk}}{\partial z^l_{j}} 
\]

However, the second term is always zero except when \(j=k\), so the
summation can be dropped. Remembering eq.\ref{eq:activation}, we note
that \(\partial a^l_{jk}/\partial z^l_{j} = \sigma'(z^l_j)\). For the
last layer \(l = L\), the first term turns into the derivative of the
cost function, finally giving us:

\begin{equation}
\delta^L_j =  |a^L_j-y_j|\sigma'(z^L_j)
\label{eq:backprop1}\end{equation}

Equation eq.\ref{eq:backprop1} relates the error in the output layer to
its inputs. This in turn is a function of all the previous inputs and
errors and we thus need to find an expression relating the error in
layer \(l\) with the error in an layer \(l+1\). Since we have an
expression for the error in the last layer, we propagate the error going
down the layers, hence the name \emph{back}propagation. Again using the
chain rule gives:

\[
\delta^l_j = \sum_k \frac{\partial C}{\partial z^{l+1}_{jk}}\frac{\partial z^{l+1}_{jk}}{\partial z^l_{j}} = \sum_k \delta^{l+1}_k\frac{\partial z^{l+1}_{jk}}{\partial z^l_{j}}
\]

Using equation eq.\ref{eq:weighted_input}, we obtain after
substitution:

\begin{equation}
\delta^l_j = \sum_k\delta^{l+1}_kw^{l+1}_{kj}\sigma'(z^l_j)
\label{eq:backprop2}\end{equation}

Using equations eq.\ref{eq:backprop1} and eq.\ref{eq:backprop2} , we
can calculate the error in C due to each neuron. Finally, we need to
relate the error in each error to \(\partial C/\partial w^l_{jk}\) and
\(\partial C/\partial b^l_{j}\). Making use yet again gives us the last
two backpropagation relations:

\begin{equation}
\frac{\partial C}{\partial b^l_{j}}\frac{\partial b^l_{j}}{\partial z^l_{j}}=\frac{\partial C}{\partial b^l_{j}}=\delta^l_j
\label{eq:backprop3}\end{equation}

\begin{equation}
\sum_k\frac{\partial C}{\partial w^l_{jk}}\frac{\partial w^l_{jk}}{\partial z^l_{j}}=\delta^l_j\to \frac{\partial C}{\partial w^l_{jk}}=a^{l-1}_{j}\delta^l_j
\label{eq:backprop4}\end{equation}

Now that we now that all back propagation equations, we state the
algorithm. It consists of four steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Complete a forward pass, i.e., calculate the expected outcomes with
  the current weights and biases.
\item
  Calculate the error using eq.\ref{eq:backprop1} and do a backward
  pass to obtain the error in each neuron using eq.\ref{eq:backprop2}.
  This can be used to calculate the gradients using
  eq.\ref{eq:backprop3} and eq.\ref{eq:backprop4}
\item
  Adjust the weights and biases using the choosen optimizer
  (e.g.~gradient descent)
\item
  Return to step 1 until the optimization problem converges.
\end{enumerate}

Mathematically, back propagation is a special case of a technique known
as automatic differentiation. Automatic differentiation is a third type
of differentiation, next to numeric and symbolic. It allows for machine
precision calculation of derivatives by writing it as a chain of simple
operations combined with the chain rule, similar to backpropagation.
Note that:

\[
\delta^0_j = \frac{\partial C}{\partial x_j}\frac{\partial x_j}{\partial z^0_j}
\]

so that:

\[
\frac{\partial C}{\partial x_j} = a^0_j \delta^0_j 
\]

Thus neural networks also give us access to high precision derivatives
with regard to each coordinate.

\hypertarget{physics-informed-neural-networks-1}{%
\section{Physics Informed Neural
Networks}\label{physics-informed-neural-networks-1}}

On the face of things, the goal of physics and neural networks are
oppsite: whereas physics tries to build an understanding of things using
models to make predictions, neural networks learn a \emph{modelless}
mapping to make predictions. Recent advancements however have merged the
two approaches together in a concept known as Physics Informed Neural
Networks (45, 26). In this approach, we encode physical laws into the
network, so that the network respects the physics. This can be used to
both numerically solve equations or fit a model to spatiotemporal data.
Even more so, it should allow us to infer coefficient fields.

\hypertarget{the-concept-1}{%
\subsection{The concept}\label{the-concept-1}}

Consider a set of 1D+1 spatiotemporal data, consisting of some property
\(u(x,t)\) at coordinates \((x,t)\). The neural network can be learned
the underlying physics by minimizing the cost function:

\[
\mathcal{L} = \frac{1}{2n}\sum_i|u_i-a^L_i|^2
\]

The process of learning requires a lot of data and is prone to
overfitting. Now assume that we know that \(u(x,t)\) is governed by some
process which is written as a partial differential equation:

\[
\partial_t u = f(1, u, u_x, u_xx, u^2, ...)
\]

where \(f\) is a function of \(u\) or its spatial derivatives. Rewriting
it as:

\begin{equation}
g = 0 = -\partial_t u + f(1, u, u_x, u_xx, u^2, ...)
\label{eq:PIcost}\end{equation}

we see that in order to satisfy the PDE, \(g\to0\). The idea of PINNs is
to add this function \(g\) to the costfunction of the neural network:

\[
\mathcal{L} = \frac{1}{2n}\sum_i|u_i-a^L_i|^2 + \lambda\sum_i|g_i|^2 = MSE + \lambda PI
\]

where \(\lambda\) sets the effective strength of the two terms. Observe
that the cost function is higher if the PDE is not satisfied. Minimizing
the costfunction will thus mean minimizing \(g\) and hence satisfying
the PDE. We effectively penalize solutions not satisfying the physics we
put in equation eq.\ref{eq:PIcost}; the added term acts a
`physics-regularizer'. Concretely, the adding of physics contrains the
solution space, preventing overfitting and making the neural network
much more data efficient. The most useful feature however is that we
don't need a vast set of training data to train the network, as we solve
the problem \emph{by} training the network.

We can also remove the mean squared error term from the cost function
and add initial and boundary conditions, similar to the PI term. If we
now train the network, it will learn the solution to the given PDE
whilst respecting the given boundary and initial conditions. This
alternative means of numerically solving a model doesn't need advanced
meshing of the problem domain or carefully constructed (unstable)
discretization schemes, as it requires the physics to be fullfilled at
every point in the spatiotemporal domain. A useful analogy here is
calculating the trajectory of a launched object. A classical numerical
solver would take small steps in time, updating the position and speed
of the object each step. A PINN however uses a completely different
approach. Given the initial (random) state of the neural network, it
calculates a first trajectory and keeps adjusting the weights of the
network until the cost is minimized, i.e.~until we obtain a solution
satisfying the included physics and initial and boundary conditions. A
classical numerical approach tries once using a correct and methodical
approach, whereas a PINN tries many times until the result satisfies its
constraints.

We can also use this framework to fit models to spatiotemporal data by
letting the coefficient of each term be a variable to be minimized as
well. More concretely, where before the cost was a function of the
weights and biases, \(\mathcal{L}=f(w^l, b^l)\), we now let it be a
function of the coefficients \(\lambda\) of the PDE as well:
\(\mathcal{L}=f(w^l, b^l, \lambda_1, \lambda_2,...)\). This is shown for
several PDEs such as the Burgers, Schrodinger or Navier-Stokes equation
in the papers of M. Raissi(45, 26). In the case of the Navier-Stokes
equation, it's shown that it's also possible to infer the pressure
field, which appears as a separate term. This is achieved by adding
another output neuron to the PINN (shown in figure fig.\ref{fig:PINN}),
so that it predicts both the pressure and the flow. In theory it should
also be possible to infer spatially and temporally varying
\emph{coefficient} fields. We investigate this claim in the next
section.

\begin{figure}
\hypertarget{fig:PINN}{%
\centering
\includegraphics{source/figures/pdf/PINN.pdf}
\caption{Left panel: a normal single output PINN. Right panel: a
multi-output PINN. The network now also predicts the coefficients values
at each data point.}\label{fig:PINN}
}
\end{figure}

\hypertarget{pinns-in-practice}{%
\subsection{PINNs in practice}\label{pinns-in-practice}}

We now wish to evaluate the use of PINNs to analyze the RUSH data. Using
a diffusive process as a toy problem, we first show how PINNs are able
to accurately determine the diffusion constant, even in the presence of
noise. Next, we prove that PINNs are indeed capable of inferring
coefficient fields and finish by analyzing some parts of the RUSH data.

In our toy problem we have an initial concentration profile:

\[
c(x, 0) = e^{-\frac{(x-0.5)^2}{0.02}}
\]

diffusing in a 1D box according to:

\[
\frac{\partial c(x,t)}{\partial t} = \nabla \cdot[D(x)\nabla c(x,t)]
\]

on the spatial domain \([0,1]\) with perfectly absorbing boundaries at
the edges of the domain:

\[
c(0,t) = c(1,t) = 0
\]

If \(D(x) = D\), this problem has an analytical solution through a
Greens function. If the diffusion coefficient is spatially dependent
though, the problem needs to be solved numerically. The code used to
generate our data can be found in the appendix. Although this toy
problem is simple and in 1D, our results easily generalize to higher
dimenions and complexity at the cost of higher computational cost.

\hypertarget{constant-diffusion-coefficient}{%
\subsubsection{Constant diffusion
coefficient}\label{constant-diffusion-coefficient}}

We now consider the mentioned problem with a diffusion coefficient of
\(D(x) = D_0 = 0.1\) and simulate it between \(t=0\) and \(t=0.5\).
Using a spatial and temporal resolution of 0.01, we have a datagrid of
101 by 51, so that our total dataset consists of 5151 samples. The
neural network consists of 6 hidden layers of 20 neurons each and
\(\lambda=1\). Figure fig.\ref{fig:constantD} shows the ground truth
for the problem and the absolute error of the neural network.

\begin{figure}
\hypertarget{fig:constantD}{%
\centering
\includegraphics{source/figures/pdf/error_constantD.pdf}
\caption{\textbf{Left panel}: Simulated ground truth of the problem.
\textbf{Right panel}: The absolute error of neural network. Note that
most of the error is located at areas with low concentration,
i.e.~signal.}\label{fig:constantD}
}
\end{figure}

The predicted diffusion coefficient is \(D_{pred} = 0.100026\), giving
an error of \(0.026\%\). In 45, the authors obtain similar accuracies
for significantly more complex problems such as the Schrodinger
equation, which means that our accurate inference is not just due to the
simplicity of the problem. Furthermore, Raissi et al.~show that the
result is robust w.r.t the architecture of the network. From the
absolute error we observe that the error seems to be higher in areas
with low concentration. This is a feature we've consistently observed:
in areas with low `signal', the neural network seems to struggle.

As good as these results are, the input data is noiseless and thus of
limited practical interest. We now show that PINNs perform equally well
with noisy data by adding \(5\%\) white noise to the data and performing
the same procedure. The network is now doing two tasks in parallel: it's
both denoising the data and performing a model fit. In the left panel of
figure fig.\ref{fig:error_constantD_noisy} we show the concentration
profile at times \(t = 0, 0.1\) and \(0.5\), with the prediction of the
PINN superimposed in black dashed lines at each time. On the right panel
we show again the absolute error from the ground truth. Observe the
similarities with the noiseless case: most of the error localizes in
areas wit low concentration.

\begin{figure}
\hypertarget{fig:error_constantD_noisy}{%
\centering
\includegraphics{source/figures/pdf/error_constantD_noisy.pdf}
\caption{\textbf{Left panel}: The original noisy concentration profile
with the neural network inferred denoised version super imposed.
\textbf{Right panel}: The absolute error of neural network with respect
to the ground truth. Note that most of the error is located at areas
with low concentration.}\label{fig:error_constantD_noisy}
}
\end{figure}

The inferred diffusion constant is \(D_0 = 0.10052\), giving an error of
\(0.52\%\). Although the error is slightly higher than in the noiseless
version, it's extremely impressive that we obtain the diffusion constant
to this precision.

\hypertarget{varying-d}{%
\subsubsection{Varying D}\label{varying-d}}

As stated, it should be possible to infer coefficient fields by using a
two output neural network. One output predicts the concentration while
the other predicts the diffusion coefficient. Such a network is indeed
capable of generating the right coefficient field as shown in figure
fig.\ref{fig:summary_constantD} . Here the network has been trained on
the constant diffusion coefficient data we used before including \(5\%\)
white noise, so that we should observe a diffusion field constant at
\(D(x,t) = D_0 = 0.1\). In the upper left we show the data on which the
network is trained, with the upper right panel the predicted
concentration profile, which shows a very good match. In the lower right
panel we show the inferred diffusion field. We observe a good match in
the middle of the plot, but the neural network again struggles in areas
with low concentration, such as the lower left and right area. A more
quantitative analysis of the predicted diffusion and concentration is
presented in the lower left corner. Here we plot the Cumulative
Distribution Function (CDF) of the absolute relative error. Note that
the PINN predicts the concentration very well, but struggles more with
the diffusion coefficient. This is expected, as the mean squared error
of the cost function is quite explicit in its use of the concentration,
whereas the diffusion coefficient is determined self-consistently in the
PI part. We also observed similar but distinctive results in different
runs, owing to the non-convexity of the problem. Overall the result is
still remarkable, given that we've inferred a diffusion field from just
concentration data with \(5\%\) noise.

\begin{figure}
\hypertarget{fig:summary_constantD}{%
\centering
\includegraphics{source/figures/pdf/summary_constantD_varyingPINN.pdf}
\caption{We show the training data and predicted concentration profile
in the upper left and right panels. The lower right panel shows the
inferred diffusion field while the lower left panel shows the CDF of the
relative error of the diffusion and
concentration.}\label{fig:summary_constantD}
}
\end{figure}

In figure fig.\ref{fig:summary_varyingD} we show a similar analysis for
a non-constant diffusion field. We've simulated data with a diffusion in
the form of a \(\tanh(x)\). Remarkably, the neural network is able to
infer the field with reasonable accuracy, although it required a more
sizeable dataset of 50000 points. Figure fig.\ref{fig:projectionD}
studies the inferred diffusion profiles in depth by projecting them
along the time axis. Here we observe the strongest dissonance close to
the edges, which, again, is where the concentration is low. Nonetheless,
we've proven that a neural network is able to infer a coefficient field
with reasonable accuracy from noisy data!

\begin{figure}
\hypertarget{fig:summary_varyingD}{%
\centering
\includegraphics{source/figures/pdf/summary_varyingD_varyingPINN.pdf}
\caption{We show the training data and predicted concentration profile
in the upper left and right panels. The lower right panel shows the
inferred diffusion field while the lower left panel shows the CDF of the
relative error of the diffusion and
concentration.}\label{fig:summary_varyingD}
}
\end{figure}

\begin{figure}
\hypertarget{fig:projectionD}{%
\centering
\includegraphics{source/figures/pdf/projection.pdf}
\caption{Projection of the inferred diffusion profile along the time
axis.}\label{fig:projectionD}
}
\end{figure}

\hypertarget{real-cell}{%
\subsubsection{Real cell}\label{real-cell}}

\hypertarget{conclusion-1}{%
\section{Conclusion}\label{conclusion-1}}

\hypertarget{weak-points-and-how-to-improve}{%
\subsection{Weak points and how to
improve}\label{weak-points-and-how-to-improve}}

\hypertarget{introduction-to-golgi-as-a-phase-separated-droplet}{%
\chapter{Introduction to Golgi as a phase separated
droplet}\label{introduction-to-golgi-as-a-phase-separated-droplet}}

In this second part of the thesis we develop a model linkingthe Golgi
function and size to protein transport. Special attention will be
devoted to adding spatial dependence, as this is what current models are
lacking. IN the first section we justify our choice of a phase-separated
droplet from a biological perspective. In the second section we present
a short primer on phase-separation theory before presenting an
approximation which makes the theory analytically tractable. We end with
a section which sets up our model, which we solve and analyze in the
next chapter.

\hypertarget{phase-separation}{%
\section{Phase separation}\label{phase-separation}}

Consider a mixture of two molecules, type A and B. One is the solvent
and one the solute. One can then try and use statistical mechanics to
derive a mean-field model and get all sorts of cool results. This is
quite complex and not particulally enlightening for us. Instead, we
follow a method devised by Landau. We know that phase separation occurs
when the free energy has two convex region, so we can construct a free
energy density function \(f\) with two minima in a minimalist way as:

\[
f(c) = \frac{b}{2(\Delta c)^2}(c-c_0^-)^2(c-c_0^+)^2
\]

where \(b\) characterizes interactions, \(\Delta c = |c_0^--c_0^+|\),
and \(c_0^\pm\) is where the two minima of the function are located.
Figure shows this free enery function. A free energy like this leads to
phase separation into a dense and dilute phase. The interface has
surface tension, so when we calculate the full free energy of the system
we add a term for the interface:

\[
F(c) = \int dV (f(c)+\frac{1}{2}\kappa \nabla^2c)
\]

We thus have constructed a Landau free energy with order parameter c.
The first term accounts for the bulk energy, while the second term
penalizes interfaces. Suittable choices of parameter lead either to a
mixed state, where \(\bar{c}=constant\) in the whole system, or to a
phase separated state such as shown in figure. When we quench a system
from a mixed state into a phase-separated state, this happens through a
process called \emph{coarsening dynamics}. Basically small domains form
and these keep growing, showing a maze-like structure as in figure
\ldots{}. In liquid-liquid phase separation, the order parameter is
conserved, as the molecules cant just change. The phases then exchange
diffusion-like and can only exchange locally, so we state that:

\[
\partial_t c = -\nabla \cdot \mathbf{j}
\]

where \(\mathbf{j}\) is a flux. We know that \(\mathbf{j}\) is related
to the to the chemical potential through:

\[
j = -\Lambda \nabla \mu
\]

where \(\Lambda\) is an Onsager coefficient. We also know that
\(\mu = \delta F/ \delta \phi\) so that we end up with

\[
\frac{\partial \phi}{\partial t} = m \nabla^2 \frac{\delta F}{\delta \phi}
\]

where \(\delta/\delta \phi\) is a functional derivative. Given the
landau free energy in eq , we obtain:

\[
\frac{\partial \phi}{\partial t}=m\nabla^2[f'(\phi)-k\nabla^2\phi]
\]

which is known as the Cahn-Hilliard equation. The Cahn-hilliard equation
is what governs the maze domain evolution shown in figure and is the
basis for studies of phase separation. Due to its non-linearity its very
hard to use, and many times a scaling relation is derived. However,
since we want to include spatial inhomogenity, this probably will not
work. We present a different approach using effective droplets in the
next section.

\hypertarget{effective-droplet}{%
\section{Effective droplet}\label{effective-droplet}}

Consider phase separation in a one-dimensional box. The system will
separate into a dilute and a dense phase, separated by an interface of
width \(w\):

\[
c^*(x) = \frac{c_0^-+c_0^+}{2}+\frac{c_0^+-c_0^-}{2}\tanh\left(\frac{x}{w}\right)
\]

The idea of the effective droplet model is that if the width of the
interface \(w\) is very small, we neglect the interfacial contribution
and describe the system as two separate bulk phases. In each bulk phase
we then solve a linearized version of the Cahn-Hilliard equation and
match the solutions at the interface of the phases. Growth of the
droplet can then be described in terms of fluxes across the interface.
We show this in figure fig.\ref{fig:eff_droplet}.

We first linearize the Cahn-Hilliard equation in the bulk phases.
Consider again the Cahn-Hilliard equation, where we've changed the order
parameter \(\phi\) to a concentration \(c\):

\[
\frac{\partial c }{\partial t} = -m\nabla\mathbf{J}
\] \[
\mathbf{J} = -\nabla \mu
\] \begin{equation}
\mu = f'(c) - k\nabla^2c
\label{eq:CH_bray}\end{equation}

where \(f'(c)=\partial f/\partial c\). We're assuming an infinitely thin
interface, so the interfacial term is neglegible. Linearizing the
chemical potential in \(c\) around the dense phase yields
\(\mu = f''(c_0^+)c\), so that eq.\ref{eq:CH_bray} becomes:

\begin{equation}
\frac{\partial c }{\partial t} = D\nabla^2c
\label{eq:diffusion}\end{equation}

where we've replaced all the constant in front of the right term by
\(D\). We thus see that linearizing the Cahn-Hilliard equation in the
bulk leads to a diffusion equation. The effective droplet theory is most
accurate in predicting steady states and thus the time-dependence of
eq.\ref{eq:diffusion} is ignored. As we've replaced a single equation
by two, we need an extra set of boundary conditions at the interface.

\hypertarget{boundary-conditions}{%
\subsection{Boundary conditions}\label{boundary-conditions}}

To determine the droplet boundary conditions, consider a system which is
phase separated and has an infinitely thin interface. The total free
energy of the system is then:

\[
F = V_1 f(\phi_1) + V_2 f(\phi_2)
\]

where \(V_i\) and \(\phi_i\) are respectively the volume and density of
phase \(i\) and \(f(\phi_i)\) is the free energy density. In
equilibrium, this free energy is minimized. Assuming incompressibility
(\(V_1+V_2=V\)) and conservation of particles
(\(V_1\phi_1+V_2\phi_2=V\phi\)), we further constrain the system to two
independent variables. Choosing to minimize the free energy with respect
to \(\phi_1\) and \(V_1\) gives us two conditions:

\[
f'(\phi_1) = f'(\phi_2) = 0
\]

\[
0 = f(\phi_1) + f(\phi_2) + (\phi_2-\phi_1)f'(\phi_2)
\]

Since \(f'(\phi) = \mu(\phi)\), the first condition states that both
phases must have the same chemical potential, while the second one
refers to the matching of osmotic pressure. One obvious solution to
these equations is a completely mixed state with \(\phi_1=\phi_2\). A
non-trivial (phase-separated) solution exists as well, where \(\phi_1\)
and \(\phi_2\) are the two minima of the free energy density function
\(f(\phi)\). In our system, this corresponds to \(c_0^+\) and \(c_0^-\).
Assuming that our droplet is locally in thermodynamic equilibrium, we
can apply this solution as boundary conditions. Concretely,

\[
c(R) = 
\begin{cases}
    c_0^-,& \text{Outside droplet}\\
    c_0^+,& \text{Inside droplet}
\end{cases}
\]

Now that we have defined boundary conditions, equations such as
eq.\ref{eq:diffusion} can be solved. The solution will give us the
concentration profile in each phase, but the droplet behaviour is
determined by the fluxes across the interface. We show this in the next
section.

\hypertarget{fluxes-and-movement-of-interfaces}{%
\subsection{Fluxes and movement of
interfaces}\label{fluxes-and-movement-of-interfaces}}

Given a concentration profile \(c(x)\), the (diffusive) flux can be
calculated by applying Ficks' law:

\[
J(x) = -D\frac{\partial c}{\partial x}
\]

Using this expression, we can calculate a flux on the inside of the
interface \(J_{in}\) and on the outside of the interface \(J_{out}\).
Note that in and out respectively refer to inside and outside the
droplet and not to the direction of the flux. Figure \textbf{???}:
eff\_droplet shows a typical concentration profile of an active droplet.
Observe that due to the activity of the droplet, the concentration
profile inside the droplet is not flat but convex. Due to this
convexity, the flux \(J_{in}\) will be pointed inwards, while the flux
\(J_{out}\) will also be pointed inwards typically due to transport or
production. Intuitively, \(J_{in}\) is the flux lost due to the decay or
activity in the droplet, while \(J_{out}\) replaces the converted
material inside with material from outside. A steady-state can be
achieved once these two fluxes are equal. Note that while the system is
at steady state, it is not in equilibrium, as the fluxes across the
interface are not zero; the fluxes are balanced, rather than
equilibrated. This is a typical characteristic of an active system.

\begin{figure}
\hypertarget{fig:interfacespeed}{%
\centering
\includegraphics{source/figures/pdf/interface.pdf}
\caption{Figure illuminating the relation between a moving interface and
the fluxes across it.}\label{fig:interfacespeed}
}
\end{figure}

If the fluxes \(J_{in}\) and \(J_{out}\) are not balanced, a net flux
exists, leading to either growth or decay of the droplet. We wish to
derive an expression for interface speed \(v_n\) in terms of the fluxes
across it. Consider figure fig.\ref{fig:interfacespeed}. If we wish to
move the interface a distance \(\Delta x\), we need a net material gain
of \(\Delta x \Delta c\). This net gain is supplied by a net flux in a
time \(\Delta t\), so that:

\[
\Delta x \Delta c = (J_{in}-J_{out})\Delta t
\]

which can be rewritten as:

\begin{equation}
\frac{\Delta x}{\Delta t} = v_n = \frac{J_{in}-J_{out}}{\Delta c}
\label{eq:interfacespeed}\end{equation}

Thus if \(|J_{in}|<|J_{out}|, v_n>0\) and the interface will move to the
right. If we consider a `free' droplet (with two interfaces in 1D), the
radius of the droplet is determined by both interfaces. Since the fluxes
need not be similar on both sides of the droplet, not only the radius
but also the position of interfaces plays a role. We thus characterize
the droplet in terms of its radius \(R\) and its geometric center
\(x_0\). Consider a droplet of radius \(R\) at position \(x_0\). The
left and right interfaces move respectively with velocities \(v_l\) and
\(v_r\). In a time \(dt\), the droplet than moves to a new postion
\(x_0+dx\) and will have a new radius \(R+dR\):

\[
x_0-R+v_ldt=x_0+dx-(R+dR)
\] \[
x_0+R+v_rdt=x_0+dx+(R+dR)
\]

Solving this set of equations for \(dx\) and \(dR\) gives:

\begin{equation}
\frac{dR}{dt}=\frac{1}{2}(v_r-v_l)
\label{eq:radius}\end{equation}

\begin{equation}
\frac{dx_0}{dt}=\frac{1}{2}(v_l+v_r)
\label{eq:position}\end{equation}

Equations eq.\ref{eq:radius} and eq.\ref{eq:position} give the change
in radius and position of the droplet. Note that these results are
quasi-steady state: we've calculated the fluxes from stationary
concentration profiles. Thus equations eq.\ref{eq:radius} and
eq.\ref{eq:position} are best used to calculate (quasi-) steady states
and not dynamics. Nevertheless, equation eq.\ref{eq:position} shows an
interesting feature: active droplets can move on their own. This
movement is the result of an imbalance in fluxes between the left and
right interface: the droplet center is displaced because one side of the
droplet grows faster than the other. This imbalance is caused by a
concentration gradient and droplets will move up the gradient, as the
flux on the high concentration side will be higher than on the low side.
Finally combining eq. eq.\ref{eq:interfacespeed} with eqs.
eq.\ref{eq:radius} and eq.\ref{eq:position} yields the position and
radius of the droplet in terms of the fluxes across its interface:

\[
\frac{dR}{dt}=\frac{1}{2\Delta c}\left[(J_{in}^{x=R}-J_{in}^{x=-R})+(J_{out}^{x=-R}-J_{out}^{x=R})\right] 
\]

\[
\frac{dx_0}{dt}=\frac{1}{2\Delta c}\left[(J_{in}^{x=-R}+J_{in}^{x=R})-(J_{out}^{x=-R}+J_{out}^{x=R})\right]
\]

This completes the effective droplet model. It allows us to rewrite the
non-linear Cahn-Hilliard equation into two or mroe separate linear
problems with proper boundary conditions. Furthermore, these linear
problems can be easily extended, such as by adding decay, sources or
advection and we will do so in the next section. To solve a typical
problem with the effective droplet theory, we thus perform the following
steps:

\begin{itemize}
\tightlist
\item
  Identify the domains and correct equations.
\item
  Solve equations inside each domain to obtain concentration profile.
\item
  Calculate the fluxes across the interfaces.
\item
  Obtain steady states at the balance of fluxes.
\end{itemize}

In the next section we present our model for the Golgi in the effective
droplet model.

\hypertarget{golgi-as-an-active-droplet}{%
\section{Golgi as an active droplet}\label{golgi-as-an-active-droplet}}

\begin{itemize}
\tightlist
\item
  The Golgi is able to form de novo (Bevis)
\item
  Microtubules position the golgi near the mtoc (Sengupta)
\item
  Nocadazola depolymerizes microtubules -\textgreater{} golgi stacks
  move towards ERES (sengupta)
\item
  Golgi size is dependent on amount of trafficking (Sengupta)
\item
  Separate ministacks seems to be fully functional (Wei)
\item
  Golgi disassembly is due to imbalanced trafficking: exit from ER is
  blocked, while outflow still continues (Ronchi)
\item
  When golgi is completely cut out, stack like struct
\end{itemize}

In this section we introduce our model for Golgi biogenesis and
maintenance. Biologically, our problem encompasses four different
populations: the immature cargo, the mature cargo, the Golgi, with all
dissolved in the cytoplasm. The immature cargo transforms into the Golgi
material, which matures into mature cargo. Similar to \textbf{ref}, we
model this however in a simple manner with just the immature cargo phase
separating in a dilute and dense phase and the mature cargo implicitly.

As stated in the previous chapter, once the microtubules are
depolymerized, the Golgi ribbon breaks into separate stacks which
colocate with the ERES. If we model not the complete Golgi but a single
stack, we can reduce our problem to 1D, where a droplet can move from
one side of the system, representing the Golgi ribbon, to the other
side, representing the ERES. This simplification permits analytical
tractability. We thus end up with a 1D box where the dilute phase
represents the cytoplasm, while the dense phase represents a single
Golgi stack.

Proteins exiting the ERES are transported towards the ER over the
microtubules. This is a stochastic process with the proteins detaching
from and re-attaching to the microtubules randomly. \textbf{ref} shows
that such a stochastic process can reduce to a Fokker-Planck equation if
the attaching and detaching is much faster than the transport itself. As
the Fokker-Planck equation is functionally equivalent to an
advection-diffusion equation, we hypothesize that we can model protein
transport using an advection-diffusion equation. Furthermore,
\textbf{ref} reports that cargo can disappear without reaching the Golgi
and we thus add a decay term to the advection-diffusion equation,
resulting in the final equation for the evolution of dilute phase:

\begin{equation}
D\partial_x^2 c(x) - v\partial_xc(x)-ac(x)=0
\label{eq:cinside}\end{equation}

We will neglect the term on the left as we're working in a quasi-static
limit. The first term on the right is a diffusive, the second term
advective with advection velocity \(v\), while the last term represents
the decay.

\textbf{ref} states that after stack-like structures are formed close to
the ERES, they are transported to the ribbon by microtubules. Hence we
use an advective term next to a diffusive term in the dense phase. As
the dense phase represents the Golgi, we need a term to account for
maturation of the proteins. Much discussion on this subject exists (see
maternal vs.~cisternal maturation), but we choose a simple decay-like
term. This gives an expression very similar to the dilute phase
(eq.\ref{eq:cinside}), save for a different decay rate:

\begin{equation}
D\partial_x^2 c(x) - v\partial_xc(x)-kc(x)=0
\label{eq:coutside}\end{equation}

Note that we assume a similar diffusion coefficient in the dense and
dilute phase. Choosing different diffusion coefficients would result in
slighly different length scales (see next section), but would not affect
the main results. As stated, we also model the mature population
implicitly, in this case having the effect that the cargo lost due to
the decay term exits the system.

We now turn to the boundary values. As demanded by the effective droplet
model, we set at the interface between the droplet and the dilute phase:

\[
c(x_0\pm R)=
\begin{cases}
    c_0^-,& \text{inside of interface}\\
    c_0^+,& \text{outside of interface}
\end{cases}
\]

Our system is a 1D box of length \(L\), with the left boundary
representing the ERES, modeled as a source, and the right boundary a
zero-flux boundary, so that all cargo exits the system either through
decay in the dilute phase or maturation in the dense phase:

\[
(-D\partial_xc+vc)|_{x=0} = J_{in}
\]

\[
(-D\partial_xc+vc)|_{x=L} = 0
\]

We solve this set of equations in the next section.

\hypertarget{golgi-model}{%
\chapter{Golgi model}\label{golgi-model}}

In the previous chapter we introduced the Cahn-Hilliard equation. Its
non-linearity makes it extremely hard to study and in this chapter we
present an approximation known as effective droplet theory. In this
chapter we introduce the effective droplet approximation, discuss our
models for the Golgi apparatus and show the results for these models.
We've divided the chapter into three sections:

\begin{itemize}
\tightlist
\item
  \textbf{Effective droplet theory -} In this section we introduce
  effective droplet theory.
\item
  \textbf{Golgi as an effective droplet-} Here we introduce our model
  and show
\item
  \textbf{Two-component model-} This third section contains a possible
  extension of the effective droplet model to two-components.
\end{itemize}

We end the chapter with a short summary of our conclusions.

\hypertarget{solution}{%
\subsection{Solution}\label{solution}}

The general solution of equations eq.\ref{eq:cinside} and
eq.\ref{eq:coutside} is given by:

\[
c(x) = C_1e^{-\frac{x}{l^-}}+C_2e^{\frac{x}{l^+}}
\]

where we have defined \(l^\pm\) as:

\begin{equation}
l^\pm= \frac{2D}{\sqrt{4kD+v^2}\pm v }
\label{eq:lengthscale}\end{equation}

where \(k\) should be replaced by \(a\) outside the droplet. Note that
\(l^\pm\) defines the lengthscale of the problem. Without advection it
simplifies to \(\sqrt{D/k}\) and both lengthscales becomes similar. It
is here we see that the advection combined with decay leads to some sort
of `symmetry-breaking' in the droplet and that the effect of advection
is more than just translating the droplet. We study this is in the next
section. Applying the boundary conditions and calculating the flux at
position \(x_0+R\) and \(x_0-R\) gives the following fluxes outside:

\begin{equation}
J_{out}^{x=-R} = J_{in}\frac{(1+\frac{l_-}{l_+})e^{\frac{-(x_0-R)}{l_-}}}{Pe_-+Pe_+\frac{l_-}{l_+}e^{\frac{-x_1}{l}}}
+\frac{c_0^{out}D}{l_+}\frac{Pe_+(1-e^{\frac{-x_1}{l}})}{1+\frac{l_-}{l_+}\frac{Pe_+}{Pe_-}e^{\frac{-x_1}{l}}}
\label{eq:leftflux}\end{equation}

\begin{equation}
J_{out}^{x=R} = -c_0^{out}D\frac{Pe_-Pe_+(1-e^{\frac{-x_2+L}{l}})}{l_+Pe_-+e^{\frac{-x_2+L}{l}}l_-Pe_+}
\label{eq:rightflux}\end{equation}

where we have introduced the coordinates \(x_1\) and \(x_2\), which are
defined respectively as \(x_0\pm R\) and correspdon to the left and
right interface. We have also defined the Peclet-like numbers
\(Pe^\pm\):

\[
Pe^\pm = 1 \mp \frac{vl^\pm}{D}
\]

and a new combined lengthscale
\(l = \frac{l^+l^-}{l^++l^-} = 1/l^++1/l^-\). The flux on the left of
the droplet consists of two terms, with the first one accounting for the
influx and the second one for the interface with the droplet. The flux
on the right is clearly similar, but lacks a second term since we've set
a zero-flux boundary at \(x=L\).

We now turn to the fluxes on the inside of the droplet. In this case
however, the fluxes on the left and right side of the droplet are not of
particular importance; considering equations eq.\ref{eq:radius} and
eq.\ref{eq:position}, we're interested in their sum and difference.
Introducing \(J_{Rad} = J_{in}^{x=R}-J_{in}^{x=-R}\) and
\(J_{Pos} = J_{in}^{x=R}+J_{in}^{x=-R}\), we obain:

\[
J_{rad} = \frac{-2c_0^+D}{l}\frac{\sinh\frac{R}{l^-}\sinh\frac{R}{l^+}}{\sinh\frac{R}{l}}
\]

\[
J_{pos} = 2c_0^{in}D\left[\frac{Pe_-}{l_-}\frac{\sinh\frac{R}{l_+}\cosh\frac{R}{l_-}}{\sinh\frac{R}{l}}-\frac{Pe_+}{l_+}\frac{\sinh\frac{R}{l_-}\cosh\frac{R}{l_+}}{\sinh\frac{R}{l}}\right]
\]

The equation for the radius can also be interpreted as the maturation
flux. It contains an important point with respect to the validity of
effective droplet theory. Consider the limit of \(R\to\infty\):

\begin{equation}
\lim_{R\to\infty} = -2c_0^+\sqrt{kD}
\label{eq:radiuslimit}\end{equation}

where for simplicity we have set the advection speed \(v\) to zero. For
an active droplet one would expect the maturation flux to scale with the
radius. However, equation eq.\ref{eq:radiuslimit} shows that the
maturation flux saturates. The answer to this conundrum is found by
studying the concentration profile in the droplet. For \(R\gg l\), the
concentration in the center of the drop goes to zero, which is
unphysical and an artifact of the effective droplet model. Thus
effective droplet theory is only valid in the region \(R\ll l\). When
advection is added back into the mix this point is slightly more
nuanced, as advection changes the concentration profile. We show this in
the next section.

The equation for the positional flux hints at the effects of the
advection and maturation. In a droplet without decay, the concentration
profile would be flat and the positional flux would be proportional to
\(c_0^+v\). Equation tells a different story. We investigate this in
depth in the next section.

\hypertarget{no-decay-in-the-dilute-phase-the-effect-of-advection}{%
\subsection{No decay in the dilute phase: the effect of
advection}\label{no-decay-in-the-dilute-phase-the-effect-of-advection}}

We first study the effect of advection on the droplet by setting the
decay in the dilute phase to zero. In this case the flux on the outside
of the droplet becomes position-independent:

\[
J_{in}^{x=-R} = J_{in}
\]

\[
J_{out}^{x=R} = 0
\]

This is expected, as the cargo can only exit the system by decaying
inside the droplet. The equationa for the flux in the droplet remain
unchanged because they are independent of the decay outside. Developing
the internal droplet fluxes for \(R\ll l^\pm\) gives:

\[
J_{rad}\approx -2 c_0^+kR
\]

\[
J_{pos}\approx
\]

Combining these equations gives us expressions for the growth rate and
speed of the droplet:

\[
\frac{dR}{dt}=
\]

\[
\frac{dx_0}{dt}=
\]

Armed with these equations we can study the effect of advection. In
figure .. we show the minimum concentration, \(dx_0/dt\) and \(dR/dt\)
as a function of \(R\) and \(v\).


In the left panel we show the minimum concentration as a function of
\(R\) and \(v\). The white line shows the concentration corresponding to
\(l=0.5\) at \(v=0\) and we indeed see that advection increases the
minimum concentration. In the middle panel we show the increase in
radius \(dR/dt\) as a function of again \(R\) and \(v\). Red colors mean
positive, blue negative and white zero, so we've plotted with x's the
line \(dRdt=0\), which corresponds to a stable droplet. On the right we
show \(dx_0/dt\); where the \(dx_0/dt=0\) line crosses with the
\(dR/dt=0\) line there's a droplet with stable radius and position.

To show this more clearly, we plot the droplet radius and minimum
concentration as a function of the advection velocity in figure .
Observe that advection decreases the droplet radius by almost \(10\%\),
while simultaneously also increasing the minimum concentration. The
advection is thus `compacting' the droplet. By what process is this
happening? To see this, consider an active droplet without advection.
The concentration profile is symmetric and convex; the flux on both
sides due to diffusion is inwards and thus in opposite directions. If we
now turn on advection, this symmetry is broken. On the left side of the
droplet, the diffusion and advection point in the same direction but on
the other side they are aligned opposite. This causes an assymetry in
the concentration and a difference in the flux to the centre: more goes
to it from the left than is taken away by the right, hence compacting
the droplet.

Can advection stabilize a droplet? Zwicker et al show that an active
droplet such as ours can divide through a shape instability. Our model
is 1D however and not easily scalable to higher D's because of the
advection. Simulations would be needed to state anything on this.

We can make a few conclusions about the situation. For a very specific
set of parameters, the droplet can both be stable in radius and
position. There's no position dependence so not that interesting. Also
observe that there are only two situations: either the droplet is moving
to the left infinitely, or moving to the right infinitely, as there's no
stable position on either side. Which situation happens is controlled by
the gradient of the concentration versus the advection. note that we
need a non-zero convection to move the droplet from one mode to the
other. The explanation is simple: the convection needs to overcome the
gradient to reverse droplet movement. At the inflection point,
\(dx_0/dt=0\).

We now make some very speculative biological connection to the
observation of Golgi properties. A strong marker is that once the
microtubules are depolymerized, the stacks move away from the
perinuclear area and colocate with the ERES. This is what we see when we
turn of the advection, the droplet moves up the gradient to the source.
A similar thing is seen with biogenesis, the stacks are made around the
ERES but need microtubules to be transported to the center.

We have a model which described some features but its unstable. Can we
make some position in our system inherently stable? We try in the next
section by adding decay to the dilute phase.

\hypertarget{decay-in-the-dilute-phase}{%
\subsection{Decay in the dilute phase}\label{decay-in-the-dilute-phase}}

We now wish to find a model in which the position has a stable position.
So not just a set of parameters where \(dx_0/dt=0\), but a position
\(x_0\) where the droplet is stable. The flux on the inside is
independent of the position of the droplet, so to get a stable position
we need a position dependent flux on the outside. To this end, we
introduce a decay term in the dilute phase. We then end up with the full
equations \ldots{} . We first numerically solve these equations before
attempting an approximation. The result is shown in figure \ldots{} and
shows a `phase-diagram' of our active droplet model. In the left panel
we show the minimum concentration in the droplet, while the middle and
right panel shows the numerical value of \(dx_0/dt\) and \(d_R/dt\), all
as a function of the CoM position \(x_0\) and radius \(R\). From the
right two plots we've extracted the lines \(dx_0/dt=0\) and \(dR/dt=0\),
which we've superimposed on all three panels. Where these lines cross a
stable state exist. We first discuss each line independently and then
discuss the stable points.

First consider the line \(dR/dt=0\), a droplet with stable radius. We
observe that left of the line \(dRdt>0\), with the opposite on the
right. This means that each radius is stable: a perturbation w.r.t to
the radius will not propagate. The \(dx_0dt=0\) is more interesting. It
has the apperance of a `finger' and has a stable and unstable branch.
The arm below the point is stable, where the arm above the point is not.
This has important implications for the crossings: the lower
intersection is stable w.r.t. to all perturbations, while the upper
intersection is unstable. Pertubations above the dx0dt line will
propagat, while perturbations below will move the droplet to the lower
intersection.

This is all fine and dandy, but there's one problem: the lower
intersection, the stable point, is unphysical. We characterize the
droplet in terms of its center of mass \(x_0\) and radius \(R\), meaning
that the left interface is at a position \(x_0-R\). The red line in the
plots shows the line \(x_0=R\) and everything below this line is
unphysical, as it means the left interface is at a position \(x_0-R<\),
past the system edge. Why does this happen? First, in a dynamical
description this wouldn't happen, as the left interface boundary
condition prevents moving past. In a static description this can however
since we specify the edge of the system as a flux boundary condition.
Inspection of the concentration (see figure ) learns that the droplet
moves on top of the source: the flux inside the droplet at a point is
similar to the boundary condition. In 2D this wouldnt be a problem but
in our 1D description it is as the BC is specified as a flux and not as
a source. That means we're left with just an unstable point. Why is this
point unstable? Investigating the fluxes shows that despite the decay
the left interface is an order of magnitude bigger than the right
interface, thus behaviour is mainly determined by the left interface.
Once the left interface starts moving due to a perturbation, the rest
follows.

\hypertarget{two-component-model}{%
\section{Two-component model}\label{two-component-model}}

Two components jeeej

\hypertarget{conclusion-2}{%
\section{Conclusion}\label{conclusion-2}}

\hypertarget{conclusion-3}{%
\chapter{Conclusion}\label{conclusion-3}}

So what can we say about our project? We tried a lot but nothing worked
ha-ha.

\hypertarget{appendix-1-some-extra-stuff}{%
\chapter*{Appendix 1: Some extra
stuff}\label{appendix-1-some-extra-stuff}}
\addcontentsline{toc}{chapter}{Appendix 1: Some extra stuff}

Add appendix 1 here. Vivamus hendrerit rhoncus interdum. Sed ullamcorper
et augue at porta. Suspendisse facilisis imperdiet urna, eu pellentesque
purus suscipit in. Integer dignissim mattis ex aliquam blandit.
Curabitur lobortis quam varius turpis ultrices egestas.

\footnotesize

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-emr_journeys_2009}{}%
1. Emr, S. \emph{et al.} Journeys through the Golgi---taking stock in a
new era. \emph{The Journal of Cell Biology} \textbf{187,} 449--453
(2009).

\leavevmode\hypertarget{ref-tang_cell_2013}{}%
2. Tang, D. \& Wang, Y. Cell cycle regulation of Golgi membrane
dynamics. \emph{Trends in Cell Biology} \textbf{23,} 296--304 (2013).

\leavevmode\hypertarget{ref-rothman_future_2010}{}%
3. Rothman, J. E. The Future of Golgi Research. \emph{Molecular Biology
of the Cell} \textbf{21,} 3776--3780 (2010).

\leavevmode\hypertarget{ref-gosavi_function_2017}{}%
4. Gosavi, P. \& Gleeson, P. A. The Function of the Golgi Ribbon
Structure - An Enduring Mystery Unfolds! \emph{BioEssays} \textbf{39,}
1700063 (2017).

\leavevmode\hypertarget{ref-budnik_er_2009}{}%
5. Budnik, A. \& Stephens, D. J. ER exit sites - Localization and
control of COPII vesicle formation. \emph{FEBS Letters} \textbf{583,}
3796--3803 (2009).

\leavevmode\hypertarget{ref-bressloff_stochastic_2013}{}%
6. Bressloff, P. C. \& Newby, J. M. Stochastic models of intracellular
transport. \emph{Reviews of Modern Physics} \textbf{85,} 135--196
(2013).

\leavevmode\hypertarget{ref-wei_unraveling_2010}{}%
7. Wei, J.-H. \& Seemann, J. Unraveling the Golgi Ribbon. \emph{Traffic}
\textbf{11,} 1391--1400 (2010).

\leavevmode\hypertarget{ref-ronchi_positive_2014}{}%
8. Ronchi, P., Tischer, C., Acehan, D. \& Pepperkok, R. Positive
feedback between Golgi membranes, microtubules and ER exit sites directs
de novo biogenesis of the Golgi. \emph{Journal of Cell Science}
\textbf{127,} 4620--4633 (2014).

\leavevmode\hypertarget{ref-newby_quasi-steady_2010}{}%
9. Newby, J. M. \& Bressloff, P. C. Quasi-steady State Reduction of
Molecular Motor-Based Models of Directed Intermittent Search.
\emph{Bulletin of Mathematical Biology} \textbf{72,} 1840--1866 (2010).

\leavevmode\hypertarget{ref-newby_random_2010}{}%
10. Newby, J. \& Bressloff, P. C. Random intermittent search and the
tug-of-war model of motor-driven transport. \emph{Journal of Statistical
Mechanics: Theory and Experiment} \textbf{2010,} P04014 (2010).

\leavevmode\hypertarget{ref-staehelin_nanoscale_2008}{}%
11. Staehelin, L. A. \& Kang, B.-H. Nanoscale Architecture of
Endoplasmic Reticulum Export Sites and of Golgi Membranes as Determined
by Electron Tomography. \emph{PLANT PHYSIOLOGY} \textbf{147,} 1454--1468
(2008).

\leavevmode\hypertarget{ref-alberts_molecular_nodate}{}%
12. Alberts. \emph{Molecular biology of the Cell}.

\leavevmode\hypertarget{ref-griffiths_rubisco_nodate}{}%
13. Griffiths, H. Rubisco is said to be both the most important enzyme
on Earth and surprisingly inefficient. Yet an understanding of the
reaction by which it fixes CO2 suggests that evolution has made the best
of a bad job. 2

\leavevmode\hypertarget{ref-glick_models_2011}{}%
14. Glick, B. S. \& Luini, A. Models for Golgi Traffic: A Critical
Assessment. \emph{Cold Spring Harbor Perspectives in Biology}
\textbf{3,} a005215--a005215 (2011).

\leavevmode\hypertarget{ref-hirschberg_kinetic_1998}{}%
15. Hirschberg, K. \emph{et al.} Kinetic Analysis of Secretory Protein
Traffic and Characterization of Golgi to Plasma Membrane Transport
Intermediates in Living Cells. \emph{The Journal of Cell Biology}
\textbf{143,} 1485--1503 (1998).

\leavevmode\hypertarget{ref-boncompain_synchronization_2012}{}%
16. Boncompain, G. \emph{et al.} Synchronization of secretory protein
traffic in populations of cells. \emph{Nature Methods} \textbf{9,}
493--498 (2012).

\leavevmode\hypertarget{ref-hyman_liquid-liquid_2014}{}%
17. Hyman, A. A., Weber, C. A. \& Jülicher, F. Liquid-Liquid Phase
Separation in Biology. \emph{Annual Review of Cell and Developmental
Biology} \textbf{30,} 39--58 (2014).

\leavevmode\hypertarget{ref-zwicker_centrosomes_2014}{}%
18. Zwicker, D., Decker, M., Jaensch, S., Hyman, A. A. \& Jülicher, F.
Centrosomes are autocatalytic droplets of pericentriolar material
organized by centrioles. \emph{Proceedings of the National Academy of
Sciences} \textbf{111,} E2636--E2645 (2014).

\leavevmode\hypertarget{ref-de_vos_seeing_2016}{}%
19. Sbalzarini, I. F. Seeing Is Believing: Quantifying Is Convincing:
Computational Image Analysis in Biology. in \emph{Focus on Bio-Image
Informatics} (eds. De Vos, W. H., Munck, S. \& Timmermans, J.-P.)
\textbf{219,} 1--39 (Springer International Publishing, 2016).

\leavevmode\hypertarget{ref-hutchison_computational_2014}{}%
20. Chen, K.-C., Qiu, M., Kovacevic, J. \& Yang, G. Computational Image
Modeling for Characterization and Analysis of Intracellular Cargo
Transport. in \emph{Computational Modeling of Objects Presented in
Images. Fundamentals, Methods, and Applications} (eds. Hutchison, D. et
al.) \textbf{8641,} 292--303 (Springer International Publishing, 2014).

\leavevmode\hypertarget{ref-lee_image-based_2015}{}%
21. Lee, H.-C. \& Yang, G. An image-based computational method for
characterizing whole-cell scale spatiotemporal dynamics of intracellular
transport. in \emph{2015 IEEE 12th International Symposium on Biomedical
Imaging (ISBI)} 699--702 (IEEE, 2015).
doi:\href{https://doi.org/10.1109/ISBI.2015.7163969}{10.1109/ISBI.2015.7163969}

\leavevmode\hypertarget{ref-yang_bioimage_2013}{}%
22. Yang, G. Bioimage informatics for understanding spatiotemporal
dynamics of cellular processes: Spatiotemporal dynamics of cellular
processes. \emph{Wiley Interdisciplinary Reviews: Systems Biology and
Medicine} \textbf{5,} 367--380 (2013).

\leavevmode\hypertarget{ref-hebert_spatiotemporal_2005}{}%
23. Hebert, B., Costantino, S. \& Wiseman, P. W. Spatiotemporal Image
Correlation Spectroscopy (STICS) Theory, Verification, and Application
to Protein Velocity Mapping in Living CHO Cells. \emph{Biophysical
Journal} \textbf{88,} 3601--3614 (2005).

\leavevmode\hypertarget{ref-kisley_characterization_2015}{}%
24. Kisley, L. \emph{et al.} Characterization of Porous Materials by
Fluorescence Correlation Spectroscopy Super-resolution Optical
Fluctuation Imaging. \emph{ACS Nano} \textbf{9,} 9158--9166 (2015).

\leavevmode\hypertarget{ref-semrau_particle_2007}{}%
25. Semrau, S. \& Schmidt, T. Particle Image Correlation Spectroscopy
(PICS): Retrieving Nanometer-Scale Correlations from High-Density
Single-Molecule Position Data. \emph{Biophysical Journal} \textbf{92,}
613--621 (2007).

\leavevmode\hypertarget{ref-raissi_physics_2017}{}%
26. Raissi, M., Perdikaris, P. \& Karniadakis, G. E. Physics Informed
Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial
Differential Equations. \emph{arXiv:1711.10566 {[}cs, math, stat{]}}
(2017).

\leavevmode\hypertarget{ref-barron_performance_1994}{}%
27. Barron, J. L., Fleet, D. J. \& Beauchemin, S. S. Performance of
Optical Flow Techniques. 60 (1994).

\leavevmode\hypertarget{ref-dong_motion_2006}{}%
28. Dong, G., Baskin, T. I. \& Palaniappan, K. Motion Flow Estimation
from Image Sequences with Applications to Biological Growth and
Motility. in \emph{2006 International Conference on Image Processing}
1245--1248 (IEEE, 2006).
doi:\href{https://doi.org/10.1109/ICIP.2006.312551}{10.1109/ICIP.2006.312551}

\leavevmode\hypertarget{ref-vig_quantification_2016}{}%
29. Vig, D. K., Hamby, A. E. \& Wolgemuth, C. W. On the Quantification
of Cellular Velocity Fields. \emph{Biophysical Journal} \textbf{110,}
1469--1475 (2016).

\leavevmode\hypertarget{ref-garcia_robust_2010}{}%
30. Garcia, D. Robust smoothing of gridded data in one and higher
dimensions with missing values. \emph{Computational Statistics \& Data
Analysis} \textbf{54,} 1167--1178 (2010).

\leavevmode\hypertarget{ref-zimon_novel_2016}{}%
31. Zimoń, M., Reese, J. \& Emerson, D. A novel coupling of noise
reduction algorithms for particle flow simulations. \emph{Journal of
Computational Physics} \textbf{321,} 169--190 (2016).

\leavevmode\hypertarget{ref-zimon_evaluation_2016}{}%
32. Zimoń, M. \emph{et al.} An evaluation of noise reduction algorithms
for particle-based fluid simulations in multi-scale applications.
\emph{Journal of Computational Physics} \textbf{325,} 380--394 (2016).

\leavevmode\hypertarget{ref-grinberg_analyzing_2009}{}%
33. Grinberg, L., Yakhot, A. \& Karniadakis, G. E. Analyzing Transient
Turbulence in a Stenosed Carotid Artery by Proper Orthogonal
Decomposition. \emph{Annals of Biomedical Engineering} \textbf{37,}
2200--2217 (2009).

\leavevmode\hypertarget{ref-grinberg_proper_2012}{}%
34. Grinberg, L. Proper orthogonal decomposition of atomistic flow
simulations. \emph{Journal of Computational Physics} \textbf{231,}
5542--5556 (2012).

\leavevmode\hypertarget{ref-bruno_numerical_2012}{}%
35. Bruno, O. \& Hoch, D. Numerical Differentiation of Approximated
Functions with Limited Order-of-Accuracy Deterioration. \emph{SIAM
Journal on Numerical Analysis} \textbf{50,} 1581--1603 (2012).

\leavevmode\hypertarget{ref-knowles_methods_nodate}{}%
36. Knowles, I. \& Renka, R. J. METHODS FOR NUMERICAL DIFFERENTIATION OF
NOISY DATA. 12

\leavevmode\hypertarget{ref-rizk_segmentation_2014}{}%
37. Rizk, A. \emph{et al.} Segmentation and quantification of
subcellular structures in fluorescence microscopy images using Squassh.
\emph{Nature Protocols} \textbf{9,} 586--596 (2014).

\leavevmode\hypertarget{ref-holcman_modeling_2007}{}%
38. Holcman, D. Modeling DNA and Virus Trafficking in the Cell
Cytoplasm. \emph{Journal of Statistical Physics} \textbf{127,} 471--494
(2007).

\leavevmode\hypertarget{ref-lagache_effective_2008}{}%
39. Lagache, T. \& Holcman, D. Effective Motion of a Virus Trafficking
Inside a Biological Cell. \emph{SIAM Journal on Applied Mathematics}
\textbf{68,} 1146--1167 (2008).

\leavevmode\hypertarget{ref-dinh_model_2005}{}%
40. Dinh, A.-T., Theofanous, T. \& Mitragotri, S. A Model for
Intracellular Trafficking of Adenoviral Vectors. \emph{Biophysical
Journal} \textbf{89,} 1574--1588 (2005).

\leavevmode\hypertarget{ref-brandenburg_virus_2007}{}%
41. Brandenburg, B. \& Zhuang, X. Virus trafficking -- learning from
single-virus tracking. \emph{Nature Reviews Microbiology} \textbf{5,}
197--208 (2007).

\leavevmode\hypertarget{ref-karpatne_physics-guided_2017}{}%
42. Karpatne, A., Watkins, W., Read, J. \& Kumar, V. Physics-guided
Neural Networks (PGNN): An Application in Lake Temperature Modeling.
\emph{arXiv:1710.11431 {[}physics, stat{]}} (2017).

\leavevmode\hypertarget{ref-sharma_weakly-supervised_2018}{}%
43. Sharma, R., Farimani, A. B., Gomes, J., Eastman, P. \& Pande, V.
Weakly-Supervised Deep Learning of Heat Transport via Physics Informed
Loss. \emph{arXiv:1807.11374 {[}cs, stat{]}} (2018).

\leavevmode\hypertarget{ref-pun_physically-informed_2018}{}%
44. Pun, G. P. P., Batra, R., Ramprasad, R. \& Mishin, Y.
Physically-informed artificial neural networks for atomistic modeling of
materials. \emph{arXiv:1808.01696 {[}cond-mat{]}} (2018).

\leavevmode\hypertarget{ref-raissi_physics_2017-1}{}%
45. Raissi, M., Perdikaris, P. \& Karniadakis, G. E. Physics Informed
Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial
Differential Equations. \emph{arXiv:1711.10561 {[}cs, math, stat{]}}
(2017).

\end{document}
