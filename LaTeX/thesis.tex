\documentclass{Dissertate}


\usepackage{layouts}


% Overwrite \begin{figure}[htbp] with \begin{figure}[H]
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}


% fix for pandoc 1.14
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% TP: hack to truncate list of figures/tables.
\usepackage{truncate}
\usepackage{caption}
\usepackage{tocloft}
\usepackage{subcaption}
% TP: end hack
\usepackage{hyperref}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

\date{}
\makeatletter
%\@ifpackageloaded{subfig}{}{\usepackage{subfig}}
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\captionsetup[subfloat]{margin=0.5em}
\AtBeginDocument{%
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\AtBeginDocument{%
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother

\begin{document}

\begin{titlepage}
​    \begin{center}

    % Delete the following line
    % to remove the UCL header logo
    %\ThisULCornerWallPaper{1.0}{style/univ_logo.eps}
        
        \vspace*{2.5cm}
        
        \huge
        Quantifying the Golgi
        
        \vspace{1.5cm}
        
        \Large
        Gert-Jan Both
    
        \vspace{1.5cm}
    
        %\normalsize
        %A thesis presented for the degree of\\
        %Doctor of Philosophy
        
        \vfill
        
        \normalsize
        Supervised by:\\
        P. Sens\\
        C. Storm
    
        \vspace{0.8cm}
    
        % Uncomment the following line
        % to add a centered university logo
        % \includegraphics[width=0.4\textwidth]{style/univ_logo.eps}
        
        \normalsize
        Eindhoven Technical University\\
        January-November 2018
    
        % Except where otherwise noted, content in this thesis is licensed under a Creative Commons Attribution 4.0 License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Copyright 2015,Tom Pollard.
    
    \end{center}
\end{titlepage}

\hypertarget{abstract}{%
\chapter*{Abstract}\label{abstract}}
\addcontentsline{toc}{chapter}{Abstract}
 The Golgi apparatus is a key component in intracellular
trafficking, altering and directing proteins essential to the cell.
Despite years of research, a model coupling Golgi to the intracellular transport is lacking. In this thesis we develop such a model. We analyse new experimental data to investigate if intracellular transport can be described by an advection diffusion equation using two novel methods based on image gradients and neural networks. Neither of the two methods proved able to answer to answer this question however. We also theoretically study if the Golgi can be modelled as an active, phase separated droplet in the advective-diffusive environment set up by the intracellular transport. We find that such a model can indeed explain several key observations such as the \emph{de novo} formation of the Golgi.




\newpage


\tableofcontents

\newpage

\setcounter{page}{1}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

The cell uses thousands of proteins and lipids to function. Many of
these are produced in the Endoplasmic Reticulum (ER), an organelle found
in eukariotic cells. Upon exiting the ER, these proteins and lipids are
then transported throughout the cell in a process known as intracellular
transport. Key component in this intracellular trafficking is the Golgi
apparatus, an organelle responsible for biochemically maturing proteins
and directing them to the right location. Intense research over the last
years has identified key players,\textsuperscript{1,2}
but an integrated model coupling Golgi size and function to the
intracellular transport is
lacking\textsuperscript{3,4}. In this thesis, we seek
to propose such a model.

\hypertarget{the-secretory-pathway-biology-101-for-physicists}{%
\section{The secretory pathway: biology 101 for
physicists}\label{the-secretory-pathway-biology-101-for-physicists}}

Proteins produced in the ER exit the organelle at specific locations
referred to as ER Exit Sites - ERES. At these sites, cargo is packaged
into a lipid bilayer and this package, known as a vesicle, buds off into
the cytoplasm\textsuperscript{5}. ERES are located throughout the cell
and thus the vesicles need to be transported to their destination: the
Golgi apparatus. In general, we can recognise two different trafficking
modes: diffusive and directive\textsuperscript{6}. In the directive
mode, molecular motors pull vesicles along microtubules by hydrolysing
ATP. Microtubules (MTs) are long tubular polymers spread throughout the
cell and form a network which acts as the backbone for intracellular
transport. They are organised around objects known as MicroTubular
Organisation Centers (MTOCs). The primary MTOC is the centrosome, an
organelle located next to the nucleus, but strong evidence exists that
the Golgi apparatus acts a MTOC
too\textsuperscript{7,8}.

Microtubules are polarised and have two distinct ends, indicated as the
(+) and (-). Different molecular motors are utilised for transport
towards each end, with dynein being (-)-directed and kinesin
(+)-directed\textsuperscript{9}. Vesicles are often attached to multiple
motors of both types, binding and unbinding constantly, making this
active transport a stochastic process which can, for example, be
described by a tug-of-war model\textsuperscript{10}. Furthermore, cargo
can also completely detach from all molecular motors. The vesicle will
then move through the cytoplasm in a diffusive way, until it reattaches
to a microtubule. Note that diffusive mode is a deceptively simple name,
as the cytoplasm is not a simple fluid; it is packed with other cellular
components, giving rise to effects such as anomalous diffusion or
crowding.

The intracellular trafficking transports the vesicles towards the Golgi,
where the cargo undergoes biochemical modification (a process generally referred to as maturation) and is sorted before
being sent to their destination. The Golgi thus acts as a sort of
post-office of the cell, receiving cargo, repackaging it and sending it to
the right destination\textsuperscript{11}. Although the function of the
Golgi is similar for different cell types, its appearance is strongly
dependent on it. In plants for example, the Golgi is distributed
throughout the cell in separate but fully functional subunits known as
stacks\textsuperscript{12} , whereas in mammals all these stacks are
localised close to the nucleus in a single organelle known as the
\emph{Golgi Ribbon}\textsuperscript{7}. A stack consists of a number of
stacked compartments of a disk-like shape called \emph{cisternae}. These
are membrane enclosed objects containing the enzymes responsible for
biochemically altering the proteins, a process generally referred to as
maturing. Proteins move through the Golgi in a particular direction and
the Golgi thus has distinct entry and exit faces. These are known
respectively as the cis and trans face, with the cisternae being labeled
analogously. The cisternae in the middle of the stack are referred to as
medial compartments.

\begin{figure}
\hypertarget{fig:Golgimodels}{%
\centering
\includegraphics[width=0.9\textwidth]{source/figures/png/Golgimodels.png}
\caption{\textbf{Left panel}: In the cisternal maturation model,
compartments mature as a whole and thus change identity. \textbf{Right
panel}: In the vesicle transport model, compartments are static objects
and cargo is being transported from compartment to compartment by
vesicles. Image taken from 13.}\label{fig:Golgimodels}
}
\end{figure}

At the cis-face vesicles fuse with the Golgi and release their cargo
into a compartment, while their lipid bilayer becomes part of the
compartment membrane. Exactly how maturation then happens is debated\textsuperscript{13,14}. The two main competing
explanations are the \emph{cisternal maturation} and \emph{vesicular
transport} models. In figure \ref{fig:Golgimodels} we show the
structure of the Golgi and and a schematic view of each model. In the
cisternal maturation model (left panel of \ref{fig:Golgimodels}),
the compartments mature as a whole, changing identity from cis to medial
and finally to trans. Trans compartments are recycled into cis
compartments by retrograde vesicular transport. In the vesicular
transport model (see right panel of \ref{fig:Golgimodels}), the
vesicles move in the opposite direction. Rather than constantly changing
identity, in this model cisternae are static entities with a defined
task and cargo is moved from one compartment to the next by vesicles.
The debate could thus be settled by analysing the direction of the
vesicles, but so far this has proven elusive. At the trans-face, the cargo is encapsulated again in a lipid bilayer
and is transported to its destination, similar to pre-Golgi
intracellular transport.


\hypertarget{quantitative-models-of-the-Golgi}{%
\subsection{Quantitative models of the
Golgi}\label{quantitative-models-of-the-Golgi}}

The Golgi has been intensively studied by biologists for many years, but
very few attempts at quantifying the link between the Golgi and the intracellular transport appear to have been made: our
research only turned up a single attempt by Hirschberg et
al\textsuperscript{15}, where the authors present a model for the
trafficking of VSVG virus from the ER to the plasma membrane. The
secretory pathway is modeled by dividing it into populations connected
by a first order rate equation, i.e.
\(d \phi_{2}/dt=k_{1\to2}\phi_{1}\). Assuming no flowback (i.e.
\(k_{i+1\to i}=0\)) and a population for the ER, Golgi and Plasma
Membrane, they find that such a model is sufficient to describe their
experimental data, as shown in figure \#fig:ratemodel.

\begin{figure}
\hypertarget{fig:ratemodel}{%
\centering
\includegraphics[width=0.8\textwidth]{source/figures/png/kineticmodel.png}
\caption{\textbf{Left panel}: First order rate model fitted to
experimental data by 15 \textbf{Right panel}: Inferred concentration in
ER, Golgi and PM using the fitted parameters from the left panel and
their model. Image reprinted from 15.}\label{fig:ratemodel}
}
\end{figure}

We reprint their main result in figure \ref{fig:ratemodel}.
Although this model describes the experimental data, it is a
phenomenological model and reduces the entire system to a few rate
parameters \(k_{i\to i+1}\). These rate parameters are not coupled to
any of the underlying processes and hence this model does not offer any
insight into the system. Furthermore, the model lacks any spatial
dependence of the concentration.

\hypertarget{this-thesis}{%
\section{This thesis}\label{this-thesis}}

The Golgi is thus intimately connected to the intracellular transport. In this thesis we seek to construct a model which links the two beyond a mere rate model and propose that the intracellular transport can be modelled as an advection-diffusion equation. The Golgi is described as an active, phase separated droplet which matures the cargo transported to it by the intracellular transport. Such a model predicts the spatial organisation of cargoes being trafficked as well as the formation of the Golgi; studies have shown that the Golgi is able to form \emph{de novo}\textsuperscript{8}, meaning that in cells from which the Golgi has been removed, it will automatically reappear.  

	We also confront our model with experimental data gathered by the team of Frank Perez at Institut Curie. This team has developed a new technique called RUSH\textsuperscript{16}, which is used to study the intracellular transport from the ER to the Golgi and
beyond using fluorescence microscopy. In the next sections we introduce the experimental data, how we intend to analyse it and justify
the description of intracellular transport and the Golgi as a phase separated droplet.

\hypertarget{experimental-data}{%
\subsection{Experimental data}\label{experimental-data}}

RUSH (Retention Using Selective
Hooks) has recently been developed\textsuperscript{16} in the team of
Frank Perez at Institut Curie to study intracellular trafficking from the ER to the Golgi and even
post-Golgi using fluorescent live-cell imaging.

\begin{figure}
\hypertarget{fig:RUSH}{%
\centering
\includegraphics[width=0.65\textwidth]{source/figures/png/RUSH.png}
\caption{Schematic overview of the RUSH system. Image taken from
16}\label{fig:RUSH}
}
\end{figure}

Figure \ref{fig:RUSH} shows the principle of the RUSH system.
Inside the ER, a core streptavidin is fused to the ER using a hook protein.
Another protein known as a streptavidin-binding-protein (SBP) binds to
streptavidin, but connected to the SBP are also the protein to be
transported (`reporter') and a fluorescent protein. Upon the addition of
biotin, the SBP is released from the streptavidin as the biotin binds to
the streptavidin. The SBP-reporter-fluorescent complex then exits the ER and can be
followed the entire secretory pathway with fluorescence microscopy. 

Because the addition of biotin is a nearly instantaneous process, RUSH allows for precise timing of release of the report complex. Another advantage is its versatility, as it can be used for many different proteins. In this
thesis we mainly focus on the \(\alpha\)-mannosidase-II, generally
referred to as ManII. The ManII protein is retained in the Golgi apparatus
after trafficking, meaning that the data we
obtain will only contain transport \emph{towards} the Golgi, greatly
simplifying the analysis as we will not have to separate pre and post-Golgi trafficking.
Figure \ref{fig:manII} shows two frames containing three cells (denoted by the black dashed line) in a typical RUSH
experiment of ManII trafficking.  Initially, all the cargo is retained in the ER and the fluorescence should thus be diffuse throughout the cell around the nucleus. This is shown in the left panel: in all cells we observe the fluorescence in a ring around a dark centre. Once the ManII is trafficked, the Golgi will show up as a bright object. This is shown in the right panel, where we have denoted the location of the Golgi by the red circle. However, some of the cargo is either still in the ER or being trafficked, considering the fluorescence outside of the Golgi.

\begin{figure}
\hypertarget{fig:manII}{%
\centering
\includegraphics[width=0.9\textwidth]{source/figures/png/frames.png}
\caption{Two frames of the ManII transport images using the RUSH
technique.}\label{fig:manII}
}
\end{figure}

\hypertarget{model}{%
\subsection{Model}\label{model}}

Vesicles exiting the ERES are transported towards the ER over the
microtubules. This is a stochastic process with the proteins detaching
from and (re-) attaching to the microtubules randomly, while the
vesicles move diffusely once detached. Several models have been
developed to describe such intracellular transport processes \textsuperscript{6, 17},
many in the light of virus trafficking \textsuperscript{18, 19, 20}. In general, these
models assume a two population model, with one population being cargo
attached to a microtubule and another cargo freely diffusing in the
cytoplasm. If one assumes that the timescale for attaching and detaching
from the microtubules is much smaller than the transport timescale, the
two populations can be assumed to be in equilibrium. In this assumption,
known as a quasi-steady-state reduction, the two population model
reduces to a Fokker-Planck equation. As the Fokker-Planck equation is
functionally equivalent to an advection-diffusion equation, we
hypothesise that we can model protein transport using an
advection-diffusion equation
: 
\begin{equation}
	\label{eq:advdiff}
	\partial_t c = \nabla (D\nabla c-\vec{v} c)
\end{equation}

 where \(c\) is the concentration of the cargo, \(D\) a diffusion
coefficient and \(v\) an advection velocity. Equation \ref{eq:advdiff}
is thus the model we fit our data to. Note that the fluorescence images
obtained from the RUSH experiment return an intensity \(I\) and not a
concentration \(c\), and hence we make the assumption \(c \propto I\).

Many biological processes and reactions require a high concentration of
some protein or lipid to occur. This can be achieved by physically
separating proteins inside a membrane (consider the lysosome), but the
cell contains several membrane-less organelles. These organelles thus
require a different means of reaching high concentrations and the prime
candidate is liquid-liquid phase separation. In this process a mixture
of liquids A and B separates into two phases, one rich in
A and one rich in B, due to the interactions between them. Phase
separation can thus produce domains with a high
concentration without membranes. It has been proposed as a model for early
protocells\textsuperscript{21} and is able to correctly describe several
phenomena such as P-granules\textsuperscript{22} and centrosome
growth\textsuperscript{23}.

We use a similar description for the Golgi, as its
biogenesis contains strong clues which point towards phase separation as the process driving the biogenesis. The Golgi is able to form \emph{de novo}, meaning
that in cells from which the Golgi has been removed, it will reappear
without any specific action. Ronchi et al\textsuperscript{8} studied
this in detail and found three phases of growth. In the first phase,
vesicles are released from the ER, but no larger structures are formed
and the vesicles disappear either due to fusion with the ER or
degradation. In the second phase larger stack-like structures are
formed, while in the third phase all these structures are clustered in a
single location; the Golgi Ribbon is formed. Phase two has the markings
of a concentration-dependent phase separation: once a critical
concentration of vesicles is reached, the mixed state becomes unstable and the vesicles aggregate and fuse to form a Golgi stack. When such an transition occurs, not all vesicles in the system aggregate: an equilibrium between single vesicles and the aggregated vesicles will exist. Coarse graining such a system yields a dilute phase with a low concentration of vesicles and a dense phase. Interpreting this dense phase as the Golgi and the dilute phase as the cytoplasm, we thus describe the Golgi as a phase separated droplet.  Also note that by describing the systems in terms of some coarse-grained vesicle concentration, we can apply phase separation - a theory normally used to describe membrane-less organelles - to a membrane delimited organelle: the Golgi. 

The Golgi is highly dynamical organelle, taking up and budding off vesicles constantly and our model needs to account for this. We neglect the precise form of maturation (i.e. cisternal maturation versus vesicular transport)
and model the maturation by considering two populations, immature and mature, with the immature population being converted into mature inside the droplet. This makes the droplet \emph{active} and we thus describe the Golgi as an \emph{active phase-separated
droplet}.

\hypertarget{biological-image-analysis}{%
\subsection{Biological image analysis}\label{biological-image-analysis}}

Image analysis is a lively and ongoing subject in cell biology, with
many new methods being developed constantly, especially with
quantisation in
mind\textsuperscript{24, 25, 26, 27}.
Techniques for quantifying intracellular transport roughly fall into two
categories: single particle tracking (SPT) or correlation spectroscopy.
SPT tracks fluorescent proteins or beads moving through the cell on a
frame-to-frame basis, so that each particle's trajectory can be
reconstructed. These trajectories can then be analysed to obtain
information about the transport. The fluorescent movies obtained from
the RUSH experiments are not clear enough to accurately localise the
vesicles, so that SPT can not be used to analyse the transport. Methods
based on correlation
spectroscopy\textsuperscript{28, 29, 30} rely on a general relationship between the fluctuations and the
underlying density of particles and transport properties. These
techniques thus require a nearly constant concentration, but the RUSH
experiments show highly dynamical concentrations. Thus, none of the techniques we found
are directly applicable to the RUSH data.

As stated, we hypothesise that we can describe the intracellular process
by an advection-diffusion equation and we wish to confront this with the
RUSH data. The question we are thus asking is a rather general one: how
do we fit some spatiotemporal (nD+1) data to a model? More specifically,
since a model is most often presented in the form of a partial
differential equation (i.e.
\(df/dt = \alpha(x, t) df/dx+\beta(x, t) d^2f/dx^2+...\)), for what parameters
\(\alpha(x), \beta(x)...\) is the temporal evolution of a given dataset best
described? We have developed and evaluated two different methods. Our
first method approaches the problem rather directly by calculating
spatial and temporal derivatives directly from the data using a
technique known as image gradients. Our second method is based on a
recently developed technique based on neural
networks\textsuperscript{31}. We will show that by encoding physics into
the neural network, we are not only able to infer the optimal
parameters (i.e. $\alpha(x,t)=\alpha_0$), but even an optimal parameter \emph{field} $\alpha(x,t)$.

\hypertarget{structure-and-main-questions}{%
\subsection{Structure and main
questions}\label{structure-and-main-questions}}
The central theme of this thesis is thus the relation of the intracellular transport and the Golgi apparatus. We approach this problem from two sides: on the one hand, we analyse experimental data to study if intracellular transport can indeed be described by an advection-diffusion equation. On the other hand we theoretically investigate how an active droplet can form and behaves in a concentration profile set up by an advection diffusion equation.

This thesis hence consists of two parts. In the first part of this thesis we show two model fitting methods we have developed and apply them to the RUSH experimental data. In the second part we construct a model for the Golgi as an active, phase separated droplet in a diffusive-advective environment and theoretically investigate if this model can explain Golgi formation and maintenance. In a chapter-by-chapter breakdown, we have the
following:

\begin{itemize}
\tightlist
\item
  \textbf{Part I - Model fitting and data analysis}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Chapter 2} introduces the framework we have developed for
    model fitting spatiotemporal data using image gradients.
  \item
    \textbf{Chapter 3} applies the method developed in chapter 2 to
    experimental data.
  \item
    \textbf{Chapter 4} shows an alternative method for model fitting
    based on neural networks.
  \end{itemize}
\item
  \textbf{Part II - Golgi as an active phase-separated droplet }

  \begin{itemize}
  \tightlist
  \item
    \textbf{Chapter 5} introduces the Cahn-Hilliard equation, which
    describes phase separation, an approximation of it known as
    effective droplet theory and develops our model.
  \item
    \textbf{Chapter 6} contains the predictions the model developed in
    chapter 5 and investigates the biological implications.
  \end{itemize}
\item
  \textbf{Chapter 7} is the concluding chapter and summarizes all the
  findings from the previous chapters.
\end{itemize}

\hypertarget{model-fitting}{%
\chapter{Model fitting}\label{model-fitting}}

In this chapter we introduce the method we have developed for fitting a
model in the form of a PDE to spatiotemporal data. We start off with the general concept and subsequent section will elaborate on each step.

Assume we have access to experimental data of some process \(f(x,t)\).
Parallelly, we have also developed a model describing this process, but
it is in the form of a PDE: 
\begin{equation}
	\label{eq:PDEone}
	\partial_t f(x,t) = \lambda_1 \nabla^2f(x,t)+\lambda_2\nabla f(x,t) +\lambda_3 f(x,t) +\lambda_4
\end{equation} 
We now wish to investigate if this model fits the data \(f(x,t)\) and what coefficient values \(\lambda_i\) best describe the dataset.
To do so, we consider each term on the right of equation \ref{eq:PDEone} in \(f(x,t)\) as some variable \(x_i\) and \(\partial_t f\) as \(y\),
so that we can rewrite it as: \[
y = \lambda_1 x_1+\lambda_2x_2 +\lambda_3 x_3 +\lambda_4
\]
If we thus can find the variables \(x_i\) and \(y\), we can perform a
fitting procedure such as least squares to obtain the coefficients
\(\lambda_i\). In other words, if we can calculate the spatial and
temporal derivatives of our data, we can fit the model. Although the
concept seems trivial, its implementation is not. Data is rarely
noiseless and obtaining accurate derivatives from noisy data is
notoriously hard, but it forms the heart of our method. In the case of biological data, segmentation into sub-areas is often required and the coefficients \(\lambda_i\) might
not be constant but space- and time- dependent. 

\newpage
The process of fitting the data thus has several steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Denoising and smoothing
\item
  Calculating derivatives
\item
  Segmenting
\item
  Fitting
\end{enumerate}

In the next sections, we describe each step separately. Note that the
method we present here has been developed empirically: there's no
theoretical background as to why this particular combination should work. Instead, it's been developed by analysing the data,
adapting each step on the go. Nonetheless, the resulting process is general and presents a new tool to quantify fluorescence microscopy. To illuminate the process, we have thus chosen to illustrate the effects of each step with RUSH experimental data instead of synthetic data.


\hypertarget{step-1---smoothing-and-denoising}{%
\section{Step 1 - Smoothing and
denoising}\label{step-1---smoothing-and-denoising}}

The first step is to denoise and smooth the data, which is required for accurately calculating the derivatives.
This is a very active area of research (especially in life
sciences) and several methods exist\textsuperscript{35}. After
evaluating several methods, we have settled on the so-called `WavInPOD'
method, introduced in 2016 by the authors of [36]. They show that this methods
outperforms several other advanced methods\textsuperscript{37} by combining two existing methods: Proper
Orthogonal Decomposition (POD) with Wavelet filtering (Wav). Both
subjects are vast (especially Wavelet transform) and as we are only
interested in the result, we only present a short
introduction here, adapted from [36].

POD is closely related to Principal Component Analysis (PCA) in statistics and is already used in physics to analyse turbulent flows \textsuperscript{38}. Similar to other transformations such as the Fourier transform, we wish to expand a function as the sum over a set of orthogonal functions:

\begin{equation}
	\label{eq:POD}
	f(x,t)=\sum_{n=1}^r E_n\alpha_n(x)\phi_n(t)
\end{equation}


where \(\alpha_n\) and \(\phi_n\) are called respectively the spatial
and temporal modes and $E_n$ indicates the relative strength of each mode. However, contrary to a Fourier transform, the orthogonal functions are not some predetermined set, but are determined from the data. More specifically, the data is decomposed into its eigenvectors $\alpha_n$ and $\phi_n$, with $E_n$ being the eigenvalues. Modes with high eigenvalues $E_n$ 'contribute more' to the behaviour of $f(x,t)$ than modes with lower eigenvalues and we can use this observation to denoise data by letting the sum of equation \ref{eq:POD} only run over the most important modes. In fact, when plotting the sorted $log10$ spectrum of $E_n$, a 'knee' is observed: modes above the knee constitute the signal, whereas modes below are essentially noise.  We show the eigenvalue spectrum of the ManII data in figure \ref{fig:eigen}.

\begin{figure}
\hypertarget{fig:eigen}{%
\centering
\includegraphics{source/figures/pdf/eigenspectrum.pdf}
\caption{Eigenvalue spectrum of the POD of the ManII data. We have placed the cutoff at mode 27.}\label{fig:eigen}
}
\end{figure}

Note that although a knee is visible, exactly where to place the mode cutoff is not clear. Techniques to consistently determine the cutoff exist\textsuperscript{38, 39}, but yielded unsatisfactory performance when applied to the shown spectrum. We thus determined the cutoff by hand by checking the if the data after applying the POD still showed the same trend and features.

 Consider an oscillation consisting of two frequencies $f_1$ and $f_2$. Initially, the signal is solely composed of $f_1$, but after some time the frequency is switched to $f_2$. Fourier transforming such a signal would yield two delta peaks at $f_1$ and $f_2$: it returns with infinite precision which frequencies are present in the signal, but not when. In a wavelet transform, this infinite precision in the frequency domain is traded for information in the time domain by the uncertainty theorem: a wavelet transform will not give back the frequencies $f_1$ and $f_2$ with infinite precision, but it will state when they are present. 
 
 In practice, wavelet transforming returns the signal as an approximation plus a set of details for each datapoint. We can then filter the signal by applying by some sort of cutoff to the details. Note that cutting off the details will only change the signal locally; we thus do not lose any sharpness anywhere else in the data as one would with Fourier filtering.
WavinPOD combines these two techniques by applying wavelet filtering to
the POD modes. First, the dataset is decomposed into its POD modes and the energy spectrum is analysed to select a cutoff mode. All retained
modes are wavelet filtered and are then retransformed to give the
denoised and smoothed signal. In figure \ref{fig:filtered} we show
the results of the smoothing in the time and spatial domain. In the left
panel we show the signal of a single pixel in time, while we plot a line
of pixels in a single frame in the right panel. The red lines denote the
original (unfiltered) signal, the blue line the effect of just applying
POD filtering and the black one the result of the WavInPOD technique. Note that the effect of the wavelet filtering is to smooth the signal
significantly and in comparing the original data to the filtered data
that we have retained the sharpness of the features whilst obtaining a significantly smoother signal.

\begin{figure}
\hypertarget{fig:filtered}{%
\centering
\includegraphics{source/figures/pdf/filtered.pdf}
\caption{Effect of POD with a cutoff of 27 and wavelet filtering with a
level 3 db4 wavelet. Left panel shows the result in the time domain,
right panel in the spatial domain. Lines have been offset for
clarity.}\label{fig:filtered}
}
\end{figure}

\hypertarget{step-2---derivatives}{%
\section{Step 2 - Derivatives}\label{step-2---derivatives}}

After having denoised the images, we calculate the spatial and temporal
derivatives. Obtaining correct numerical derivatives is hard and becomes
much more so in the presence of noise\textsuperscript{40}. Next to a
finite-difference scheme, one can for example (locally) fit a polynomial
and take its derivative.\textsuperscript{41} However, the computational cost of these methods is high and they do not scale well to dimensions
higher than one. We thus require an alternative method. In fact, obtaining the gradient of a 2D discrete grid has another subtlety which we need to address.

Naively, one could obtain the gradient of a 2D grid by taking the
derivative using a finite difference scheme with respect to the first
and second axis. If there are features on the scale of the
discretisation (\(\sim\) few pixels), such an operation will lead to
artefacts and underestimate the gradient. These issues have long been
known and several techniques have been developed to accurately calculate
the gradient of an `image'. The most-used image-gradient technique is
the so-called Sobel operator and we derive its structure here. Consider a basic central finite difference scheme:

\[
\frac{df(x_i)}{dx}\approx\frac{f(x_{i+1})-f(x_{i-1})}{2h}
\]

where \(h\) is defined as \(x_{i+1}-x_{i}\). Applying this operation to a set of three pixels thus returns the derivative of the middle pixel. We can rewrite this as a matrix $S$
 \[
S=\frac{1}{2}\cdot
\begin{bmatrix}
-1 & 0 & 1
\end{bmatrix}
\] 
which, when applied \emph{element wise} to the three pixels, returns the middle pixels' derivative. Applying the matrix $S$ element wise to each point in a dataset is known as an convolution and convolving a matrix $A$ with the matrix $S$ yields its derivative:
 \[
\partial_xA\approx A*\frac{1}{2}\begin{bmatrix}
-1 & 0 & 1
\end{bmatrix}
\] 
\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{source/figures/pdf/derivative.pdf}
	\caption{In the left panel we show how a finite difference operator would be applied to the black pixel. The right panel shows this for the Sobel operator.}
	\label{fig:Sobel}
\end{figure}

As stated, this operation is inaccurate and introduces artefacts. To
improve this, we wish to include the pixels on the diagonal of the pixel
we are performing the operation on as well (see figure
\ref{fig:Sobel}). The distance between the diagonal pixels and the
center pixel is not 1 but \(\sqrt{2}\) and the diagonal gradient also
needs to be decomposed into \(\hat{x}\) and \(\hat{y}\), introducing
another factor \(\sqrt{2}\). We thus obtain the classic
\(3\times3\) Sobel filter in the $\hat{x}$ and $\hat{y}$ direction:  \[
\mathbf G_x=\frac{1}{8}\cdot
\begin{bmatrix}
-1 & 0 & 1\\
-2 & 0 & 2\\
-1 & 0 & 1
\end{bmatrix}
\mathbf G_y=\frac{1}{8}\cdot
\begin{bmatrix}
-1 & -2 & -1\\
0 & 0 & 0\\
1 & 2 & 2
\end{bmatrix}
\]

Increasing the size of the Sobel filter increases its accuracy and we have
implemented a 5x5 operator. The matrix implementation is also beneficial from a computational standpoint, as convolutional operations are very efficient and one can derive a Sobel operator for arbitrary dimensions. Separate methods to calculate second order derivatives exist, we simply apply the Sobel operator twice. We also make use the derivatives to segment the movie. We show this in the next section. 


\hypertarget{step-3---segmentation}{%
\section{Step 3 - Segmentation}\label{step-3---segmentation}}

In the case of the RUSH data, obtained images and movies often contain
multiple cells. Each of these cells can be further segmented into two
more areas of interest: the cytoplasm, which is were we want to fit our
model and the Golgi apparatus. We wish to make a mask which allows us to
separate the cells from the background and divide each
cell into cytoplasm or Golgi. Figure \ref{fig:manII} shows two
typical frames in the MANII transport cycle. Note that no sharp edges
can be observed, especially once the MANII localises in the Golgi. No
bright field images were available as well, together making use of
techniques such as described in [42] unavailable. We have thus developed our own method which depends on the intensity and its time derivative. It consists of four steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Normalize the intensity \(I\) and its time derivative between 0 and 1.
\item
  Sum all the frames over some quantity. To separate the Golgi, we determined \(\sum_n I(x, y, t_n)\), while we calculated \(\log_{10}\left(\sum_nI(x,y,t_n)\cdot\partial_tI(x,y,t_n)\right)\) for the cytoplasm. 
 \item
  Threshold the image to obtain the mask. This is either done
  automatically through an Otsu threshold or by manually adjusting the
  threshold until desired result.
\item
  The mask is post-processed by filling any potential holes inside the
  mask.
\end{enumerate}

We show this process in figure \ref{fig:mask}. The upper two panels show
the images obtained after performing the summing operation for the Golgi
and cytoplasm (also referred to as 'active area') respectively, while the lower left panel shows the final mask obtained after thresholding these two images. For comparison, we have plotted a frame of the data to compare the mask to. 

\begin{figure}
\hypertarget{fig:mask}{%
\centering
\includegraphics[width=0.7\textwidth]{source/figures/pdf/segmenting.pdf}
\caption{Four panels showing the different stages of making the mask.
From segmenting the upper two panels we determine the Golgi and active
area, leading to the mask in the lower left. This can be compared to the actual data in the lower right frame. }\label{fig:mask}
}
\end{figure}

\hypertarget{step-4---fitting}{%
\section{Step 4 - Fitting}\label{step-4---fitting}}

The final step in our method is to fit our model to the data. By having determined both the spatial and temporal derivatives, we have effectively reduced the movie to an $m$ by $n$ sized dataset, where $m$ is the number of datapoints and $n$ the amount of features calculated. In the case of the RUSH data movie, $m$ is number of pixels multiplied by the number of frames, while $n=5$ as we calculate $\partial_x I, \partial_{xx} I, \partial_y I, \partial_{yy} I$ and $\partial_t I$. The fitting thus becomes a generic problem, which can be solved by virtually any fitting method. We use least-squares, but one could use for example a Bayesian method. Each fitting method assumes that the fitting parameters are constant across a dataset however. Given the nature of the cell, the diffusion constant and advection will not be constant: they will be spatially dependent and it is not unlikely that they will show some temporal dependence as well. On a small enough scale however, we can reasonably approximate these fields as constant and here we can perform a fit. To not lose any spatial resolution and prevent artefacts, we use the sliding-window technique, which is illustrated in figure \ref{fig:slidingwindow}. In a small window around a pixel, we perform the fitting procedure, thus yielding the diffusion coefficient and advection velocity for that pixel. We then move the window to the next pixel, thus finding diffusion and advection fields with a similar resolution as the data. In the next chapter we apply this method to the RUSH experimental data.

\begin{figure}
\hypertarget{fig:slidingwindow}{%
\centering
\includegraphics{source/figures/pdf/slidingwindow.pdf}
\caption{Schematic overview of the sliding window technique. The solid
black line encompasses an area around its blue coloured central pixel
and the fit output is assigned to that pixel. We then move the window
(dashed black line) and perform the fit for the orange coloured
pixel.}\label{fig:slidingwindow}
}
\end{figure}


\hypertarget{data-analysis}{%
\chapter{Data analysis}\label{data-analysis}}

In this chapter we apply the method developed in the previous chapter to the ManII experimental data obtained using the RUSH technique. We first present an initial analysis of the data by investigating the fluorescence curves of several areas of interest and study movies' time derivative. We then analyse the results of the least squares fit.  

\hypertarget{initial-analysis}{%
\section{Initial analysis}\label{initial-analysis}}

We stated in the introduction that we assume that the concentration is proportional to the intensity of the fluorescence, $c\propto I$. To test this assumption, we plot the time-evolution of the fluorescence of the entire cell. We normalise the fluorescence between 0 and 1 before computing the mean over each frame, to get rid of the background in our statistics. The left panel of frame \ref{fig:fluorescence} shows the average fluorescence of each frame, normalised on the maximum average intensity. In the right panel we plot the same quantity but for the fluorescence in each of the three cells' Golgi. Note a significant drop of almost \(30\%\) in total fluorescence between the initial and final frame. We recognise two regimes: a strong initial drop up until frame 100 and a slower decay after. The right panel shows a saturation of the fluorescence in the Golgi after frame 100, so we attribute the decrease in total fluorescence after frame 100 to photobleaching. 
The first regime is more troublesome, as this strong decrease is not explained by photobleaching and thus casts strong doubts on our assumption that $c \propto I$. Since this drop is spread out over a hundred frames, on a frame-to-frame basis this effect can be neglected in our model.

\begin{figure}
	\centering
	\includegraphics{source/figures/pdf/general_fluorescence.pdf}
	\caption{\textbf{Left panel:} Normalised total fluorescence per frame. \textbf{Right panel:}Normalised fluorescence in each of the three cells' Golgi.\label{fig:fluorescence}}
\end{figure}

In the right panel we observe that all three curves show a roughly linear increase in fluorescence. The blue line seems to have some sort of delay, but also increases linearly after this delay. The cell indicated by the purple line shows a significant drop at frame 200, but
since the ManII protein is retained in the Golgi, this is not caused by any
type of intracellular transport and thus not of interest to us. The
linear increase and common pattern suggests that the transport
properties are not concentration dependent at these concentrations. We now study the time derivative of the fluorescence. We have plotted it at frame 0, 20 50 and 100 in figure \ref{fig:timederiv}.

Although figure \ref{fig:fluorescence} has shown that $c\propto I$ is a doubtful assumption, an increase in fluorescence will mean an increase in concentration. Areas where the time derivative is positive thus correspond to a concentration increase, whereas areas with a negative time derivative correspond to a concentration decrease. In figure \ref{fig:timederiv}, positive areas are shaded, while negative are shaded blue. As expected, the Golgi shows up in each
cell as a bright red object. Note however that we also observe red areas
towards the edges of the cells. As the concentration close to the Golgi
decreases due to trafficking, the blue area moves outwards and slowly takes over the red area. As to the cause of this outer red ring in each cell, we speculate this is caused by a diffusion: reporters exiting the ER through the ERES initially diffuse, increasing the concentration in some areas. 


\begin{figure}
\hypertarget{fig:timederiv}{%
\centering
\includegraphics{source/figures/pdf/time_deriv.pdf}
\caption{The determined time derivative four different frames of the
ManII RUSH experiments.\label{fig:timederiv}}
}
\end{figure}


\hypertarget{analysis-of-ls-fit}{%
\section{Analysis of least-squares fit}\label{analysis-of-ls-fit}}

In this section we present the results of our least squares fit. We have used
a 7 by 7 pixels area in the spatial domain to perform the sliding window
operation and have fitted each frame of the movie independently. The sliding window operation returns diffusion and advection fields the same size as our data. Due to the physical limitations of print media, we only show several frames of the results here. Since a picture is worth more than a thousand words, an animation of 272 frames must be worth a book and we refer the reader to our \href{https://github.com/GJBoth/Masters-thesis}{Github} to find the full results.

We show the inferred diffusion field at frame 4 and 40 in figure \ref{fig:diff_ls}, together with the distribution of diffusion coefficients throughout the entire movie and the fraction of positive diffusion constants per frame. 

\begin{figure}
\hypertarget{fig:diff_ls}{%
\centering
\includegraphics{source/figures/pdf/Diff.pdf}
\caption{Analysis of the inferred diffusion field. The upper row shows
the inferred field at two frames, while the lower row shows the
distribution of values and the fraction of physical values as a function
of time.}\label{fig:diff_ls}
}
\end{figure}

We observe structures larger than the fitting window which, although varying in time, do not differ greatly no a frame-to-frame basis. This means that our fit is capturing at least some of the underlying dynamics. On the other hand, we observe many areas with a negative diffusion coefficient and the inferred diffusion coefficients are on the order of $10^{-4}\mu m^2/s$- orders of magnitude than expected. In the lower left panel we plot the distribution of values, which shows that roughly \(60\%\) of the inferred field has a positive diffusion coefficient. In the lower right panel we have determined this fraction as a function of time. It shows
that, save for a few initial frames, this fraction is not (strongly)
time-dependent. Note however that results are skewed due to its strong peak around 0; many coefficients are negative but extremely close to zero (e.g
\(-10^{-5}\)). Negative diffusion coefficients could indicate to clustering,
but could also be the result of an incorrect fit. We investigate this in
depth after studying the advection profiles, which we show in figure
\ref{fig:advection}. In the four panels we show the inferred
velocity in the \(\hat{x}\) and \(\hat{y}\) direction in the upper two panels, and the magnitude and angle in the lower two.

Similar to the diffusion, we observe patterns both in time and space
bigger than our fitting window, meaning that the fit is not completely
random. On the other hand, we are not able to discern any specific flow patterns from the figures in \ref{fig:advection}. For example, we would expect a rainbow-like pattern pointing towards the Golgi
 in the lower right corner, but no such pattern is observed.
 

\begin{figure}
\hypertarget{fig:advection}{%
\centering
\includegraphics{source/figures/pdf/advection.pdf}
\caption{Analysis of the velocity fields. The upper rows show
respectively \(v_x\) and \(v_y\), while the lower row shows the
magnitude and angle.}\label{fig:advection}
}
\end{figure}




\begin{figure}
\hypertarget{fig:timepixel}{%
\centering
\includegraphics{source/figures/pdf/general_fit.pdf}
\caption{Diffusion and advection velocities of a single pixel in time.
We have plotted the scaled and translated signal as a black dashed line to
show the correlation.}\label{fig:timepixel}
}
\end{figure}

To gain more insight into
our fit, we analyse a single pixel in time. Figure
\ref{fig:timepixel} shows the diffusion constant and advection velocities as fitted at this pixel as a function of time. The scaled and translated intensity of the pixel is plotted by the black dashed line as a reference.

The signal of this pixel remains roughly constant for the first 10 frames and then decreases to noise level. Note that during this initial constant phase, the diffusion constant is negative. Once the signal starts
decreasing, or, in other words, cargo starts flowing, we
see a physical diffusion constant and non-zero velocities. Once the
signal returns to around noise-level around frame 50, the inferred
velocities and diffusion constant seem to become random around 0. In
other words, our method seems to work when the signal is changing but
struggling when the signal is either constant or at noise level. We
observe similar behaviour in other pixels, so we contribute the unphysical diffusion values to constant and noise-level signal.

Using the Einstein-Stokes relation, the diffusion constant of a vesicle with a 50nm radius should be on the order of 5$\mu m^2/s$. Although this is an upper bound, as the vesicle can hardly be considered freely diffusing through the cytoplasm, this is still several orders of magnitude higher than the observed diffusion constants of $10^{-4}\mu m^2/s$. One possible explanation for this difference is the `mixing' of the transport fluorescence with the fluorescence of the
ER. After the addition of biotin, the fluorescent cargo gets released,
but still has a finite residence time in the ER. Since the obtained
images are projected over an axis, changes in fluorescence we observe
can be both due to intracellular transport as well as processes inside
the ER. If these processes have different timescales, this can strongly
affect the inferred coefficients. 
Also note that we have assumed that the intensity of a pixel describes a coarse-grained concentration which we can describe by an advection-diffusion equation. Once the size of the vesicles becomes on the order of the pixel size, this assumption breaks down. In the case of the ManII trafficking, the pixels are roughly two to three times the size of a vesicle, meaning that we are at the limits of our assumption. 

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

We have applied the method developed in the previous chapter to the RUSH
trafficking data of the ManII protein. Both the diffusion and the advection fields show patterns larger than the fitting window, but no clear structure can be discovered. Furthermore, roughly 40\% of the diffusion coefficients were negative, but we also can attribute many of these negative values to either extremely close to zero or a wrong fit due to constant data. Concludingly, we are unable to determine if intracellular transport can be described by an advection-diffusion equation. We present our recommendations to the experimentalists and possible improvements of the fitting method in the conclusion. 

\hypertarget{physics-informed-neural-networks}{%
\chapter{Physics Informed Neural
Networks}\label{physics-informed-neural-networks}}

In the previous chapters we showed the difficulties in fitting a model
in the form of a partial differential equation to spatio-temporal data.
The method we developed was a classical numerical approach, separating
the problem into several substeps such as denoising, smoothing and
numerical differentiation. The main weak points of the developed method are the calculation of numerical derivatives and its rather crude fitting method. In this chapter we present an alternative
technique, generally referred to as Physics Informed Neural Networks
(PINNs), which solves these issues. Although only recently introduced, it has already shown impressive performance in fitting
models and numerically solving
equations\textsuperscript{31, 43, 44, 45, 46}.
Neural networks are a new technique in physics and this chapter also
serves as an introduction to neural networks in general. The chapter
has the following structure:

\begin{itemize}
\tightlist
\item
  \textbf{Neural Networks} - This part will cover the basics of neural
  networks: their inner workings, training and other general features.
\item
  \textbf{Physics Informed Neural networks} - In this second part we
  introduce the concept behind PINNs, use it to solve a toy problem and
  apply it to our RUSH data.
\end{itemize}

\newpage
\hypertarget{neural-networks}{%
\section{Neural Networks}\label{neural-networks}}

Normally when programming a computer to perform some task, we break the
problem into smaller pieces and write down precise instructions.
Often, a model of the underlying process is also needed to transform
some input into an output. The performance of the algorithm is then only
as good as the underlying model and when dealing with complex processes,
such models often become intractable or oversimplified. Artificial
Neural Networks (ANNs) are a different approach to such a problem.
Instead of being \emph{programmed}, they are \emph{trained} and hence
`learn' an underlying model. In a process known as \emph{supervised
learning}, the network is given inputs and the desired outputs for each
input. Training the network then consists of adjusting its internal
parameters until the predictions match the desired outputs. In the next
sections we discuss how to adjust these parameters.

\hypertarget{architecture}{%
\subsection{Architecture}\label{architecture}}

\emph{An excellent introduction is given by Michael Nielsen in his
freely available book ``Neural networks and deep learning.'' The
following section has been strongly inspired by his presentation.}

At the basis of each neural network lies the neuron. It transforms
several inputs into an output in a two-step process. In the first step,
the inputs \(x\) are multiplied with a weight matrix \(w\) and a bias
\(b\) is added: \[
z = w\mathbf{x}+b
\] \(z\) is called the weighted input and is transformed in the second
step by the neuronal \emph{activation function \(\sigma\)}. This gives
the output of the neuron \(a\), also known as the activation:
\begin{equation}
a = \sigma(z) = \sigma(w\mathbf{x}+b)
\label{eq:activation}\end{equation} The activation introduces
non-linearity into the network and hence is crucial; without it a neural
network would merely be several matrix multiplications. The classical
activation is a tanh, i.e \(\sigma(z)=\tanh(z)\), but many other forms
exist, each having its benefits. Several neurons in parallel constitute
a \emph{layer} and several layers can be connected to create a network.
The layers in the middle of the network are referred to as \emph{hidden
layers}. An example of such a network with two hidden layers is shown in
figure \ref{fig:neuralnetwork}.

\begin{figure}
\hypertarget{fig:neuralnetwork}{%
\centering
\includegraphics{source/figures/pdf/neuralnetwork.pdf}
\caption{Schematic overview of a neural network. The left layer is known
as the input layer, the right layer as the output layer and the layers
inbetween are referred to as hidden layers.}\label{fig:neuralnetwork}
}
\end{figure}

\hypertarget{training}{%
\subsection{Training}\label{training}}

Consider again equation \ref{eq:activation}. In a network with
multiple layers, it is useful to express the activation \(a^l\) of layer
\(l\) in the activation of layer \(l-1\), so that
\ref{eq:activation} becomes : \begin{equation}
a^l = \sigma(z^l) = \sigma(w^la^{l-1}+b^l)
\label{eq:weighted_input}\end{equation} where \(w^l\) and \(b^l\) are
respectively the weight matrix and bias of layer \(l\). As stated,
training a neural network means adjusting the weights \(w^l\) and biases
of each layer \(b^{l}\) until the output of the neural network \(a^L\) -
the activation of the last layer \(L\) - matches the desired output
\(y_i\). We thus require a metric to define the difference between the
prediction and the desired output. This metric is known as the
\emph{cost function} \(\mathcal{L}\) and one of the most commonly used
cost functions is the mean squared error: \begin{equation}
\mathcal{L} = \frac{1}{2n}\sum_i|y_i-a^L_i|^2
\label{eq:MSE}\end{equation}

where \(n\) is the number of samples, \(y_i\) the desired output of
sample \(i\) and the prediction of the network given the inputs of
sample \(i\). As the cost function is a measure of the difference
between the prediction and desired outputs, training a neural network
comes down to minimizing the cost function. Such minimization problems
are solved by gradient descent techniques.

Gradient descent techniques are based on the fact that given some
position, the minimum from that position can be reached by
following the steepest descent. Thus, given a function \(f(\mathbf{x})\)
to minimize with respect to \(\mathbf{x}\), we guess an initial position
\(x_n\) and iteratively update it until it converges:

\begin{equation}
\mathbf{x}_{n+1} = \mathbf{x}_{n}-\gamma\nabla f(\mathbf{x}_n)
\label{eq:gradientdescent}\end{equation}

\(\gamma\) is known as the learning rate and sets the `stepsize'.
Although this is an iterative technique, if the minimization problem is
convex (i.e. no local minima), gradient descent is guaranteed to
converge to it. Note that gradient descent requires calculation of the
derivative with respect to to the variables of the function to be minimized. In other words, one needs to know the derivative of the cost function with respect to each of the weights and biases in the network. A naive finite difference
scheme would quickly grow computationally untractable, even for networks
with just two hidden layers. Alternatives to gradient descent exist, but
all require calculation of the derivatives. In the next section we
present an algorithm which is able to efficiently calculate these
derivatives.

\hypertarget{back-propagation-and-automatic-differentiation}{%
\subsubsection*{Back propagation and automatic
differentiation}\label{back-propagation-and-automatic-differentiation}}
\addcontentsline{toc}{subsubsection}{Back propagation and automatic
differentiation}

In this section we introduce the so-called \emph{backpropagation}
algorithm. The backpropagation algorithm allows for the efficient
calculation of the cost function derivatives in a neural network. For
simplicity, we move away from a vector notation and introduce
\(w^l_{jk}\), the weight of the \(k\)-th neuron in layer \(l-1\) to
neuron \(j\) in layer \(l\) and \(b^l_j\), the bias of the neuron \(j\)
in the \(l\)-th layer. We introduce the error of neuron \(j\) in layer
\(l\) as: \[
\delta^l_j=\frac{\partial C}{\partial z^l_j}
\]

We can rewrite this using the chain rule as:

\[
\delta^l_j = \sum_k \frac{\partial C}{\partial a^l_{k}}\frac{\partial a^l_{k}}{\partial z^l_{j}}
\]

The second term on the right is always zero except when \(j=k\), so the
summation can be dropped. Given equation \ref{eq:activation}, we
note that \(\partial a^l_{j}/\partial z^l_{j} = \sigma'(z^l_j)\). For
the last layer \(l = L\), we can directly calculate the derivative,
resulting in:

\begin{equation}
\delta^L_j =  |a^L_j-y_j|\sigma'(z^L_j)
\label{eq:backprop1}\end{equation}

Equation \ref{eq:backprop1} relates the error in the output layer to
its activation and weighted input. Again using the chain rule, we can
express the error in a layer \(l\), \(\delta^{l}_j\) ,in terms of the
error in the next layer, \(\delta^{l+1}_j\): \[
\delta^l_j = \sum_k \frac{\partial C}{\partial z^{l+1}_{k}}\frac{\partial z^{l+1}_{k}}{\partial z^l_{j}} = \sum_k \delta^{l+1}_k\frac{\partial z^{l+1}_{k}}{\partial z^l_{j}}
\]

Using equation \ref{eq:weighted_input}, we obtain after
substitution:

\begin{equation}
\delta^l_j = \sum_k\delta^{l+1}_kw^{l+1}_{kj}\sigma'(z^l_j)
\label{eq:backprop2}\end{equation}

Equation \ref{eq:backprop1} gives us the error in the final layer,
while equation \ref{eq:backprop2} allows us to propagate the error
back through the network - hence the algorithm is named backpropagation.
Two more expressions are needed to relate the error in each neuron
\(\delta^l_j\) to the derivatives with respect to. the weights and biases. Making
use yet again of the chain rule gives the last two backpropagation
relations: \begin{equation}
\frac{\partial C}{\partial b^l_{j}}\frac{\partial b^l_{j}}{\partial z^l_{j}}=\frac{\partial C}{\partial b^l_{j}}=\delta^l_j
\label{eq:backprop3}\end{equation}

\begin{equation}
\delta^l_j=\sum_k\frac{\partial C}{\partial w^l_{jk}}\frac{\partial w^l_{jk}}{\partial z^l_{j}}\to \frac{\partial C}{\partial w^l_{jk}}=a^{l-1}_{k}\delta^l_j
\label{eq:backprop4}\end{equation}

Given the four fundamental backpropagation relations, we state the
algorithm. It consists of four steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Complete a forward pass, i.e., calculate \(a^L_j\).
\item
  Calculate the error in the final layer using \ref{eq:backprop1}
  and propagate it backwards using \ref{eq:backprop2} to obtain the
  error in each neuron. Using \ref{eq:backprop3} and
  \ref{eq:backprop4}, calculate the derivatives required for the
  minimizer.
\item
  Perform a minimization step (e.g.~equation
  \ref{eq:gradientdescent}) and update the weights and biases.
\item
  Return to step one until the minimization algorithm in step three
  converges.
\end{enumerate}

Mathematically, back propagation is a version of a more general
technique known as automatic differentiation. Suppose we want to
calculate the derivative of some data \(f(x)\). Symbolic differentiation
would give the most precise answer, but often the function \(f\) is not
known. Furthermore, even if \(f\) would be known, it quickly becomes too
hard to calculate a symbolic derivative of a complex function \(f\). One
could then turn to numerical differentiation using some finite
difference scheme or locally fitting a polynomial whose derivative is
then calculated. All these methods require relative closely spaced data
and are very sensitive to noise. Automatic differentiation is a third type of differentiation which allows for
the precise calculation of derivatives. At its
fundamental level, any computational operation, no matter how complex,
is a long string of elementary operations whose derivative is easily
determined. Using the chain rule, we can then calculate the derivative
of any computation in terms of these smaller elementary operations. To
see this, consider a function \(f(x) = a + bx\). Writing this in terms
of elementary operations gives: \[
f(x) = a+bx = w_1+ w_2w_3=w_1+w_4=w_5
\] The derivative of each subexpression \(w_i\) is easily calculated: \[
w_1' = 0, w_2'= 0, w_3'=1,w_4'=w_2'w_3+w_2w_3', w_5'=w_4'+w_1'
\] The derivative of \(f\) is then: \begin{equation}
f' = w_5' = w_4'+w_1' = (w_2'w_3+w_2w_3')+w_1'
\label{eq:autodiff}\end{equation} We have thus expressed the derivative
of \(f\) in quantities we know and indeed, after filling in the
remaining derivatives we obtain \(f' = w_2 =b\). Note the similarity to
backpropagation; in automatic differentiation we are only interested in
the final expression on the right of equation \ref{eq:autodiff},
whereas in backpropagation we wish to know the intermediate derivatives
(i.e. \(w_5', w_4'\)) too. Back propagation is thus a version of
automatic differentiation in which the intermediate values are
calculated too. In the next section we show that automatic
differentiation enables easy encoding of physics into a neural network,
leading to a so-called Physics Informed Neural Network (PINN).

\newpage

\hypertarget{physics-informed-neural-networks-1}{%
\section{Physics Informed Neural
Networks}\label{physics-informed-neural-networks-1}}

In this section we introduce Physics Informed Neural Networks (PINNs), a
recently developed technique\textsuperscript{31, 46}
which merges physical models and neural networks. We first introduce how
PINNs encode physical laws and models in neural networks and discuss why
this yields such a powerful technique. This is illustrated by applying
it to a simple diffusive process and show that even in the presence of
noise, PINNs can accurately infer a (spatially-varying) diffusion
constant. We then apply a PINN to the RUSH data and end the chapter with
our conclusions.

\hypertarget{the-concept-1}{%
\subsection{The concept}\label{the-concept-1}}

Consider a set of spatiotemporal experimental data, \(u(x,t)\) and a
model which describes the temporal evolution of this dataset:
\begin{equation}
\partial_t u = \lambda_1 + \lambda_2 u + \lambda_3\nabla u + \lambda_4 \nabla^2 u = f(1, u, u_x, ...)
\label{eq:PDE}\end{equation} We now wish to know which value for the
parameters \(\lambda_i\) best describes the dataset \(u(x,t)\). Naively,
one could train a neural network on a training set created by
numerically solving \ref{eq:PDE} for different values
\(\lambda_{i}\) and then feed this network the experimental data
\(u(x,t)\). Although theoretically this yields the correct result, for
complex processes such as a Navier-Stokes flow or the Schrodinger
equation this quickly grows intractable due to the massive amount of
training data required for an accurate prediction.

PINNs circumvent this issue by directly encoding physical laws and
models such as \ref{eq:PDE} into the neural network. We can write
any PDE as: \begin{equation}
g = -\partial_t u + f(1, u, u_x, u_xx, u^2, ...)
\label{eq:PIcost}\end{equation}

This function \(g\) can be added to the cost function, because to satisfy
the PDE, \(g \to 0\): \[
\mathcal{L} = \frac{1}{2n}\sum_i|u_i-a^L_i|^2 + \frac{l}{n}\sum_i|g_i|^2
\]

where \(l\) sets the effective strength of the two terms. By adding
\(g\) to the cost function, it acts as `physics-regularizer' and
unphysical solutions are penalized; we have thus encoded the physics
directly into the neural network. Since neural networks also return high precision derivatives through automatic differentiation, equation \ref{eq:PIcost} can be accurately determined. Note that while we know the form of
\(g\), its coefficients \(\lambda_i\) are unknown. However, we can treat
the coefficients as variables of the cost function, i.e.
\(\mathcal{L}(w^l,b^l, \lambda)\) and thus by training the network on
the dataset \(u(x,t)\), we automatically infer the coefficients.
Consequently, we do not need a vast set of training data, as we solve the
problem \emph{by} training the network.

Theoretically, PINNs should not only be able to infer constant
coefficients, but also coefficient \emph{fields}. Instead of treating
the coefficients as a variable to be optimized, we add another output to
the network. Such a network is known as a multi-output PINN and the
difference between a single and multi output network is shown in figure
\ref{fig:PINN}. PINNs can also be used to numerically solve PDEs.
By removing the mean squared error term from the cost function but
adding initial values and boundary conditions, training the network will
now result in the network learning the solution to the PDE \(g\), whilst
respecting the given boundary and initial conditions. This alternative
means of numerically solving a model does not need advanced meshing of
the problem domain required in computational fluid dynamics or carefully
constructed (yet often unstable) discretization schemes, as it requires
the physics to be fullfilled at every point in the spatiotemporal
domain.

\begin{figure}
\hypertarget{fig:PINN}{%
\centering
\includegraphics{source/figures/pdf/PINN.pdf}
\caption{\textbf{Left panel:} a single output PINN. \textbf{Right
panel:} A multi-output PINN. The network now also predicts the
coefficients values at each data point.}\label{fig:PINN}
}
\end{figure}

\hypertarget{pinns-in-practice}{%
\subsection{PINNs in practice}\label{pinns-in-practice}}

Before applying a PINN to the RUSH data, we study a toy problem to gain
more insight into its behaviour. We also prove that a PINN is able to
correctly infer a coefficient field from noisy data. Our toy problem of
an initial gaussian 1D concentration profile: \[
c(x, 0) = e^{-\frac{(x-x_0)^2}{2\sigma}}
\]

with \(x_ = 0.5\) and \(\sigma =0.01\) diffusing in a box of length
\(L\) according to:

\begin{equation}
\frac{\partial c(x,t)}{\partial t} = \nabla \cdot[D(x)\nabla c(x,t)]
\label{eq:toyproblem}\end{equation}

on the spatial domain \([0,1]\) with perfectly absorbing boundaries at
the edges of the domain:

\[
c(0,t) = c(1,t) = 0
\]

If \(D(x) = D\), this problem can be solved using a Greens function.
Although being a simple problem, it contains all the essential features
of a PINN. For the application of a PINN to more complex systems such as
the Burgers, Schrodinger or Navier-Stokes equations, we refer the reader
to the papers of M. Raissi et al (46, 31).

\hypertarget{constant-diffusion-coefficient}{%
\subsubsection{Constant diffusion
coefficient}\label{constant-diffusion-coefficient}}

We first numerically solve equation \ref{eq:toyproblem} with a
diffusion coefficient of \(D(x) = D_0 = 0.1\) between \(t=0\) and
\(t=0.5\). Using a spatial and temporal resolution of 0.01, our total
dataset consists of 5151 samples, while we have configured the neural
network with 6 hidden layers of 20 neurons each and have set
\(\lambda=1\). The left panel of figure \ref{fig:constantD} shows
the ground truth (i.e.~the numerical solution of equation
\ref{eq:toyproblem}) and the absolute error with respect to to the groundtruth
of the neural network output.

\begin{figure}
\hypertarget{fig:constantD}{%
\centering
\includegraphics{source/figures/pdf/error_constantD.pdf}
\caption{\textbf{Left panel}: Simulated ground truth of the problem.
\textbf{Right panel}: The absolute error of neural network. Note that
most of the error is located at areas with low concentration,
i.e.~signal.}\label{fig:constantD}
}
\end{figure}

The inferred diffusion coefficient is \(D_{pred} = 0.100026\): an error
of \(0.026\%\). From the absolute error we observe that the error seems
to localize in areas with low concentration. This is a feature we have
consistently observed: in areas with low `signal', the neural network
struggles. Considering that in these areas there is simply not much data
to learn from, this is not unexpected.

The input data of the previous problem is noiseless and thus of limited
practical interest. We add \(5\%\) white noise to the data of the
previous problem and train the network on this noisy dataset. Note that
the network is now doing two tasks in parallel: it's both denoising the
data and performing a fit. In the left panel of figure
\ref{fig:error_constantD_noisy} we show the concentration profile
at times \(t = 0, 0.1\) and \(0.5\), with the prediction of the PINN
superimposed in black dashed lines at each time. On the right panel we
show the absolute error with respect to the ground truth. Observe that
the error again localizes in areas with low concentration. The inferred
diffusion constant is \(D_0 = 0.10052\): an error of \(0.52\%\).
Although the error is an order of magnitude higher compared to the
noiseless data, an error of less than \(1\%\) is extremely impressive.

\begin{figure}
\hypertarget{fig:error_constantD_noisy}{%
\centering
\includegraphics{source/figures/pdf/error_constantD_noisy.pdf}
\caption{\textbf{Left panel}: The original noisy concentration profile
at several times with the neural network inferred denoised version
superimposed. \textbf{Right panel}: The absolute error of neural network
with respect to the ground truth. Note that most of the error is located
at areas with low concentration.}\label{fig:error_constantD_noisy}
}
\end{figure}

\hypertarget{varying-coefficients}{%
\subsubsection{Varying coefficients}\label{varying-coefficients}}

As stated, it should be possible to infer coefficient fields by using a
two output neural network. We first test this on the noisy constant
diffusion (\(D_0=0.1\)) dataset of the previous problem. In this case,
while the neural network is allowed to assign a different diffusion
constant to each point in the spatiotemporal domain, it should return
\(D=0.1\) for each. Figure \ref{fig:summary_constantD} shows a
summary of the results in four panels. In the upper left we show the
data on which the network is trained, while the upper right panel shows
the predicted concentration profile. Note the excellent match between
the two. In the lower right panel we show the inferred diffusion field.
We observe a good match in the middle of the plot, but the neural
network again struggles in areas with low concentration, such as close
to the edges of the system. A more quantitative analysis of the
predicted diffusion and concentration is presented in the lower left
corner. Here we plot the Cumulative Distribution Function (CDF) of the
absolute relative error of both the concentration and the diffusion
constant. Note that the PINN predicts the concentration very well, with
roughly \(80\%\) of the points having less than \(5\%\) error, but
struggles more with the diffusion coefficient. Given that the diffusion
coefficient is inferred self-consistently thorugh its role in the
physics-informed part of the cost function, this is not unexpected.

\begin{figure}
\hypertarget{fig:summary_constantD}{%
\centering
\includegraphics[width=0.9\textwidth]{source/figures/pdf/summary_constantD_varyingPINN.pdf}
\caption{We show the training data and predicted concentration profile
in the upper left and right panels. The lower right panel shows the
inferred diffusion field while the lower left panel shows the CDF of the
relative error of the diffusion and
concentration.}\label{fig:summary_constantD}
}
\end{figure}

In figure \ref{fig:summary_varyingD} we show a similar analysis for
data with a non-constant diffusion field. Equation
\ref{eq:toyproblem} has been numerically solved on a grid consisting
of 50000 points and diffusion constant profile
\(D(x) = 0.2 + 0.1\tanh(x)\). Remarkably, the neural network is able to
accurately infer the network with \(85\%\) of the diffusion field having
an error of less than \(10\%\). In figure \ref{fig:projectionD} we
show the inferred diffusion profiles in more detail by projecting them
along the time axis. Observe that, yet again, the error is largest where
the signal is lowest. Nonetheless, we have proven that a neural network is
able to accurately infer a coefficient field from noisy data.

\begin{figure}
\hypertarget{fig:summary_varyingD}{%
\centering
\includegraphics[width=0.9\textwidth]{source/figures/pdf/summary_varyingD_varyingPINN.pdf}
\caption{We show the training data and predicted concentration profile
in the upper left and right panels. The lower right panel shows the
inferred diffusion field while the lower left panel shows the CDF of the
relative error of the diffusion and
concentration.}\label{fig:summary_varyingD}
}
\end{figure}

\begin{figure}
\hypertarget{fig:projectionD}{%
\centering
\includegraphics[width=0.8\textwidth]{source/figures/pdf/projection.pdf}
\caption{Projection of the inferred diffusion profile along the time
axis.}\label{fig:projectionD}
}
\end{figure}

\hypertarget{real-cell}{%
\subsubsection{Real cell}\label{real-cell}}

We now apply the PINN technique to the RUSH data. Having observed in the
previous section that the technique struggles in domains with low
signal, we select a subset of the data consisting of 10 by 10 pixels
during the first 30 frames, thus giving a dataset of 3000 points. We
first fit this data assuming that it is described by a single diffusion
coefficient and advection speed. The physics informed part of the cost
function is thus: \[
g=0=-\partial_tc+D(\partial_{xx}c+\partial_{yy}c)-v_x\partial_xc-v_y\partial_yc
\] We train the network on the raw data: none of the filtering
procedures presented in the model fitting chapter are used. The neural
network gives the following results:
\(D=-3\cdot10^{-6}, v_x=0.82, v_y=0.32\), whereas the least-squares
fitting gives \(D=0.049, v_x=-0.046, v_y=0.013\). These results are
completely different: the least squares predicts a diffusion constant
and velocity roughly on the same order, whereas the neural network
predicts a negligibly small diffusion constant. The direction and magnitude
of the advection is different as well. To gain more insight into the
fit, we study the concentration profiles of frame 5 as given by the
original noisy signal, the output of the neural network, the filtered
signal and the reconstructed signal of the least-squares fit. The signal is
reconstructed by propagating the first frame using the calculated time derivative with the optimal fit parameters. The result is shown in figure
\ref{fig:nnconstant}.

\begin{figure}
\hypertarget{fig:nnconstant}{%
\centering
\includegraphics{source/figures/pdf/NN_Man_constant.pdf}
\caption{In the upper right panel we show the unfiltered data of our subset for frame 5. The upper right panel shows the inferred profile by the PINN, while the lower two panels show respectively the filtered data and the data reconstructed from the least squares fit.}\label{fig:nnconstant}
}
\end{figure}

As can be observed from figure \ref{fig:nnconstant}, the inferred
concentration profile by the neural network matches the raw data very
well, while the least squares fit does not. Since we have taken a 10 by
10 pixel patch of the data, this a fairly small scale and seeing such a
close resemblance to the raw data might mean our model is fitted to the noise instead of the signal. Later frames
reveal an inferred concentration profile less like the raw data however. Concludingly, the neural network seems to outperform the
least-squares method, but due to the small scale of our data, no clear verdict can be rendered. Increasing the scale of our data however make a cost function with constant coefficients unlikely. 

In the previous section we proved that PINNs are able to infer
coefficient fields. We now try to infer the coefficient fields for our
subset of data. In figure \ref{fig:nnfull} we show the result for a
single frame.

\begin{figure}
\hypertarget{fig:nnfull}{%
\centering
\includegraphics{source/figures/pdf/NN_Man_full.pdf}
\caption{We show the raw data, inferred concentration, diffusion and advection fields and the physics informed cost as a function of the frames.}\label{fig:nnfull}
}
\end{figure}

In the six panels, we show respectively the raw data, the inferred
concentration profile, the diffusion coefficient and advection and in
the lower right corner the physics informed cost \(g\) per frame.
Observe that the diffusion and advection profiles are exactly equal.
Inspection of the concentration profile derivates
\(c_t, c_x, c_{yy}...\) shows no aberrant behaviour, implying that the
neural network is functioning properly. These diffusion and advection
profiles thus minimize the cost function but having similar coefficient
values at each point is unlikely. Recall also that the diffusion
coefficient obtained in the constant coefficient model above is orders of
magnitude smaller than the advection. Inspection of the physics informed
part of cost function shows that is on the order of \(10^{-3}\).
Although one to two orders of magnitude higher than a typical cost for synthetic data, for
real experimental data this does not seem suspiciously high. Concludingly, our results are clearly incorrect, but the neural network seems to
perform properly and we thus cannot speculate on the causes of these results.

\hypertarget{Golgi-as-a-phase-separated-droplet}{%
\chapter{Golgi as a phase separated
droplet}\label{Golgi-as-a-phase-separated-droplet}}

In the first part of thesis we investigated if intracellular transport could be described by an advection-diffusion equation by analysing experimental data of the RUSH experiments. Unfortunately, the data resisted all our methods and no conclusion could be drawn. In this second part of the thesis we turn the question on its head and theoretically investigate if the Golgi can be described as an active, phase separated droplet in a diffusive-advective flow. In this chapter we present a general introduction to phase separation, followed by a section where we discuss an approximation known as the effective-droplet approximation. This approximation makes the phase separation analytically tractable. We then introduce our model and biologically justify it. We study the models' behaviour in the next chapter.

\hypertarget{phase-separation}{%
\section{Phase separation}\label{phase-separation}}

Consider a mixture of two molecules A and B, with underling
interaction strengths \(\chi_{ij}\). Defining an \emph{order parameter}
\(c=N_A/N_B\), the system can either be in a mixed state with $c=\bar{c}$ everywhere or in a phase separated state with two states of concentration $c_0^-$ and $c_0^+$, depending on the strength and sign of 
of the interactions $\chi_{ij}$. The phase separated state can be described by a phenomenological free energy density function $f$ with two minima at $c_0^-$ and $c_0^+$\textsuperscript{47}:
\[
f(c) = \frac{b}{2(\Delta c)^2}(c-c_0^-)^2(c-c_0^+)^2
\]

where \(b\) characterises the strength of molecular interactions and
\(\Delta c = |c_0^--c_0^+|\). This free energy density function is shown in figure \ref{fig:freeenergy} for $c_0^-=0.1, c_0^+=0.9$.

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{source/figures/pdf/Freeenergydensity.pdf}
	\caption{Phenomenological free energy density with minima at $c_0^-$ and $c_0^+$.}
	\label{fig:freeenergy}
\end{figure}

The total free energy of the system must also include a term penalising sharp gradients, so that we write the free energy of the system as 

\begin{equation}
	\label{eq:totfree}
	F(c) = \int dV (f(c)+\frac{1}{2}\kappa (\nabla c)^2
\end{equation}

This is known as a Ginzburg-Landau free energy. The equilibrium concentration profile is found by minimising equation \ref{eq:totfree}:

\begin{equation}
\frac{\delta F}{\delta c} = f'(c)-k\nabla^2c=\mu(x) =0,
\label{eq:euler}\end{equation}

where \(\delta F/\delta c\) is a functional derivative, as we minimize
with respect to the concentration \emph{profile}. In general, no analytical solution can be found due to the non-linear nature of the equation. We see however that the free energy density $f$ is minimised by two phases of concentration $c_0^-$ and $c_0^+$, but that these are separated by a domain wall with a finite width due to the inclusion of the second term in equation \ref{eq:totfree}.

If a system initially in a mixed state is quenched so that it becomes unstable, fluctuations throughout the system will give rise to a maze-like concentration pattern in a process known as spinodal decomposition. This is shown for different times in figure \ref{fig:maze}.

\begin{figure}
\hypertarget{fig:maze}{%
\centering
\includegraphics{source/figures/png/CahnHilliard.png}
\caption{Cahn hilliard domains}\label{fig:maze}
}
\end{figure}

The domains grow in time in a process known as coarsening. This process will continue until only two separate domains are left: one with concentration $c_0^-$ and one with $c_0^+$. We can derive an equation governing the dynamics shown in figure \ref{fig:maze}. Since the order parameter is a concentration, it can only exchange locally so that: 
\[
\partial_t c = -\nabla \cdot \mathbf{j}
\]

where \(\mathbf{j}\) is a flux. This type of dynamics of phase separation is also known as 'Model-B'. We can relate the flux to the chemical
potential:

\[
\mathbf{j} = -m \nabla \mu
\]

where \(m\) is a mobility. Equation\ref{eq:euler} also gives us an expression for the chemical potential, so that we finally obtain the \emph{Cahn-Hilliard equation}:

\[
\frac{\partial c}{\partial t}=m\nabla^2[\frac{df}{dc}-\kappa\nabla^2c]
\]

It is this equation which governs the behaviour observed in figure
\ref{fig:maze}. Due to its non-linearity and fourth order
derivatives solving the Cahn-Hilliard is usually forsaken in
favour of deriving scaling relations, which for example show that the domain size $R$ scales with time $t$ for some constant $\alpha$: $R \sim t^{\alpha}$. Another option is to study the system in the so-called effective droplet approximation, as we do in the next section.

\hypertarget{effective-droplet}{%
\section{Effective droplet}\label{effective-droplet}}

Consider a phase one-dimensional phase separated system. If the domain wall is extremely thin, we can approximate the system by describing it as two bulk phases separated by an interface. We effectively split the system into two separate problems - one concerning the dilute phase and one concerning the dense phase- and match them at the interface through appropriate boundary conditions. By assuming the interface to be at thermodynamic equilibrium, the growth of the droplet is described in terms of the fluxes across the interface. This is shown in figure \ref{fig:effectivedroplet}, where we have approximated the concentration profile (blue line) by an effective droplet which exchanges material with its environment through its interface (black dashed line).

\begin{figure}
\hypertarget{fig:effectivedroplet}{%
\centering
\includegraphics{source/figures/pdf/effectivedroplet.pdf}
\caption{Model of an effective droplet. Blue line is full Cahn-Hilliard model,
black dashed line effective droplet.}\label{fig:effectivedroplet}
}
\end{figure}

Consider again the Cahn-Hilliard equation. In the bulk phase, the interfacial term can be ignored so that we obtain:

\begin{equation}
\label{eq:diffusion}
\frac{\partial c}{\partial t} = m\nabla^2\mu.
\end{equation}

Linearising the chemical potential $\mu$ around an equilibrium concentration yields

\begin{equation}
\frac{\partial c }{\partial t} = mf'(c_0^\pm)\nabla^2c = D\nabla^2c,
\label{eq:diffusion}
\end{equation}

where all multiplicative constants can be absorbed into a single constant $D$. Observe that linearising the chemical potential yields a diffusion equation. A solution to equation \ref{eq:diffusion} requires a boundary condition at the interface of the droplet. We derive the boundary conditions at the interface by assuming the interface to be at thermodynamic equilibrium. Consider a phase-separated system with an infinitely thin interface. The total free
energy of the system can then be written as: \[
F = V_1 f(c_1) + V_2 f(c_2)
\]

where \(V_i\) and \(c_i\) are respectively the volume and
concentration of phase \(i\) and \(f(c_i)\) is the free energy
density. Assuming incompressibility (\(V_1+V_2=V\)) and conservation of
particles (\(V_1c_1+V_2c_2=Vc\)) constrains the system to two
free variables, so that minimising the free energy with respect to
\(c_1\) and \(V_1\) gives two conditions:

\[
f'(c_1) = f'(c_2)
\]

\[
0 = f(c_1) + f(c_2) + (c_2-c_1)f'(c_2)
\]

Since \(f'(c) = \mu(c)\), the first condition requires that both
phases have the same chemical potential, while the second one
states that the pressure in each phase must be equal. The obvious solution
to these equations is a mixed state with \(c_1=c_2\). A
non-trivial phase-separated solution can be found as well using Maxwells tangent construction. We thus see that an equilibrium also exists if the system is phase separated into two phases at concentration $c_0^-$ and $c_0^+$, the minima of the free energy density function. Note that this is valid for our 1D description. In higher dimensions, the curvature of the droplet will affect the boundary conditions due to the Laplace pressure and one can show that this leads to an extra term which scales with the inverse radius\textsuperscript{48}.

Having defined the boundary conditions at the interface, the stationary concentration profile (i.e. $dc/dt=0$) inside and outside the droplet can be solved. From these concentration profiles the fluxes at the interface which determine the droplet growth can be calculated. We show this in the next section.

\newpage
\hypertarget{fluxes-activity-and-interfaces}{%
\subsection{Fluxes, activity and
interfaces}\label{fluxes-activity-and-interfaces}}

Given a concentration profile \(c(x)\), a diffusive flux can be
calculated by applying Ficks' second law:

\[
J(x) = -D\frac{\partial c}{\partial x}
\]

Using this expression, the flux at the interface on the inside and
outside of the droplet, \(J_{in}\) and \(J_{out}\), can be calculated.
Note that 'in' and 'out' refer respectively to inside and outside of the
droplet rather than the direction of the flux; the boundary conditions
fix the concentration at the interface but not the fluxes. If $J_{in}$ and $J_{out}$ are not balanced, a net flux exists across
the interface, which leads to either growth or decay of the droplet. This change in droplet radius can be described by the interface velocity \(v_n\). We now
derive an expression for \(v_n\) in terms of the fluxes across it. To
move an interface a distance \(\Delta x\), a net material gain of
\(\Delta x \Delta c\) is required. This net gain is determined by the net flux
in a time \(\Delta t\), so that: \[
\Delta x \Delta c = (J_{in}-J_{out})\Delta t
\] which can be rewritten as: \begin{equation}
\frac{\Delta x}{\Delta t} = v_n = \frac{J_{in}-J_{out}}{\Delta c}
\label{eq:interfacespeed}\end{equation} 

In a passive droplet the concentration profile is a solution of the Laplace equation: $\nabla^2c=0$ and $J_{in}$ will thus be zero, meaning that the interface speed will always be bigger zero. Indeed, passive droplets grow to infinity in an infinite system. We show in the next chapter that by making the droplet \emph{active}, the flux $J_{in}$ will be non-zero. This has two important consequences. First, if $J_{in}=J_{out}$, the interface velocity is zero and the droplet will have a stable radius. The second effect is that an active droplet moves itself up a concentration gradient
Consider a droplet of radius \(R\) at position \(x_0\) with two interfaces moving respectively at \(v_l\) and \(v_r\). In a time \(dt\), the droplet moves to a new position \(x_0+dx\) and will have a new radius \(R+dR\): 
\[
x_0-R+v_ldt=x_0+dx_0-(R+dR)
\] \[
x_0+R+v_rdt=x_0+dx_0+(R+dR)
\]

Solving this set of equations for \(dx_0\) and \(dR\) gives:

\begin{equation}
\frac{dR}{dt}=\frac{1}{2\Delta c}(v_r-v_l)
\label{eq:radius}\end{equation}

\begin{equation}
\frac{dx_0}{dt}=\frac{1}{2\Delta c}(v_l+v_r)
\label{eq:position}\end{equation}

Combining these equations with equation \ref{eq:interfacespeed}
relates the growth and movement of the droplet to the fluxes
across the interface: 

\begin{equation}
	\label{eq:dRdt}
	\frac{dR}{dt}=\frac{1}{2\Delta c}\left[(J_{in}^{x=R}-J_{in}^{x=-R})+(J_{out}^{x=-R}-J_{out}^{x=R})\right]
\end{equation}

\begin{equation}
\label{eq:dx0dt}
	\frac{dx_0}{dt}=\frac{1}{2\Delta c}\left[(J_{in}^{x=-R}+J_{in}^{x=R})-(J_{out}^{x=-R}+J_{out}^{x=R})\right]
\end{equation}

Assuming the droplet is symmetric, $J_{in}^{x=-R}+J_{in}^{x=R}=0$, so equation \ref{eq:dx0dt} simplifies to

\begin{equation}
	\frac{dx_0}{dt}=-(J_{out}^{x=-R}+J_{out}^{x=R})
\end{equation}

If the droplet is in a concentration gradient $c = \alpha+\beta x$, we can estimate the flux at $x=-R$ as $-D(\alpha-c_0^-)/l$, with $l$ some lengthscale. At $x=R$ the flux is then $-D(c_0^--(\alpha+2\beta R)/l$. We thus obtain

\begin{equation}
	\frac{dx_0}{dt} \propto -\frac{2DR\beta}{l}
\end{equation}

and we see that an active droplet moves up the gradient. Equations \ref{eq:dx0dt} and \ref{eq:dRdt} completely determine the behaviour of the droplet. In the effective droplet model we thus write everything in terms of fluxes across the interface and then use equations \ref{eq:dx0dt} and \ref{eq:dRdt} to find a stable droplet.

\hypertarget{Golgi-as-an-active-droplet}{%
\section{Golgi as an active droplet}\label{Golgi-as-an-active-droplet}}

In the introduction we justified using a phase-separation approach to
describe the Golgi. In this section we develop our model for the Golgi
from biological considerations, but having established the mathematical
background of phase separation, we present the mathematical
description in parallel. Due to the addition of advection to our system, our model is unsolvable in any dimension higher than 1D. Not only do we ignore the Laplace pressure, the replacement of a single continuous interface of the droplet in 2D or higher by two separate interfaces in 1D precludes any shape deformations, which could play a big role. 

Our model comprises four different populations: Immature
cargo, mature cargo, the Golgi itself and the cytoplasm, which acts as the solvent. A typical 'cycle' would consist of immature cargo entering the system, being transported towards the Golgi where it is turned into the material comprising the Golgi before being matured after which it is transported out of the system. The solvent plays no explicit role in this and we thus model it implicitly by describing the cargo in terms of concentration. We also ignore the mature cargo for now and simply assume the immature cargo decays inside the droplet. As stated, we model the Golgi as a phase separated droplet, so that we can describe the system in terms of a single concentration $c$, with the dilute phase corresponding to immature cargo and the dilute phase representing the Golgi.

Using the effective droplet concentration, we have essentially 'decoupled' the dense and dilute phase. We exploit this to couple our model for the intracellular transport to the Golgi. The dilute phase corresponds to immature cargo being trafficked towards the Golgi and hence is similar to the concentration profile set up by the intracellular transport. We thus use an advection-diffusion equation to model the concentration profile of this phase. As some evidence exists of vesicles degrading\textsuperscript{8}, we add an additional decay term to the intracellular transport model, so that we obtain:

\begin{equation}
D\partial_x^2 c(x) - v\partial_xc(x)-ac(x)=0
\label{eq:cinside}\end{equation}

with \(v\) an advection velocity and \(a\) some decay constant. Note that equation \ref{eq:cinside} is exactly the model we have used to fit the experimental data, but we have now added an additional degradation term.

We now turn our attention to the dense phase. Upon adding the drug nocadazole to mammalian cells, microtubules are depolymerised and the Golgi ribbon breaks up into separate stacks\textsuperscript{50}. These stacks are fully functional\textsuperscript{7} but move away from their perinuclear
location to colocate with an ERES. Identifying the dense phase with a single Golgi stack instead of the entire Golgi ribbon allows us to use a one-dimensional approach where a droplet can move from one side of the system representing the Golgi ribbon, to the other side representing the ERES. 
As each stack is fully functional, we make no simplifications with respect to the function of the Golgi. Since we are not interested in how the Golgi matures the cargo (see the cisternal maturation versus vesicular transport models), we model it as decay term with maturation rate $k$, leading to the following equation for the dense phase:


\[
\frac{\partial c}{\partial t} = D\nabla^2 c - kc
\]

The addition of this maturation term makes the droplet \emph{active}. Studies have found that newly formed stacks are moved from their location close to the ERES to the Ribbon by active transport\textsuperscript{8}, so we also add active transport, modelled by and advection term, to the dense phase:

\begin{equation}
D\partial_x^2 c(x) - v\partial_xc(x)-kc(x)=0
\label{eq:coutside}
\end{equation}

We assume for simplicity that both phases are advected with velocity $v>0$ and have diffusion constant $D$. The only difference between the two phases then is the decay rate $a$ and the maturation constant $k$. As stated, we model our system in 1D, with one boundary representing the
ERES and the other boundary as the location of Golgi Ribbon. We place
the ERES on the left side of the system and thus model this boundary as a
source: \[
(-D\partial_xc+vc)|_{x=0} = J_{in}
\]

The other side of the system represents the Golgi Ribbon. Here, many different stacks come together due to the advection and we thus model it as a zero-flux boundary: \[
(-D\partial_xc+vc)|_{x=L} = 0
\]

As our free energy function has minima at \(c_0^+\) and \(c_0^{-}\), the boundary conditions at the interface between the dense and dilute phase are: \[
c(x_0\pm R)=
\begin{cases}
    c_0^+,& \text{inside}\\
    c_0^-,& \text{outside}
\end{cases}
\]

We thus model the Golgi as an active droplet in a concentration gradient, similar to\textsuperscript{51}, but have also added advection. We solve our model in the next chapter both analytically and numerically to investigate the behaviour of advected active droplets. 


\hypertarget{results-model}{%
\chapter{Results model}\label{results-model}}

The previous chapter introduced phase separation and our model for the
Golgi as a phase-separated droplet. In this chapter we study the
behaviour of such a model. In the first section, we analytically solve the
model for a free droplet, i.e. a droplet free to move throughout the
system. Using these expressions, we investigate the effect of advection
on an active droplet and study the steady states of our model.

Considering the biology, the diffusion constant \(D\) and the decay
rates \(k\) and \(a\) will most likely be system parameters and thus
fixed. On the other hand, the advection speed \(v\) encompasses the
active transport across the microtubules and could easily vary,
depending on the amount of molecular motors available and the rate at
which they hydrolyse ATP; the influx \(J_{in}\) is dependent on the activity
of the ER and will probably vary too. We are thus interested in
creating phase diagrams of the steady state radius and position as a
function of\(J_{in}\) and \(v\).

In the second section we a slightly modified model in which the droplets are located at the edge of the system. Taking a broader view, we study when phase separation takes place and if an effective droplet exists when the system separates. We also numerically study the model and validate the effective droplet model by confirming mass conservation. The
chapter ends with a short section discussing our conclusions and
possible biological connections.

\hypertarget{effective-droplet-1}{%
\section{Effective droplet}\label{effective-droplet-1}}

In this section we derive analytical expressions for the fluxes across
the interface of the droplet. We present the most general case,
including advection, decay and maturation and derive simplified
expressions later. Both the dense and dilute phase are described by an
advection-diffusion-decay equation, which has a general solution given
by \begin{equation}
c(x) = C_1e^{-\frac{x}{l^-}}+C_2e^{\frac{x}{l^+}}.
\label{eq:cgeneral}\end{equation}

We have defined a lengthscale \(l^\pm\) as

\begin{equation}
l^\pm= \frac{2D}{\sqrt{4kD+v^2}\pm v },
\label{eq:lengthscale}\end{equation}

where the maturation rate \(k\) should be replaced by the decay rate
\(a\) in the dilute phase. Note it is a combination of a lengthscale set
by the diffusion \(l_D=\sqrt{D/k}\) and a lengthscale set by the
advection \(l_v=2D/v\): \[
\frac{1}{l^{\pm}} = \sqrt{\frac{k}{D}+\left(\frac{v}{2D}\right)^2}\pm\frac{v}{2D}=\sqrt{\frac{1}{l_D^2}+\frac{1}{l_v^2}}\pm\frac{1}{l_{v}}.
\] We have defined symmetric boundary conditions for the droplet,
\(c(R)=c(-R)=c_0^+\). Solving equation \ref{eq:cgeneral} with these
boundary conditions will lead to a convex concentration profile. In a system without advection we have \(l^+=l^-=l_D\) and the concentration profile will thus be symmetric around \(c(0)\). If \(v>0\) however, we have \(l^- >l^+\) and
the droplet is no longer symmetric around \(c(0)\); rather, the position
of the minimum concentration moves right, while the minimum
concentration itself increases. This is shown in figure
\ref{fig:concprofile}, where we have plotted a concentration
profile for \(v=0\) in blue and \(v>0\) in orange.

\begin{figure}
\hypertarget{fig:concprofile}{%
\centering
\includegraphics[width=0.6\textwidth]{source/figures/png/concprofile.png}
\caption{Concentration profiles inside an active droplet for v=0 (blue)
and v\textgreater{}0 (orange). Note that the minimum concentration
increases and that its location moves right.}\label{fig:concprofile}
}
\end{figure}

For a diffusive-advective flow, the flux is determined by
\(J(x) = -D\partial_xc(x)+vc(x)\) and applying this to the droplet
concentration yields the fluxes. The fluxes itself are not particularly
insightful, but considering equations \ref{eq:radius} and
\ref{eq:position}, we can define a \emph{maturation flux}
\(J_{mat} = J_{in}^{x=R}-J_{in}^{x=-R}\) and a \emph{positional flux}
\(J_{pos} = J_{in}^{x=R}+J_{in}^{x=-R}\), so that \begin{equation}
\frac{dR}{dt}=\frac{1}{2\Delta c}\left[J_{mat}+(J_{out}^{x=-R}-J_{out}^{x=R})\right]
\label{eq:drdtalt}\end{equation}

\begin{equation}
\frac{dx_0}{dt}=\frac{1}{2\Delta c}\left[J_{pos}-(J_{out}^{x=-R}+J_{out}^{x=R})\right]
\label{eq:dxdtalt}\end{equation}

The maturation flux \(J_{mat}\) is the flux at the interface due to the
maturation in the droplet. Note it is solely determined by the diffusive
flux, as 

\begin{multline}
	J_{in}^{x=R}-J_{in}^{x=-R} = (-D\partial_xc(x)+vc(x))|_{x=R}-(-D\partial_xc(x+vc(x))|_{x=-R}\\=D(\partial_xc(x)|_{x=-R}-\partial_xc(x)_{x=R}).
\end{multline}

The maturation flux does have a dependence on the advection through the concentration profile. Since \(J_{rad}\) is solely
determined by the diffusive flux and the solutions of
\ref{eq:cgeneral} are convex, the fluxes at the two interfaces have
opposite signs. More so, \(J_{in}^{x=R}<0\) and \(J_{in}^{x=-R}>0\), so
that \(J_{mat}<0\). This means that the droplet will shrink unless
sustained by some influx from outside the droplet, as can be seen from
equation \ref{eq:drdtalt}. If the maturation flux is exactly
balanced by this influx, the droplet radius remains stable. Whereas
passive droplets will grow to an infinite radius, active droplets remain
at a finite radius due to their suppresion of the Ostwald
Ripening\textsuperscript{49}. For our particular choice of boundary
conditions, we have derived for \(J_{mat}\): \begin{equation}
J_{mat} = \frac{-2c_0^+D}{l}\frac{\sinh\frac{R}{l^-}\sinh\frac{R}{l^+}}{\sinh\frac{R}{l}},
\label{eq:Jmat}\end{equation} where we have defined an `effective
lengthscale' \(l\) as \[
l = \frac{l^+l^-}{l^++l^-}.
\] 
Notice that if $k=0$, the maturation flux vanishes; without the activity the concentration profile inside the droplet would be flat and no maturation flux exists. Also note that as long as $k\neq 0$, the maturation flux will be non-zero as well and the system is thus out of equilibrium. This is a key aspect of active matter: a steady state may be reached (i.e $dR/dt=0$), but the system will have non-zero fluxes and hence will not be at thermal equilibrium.

For a small, non-advected droplet, \(l^-=l^+=l_D\) , \(l=l_D/2\) and
\(R\ll l_D\) , we can approximate the maturation flux as
\begin{equation}
J_{mat}=-2c_0^+kR.
\label{eq:Jmatapprox}\end{equation} Effectively, we have approximated
the concentration profile inside the droplet as \(c(x)=c_0^+\), so that
the flux lost due to decay with rate \(-k\) for a droplet with size
\(2R\) indeed gives equation \ref{eq:Jmatapprox}. One would expect that the limit of \(R\to \infty\) would yield an infinite flux
but taking the limit of \ref{eq:Jmat} gives \[
\lim_{R\to\infty} = -2c_0^+\sqrt{kD},
\] 
which shows that the flux saturates for
\(R>\sqrt{D/k}=l_D\). When \(R\gg l_D\), the concentration in the middle
of the droplet drops to zero and since the maturation scales with the
concentration, the flux saturates. In this regime, the effective droplet
theory is not valid and hence we require that \(R<l_D\). In the case of
an advected droplet this is a more subtle point, as advection increases
the minimum concentration inside the droplet (as can be seen in figure
\ref{fig:concprofile}). We study this numerically in the next
section.

The positional flux \(J_{pos}\) is the internal flux which leads to
droplet movement. For our set of boundary conditions, we have derived
\begin{equation}
J_{pos} = 2c_0^{in}D\left[\frac{Pe_-}{l_-}\frac{\sinh\frac{R}{l_+}\cosh\frac{R}{l_-}}{\sinh\frac{R}{l}}-\frac{Pe_+}{l_+}\frac{\sinh\frac{R}{l_-}\cosh\frac{R}{l_+}}{\sinh\frac{R}{l}}\right],
\label{eq:Jpos}\end{equation} where we have defined the Peclet-like
numbers \[
Pe^\pm = 1 \mp \frac{vl^\pm}{D}
\] In a passive droplet \(c(x)=c_0^+\) so that the positional flux
equals \(2c_0^+v\), but in an active droplet we need to take into
account the internal diffusion. Recall that the diffusive fluxes point
inwards and hence are aligned antiparallel, whereas the advective
fluxes are aligned. The net flux at the two interfaces is thus
different, leading to equation \ref{eq:Jpos} instead of \(2c_0^+v\).

We now turn to the fluxes on the outside of the droplet. A droplet of
radius \(R\) at position \(x_0\) has its interfaces at \(x_0 \pm R\) and
defining \(x_1=x_0-R\) and \(x_2=x_0=R\) we have derived the following
expressions for the flux at the interfaces \begin{equation}
J_{out}^{x=-R} = J_{in}\frac{(1+\frac{l_-}{l_+})e^{\frac{-x_1}{l_-}}}{Pe_-+Pe_+\frac{l^-}{l_+}e^{\frac{-x_1}{l}}}
+\frac{c_0^{out}D}{l_-}\frac{Pe_+(1-e^{\frac{-x_1}{l}})}{\frac{l^+}{l^-}+\frac{Pe_+}{Pe_-}e^{\frac{-x_1}{l}}}
\label{eq:leftflux}\end{equation}

\begin{equation}
J_{out}^{x=R} = -c_0^{out}D\frac{Pe_-Pe_+(1-e^{\frac{-x_2+L}{l}})}{l_+Pe_-+e^{\frac{-x_2+L}{l}}l_-Pe_+}
\label{eq:rightflux}\end{equation}

Although not particularly enlightening, we note the similarity between
the second term of \ref{eq:leftflux} and \ref{eq:rightflux}. The
flux on the left of the droplet has another term in \(J_{in}\),
accounting for the source we have placed at the left boundary. In the
next section we study the phase diagram equations \ref{eq:Jmat},
\ref{eq:Jpos}, \ref{eq:leftflux} and \ref{eq:rightflux} give
rise to.

\hypertarget{free-droplet}{%
\section{Free droplet}\label{free-droplet}}

In this section we study the phase diagram of free droplets and their
steady states. More specifically, we wish to investigate when droplets
have a stable state (i.e. \(dR/dt=dx_0/dt=0\)) at some position \(x^*\)
in the system. The first configuration we study ignores the decay
outside the droplet, i.e. \(a=0\). In this case, the outside fluxes
become constant and independent of the location of the droplet, as the
only way for the cargo to `exit' the system is to mature in the droplet:

\[
J_{out}^{x=-R} = J_{in}
\]
\[
J_{out}^{x=R} =0
\]

The flux on the right interface of the droplet is zero as there is no
source nor decay and in our quasi-steady state approximation the flux
then must be equal to zero. The equations for the flux in the droplet remain
unchanged as they are independent of the transport parameters.
Developing the internal droplet fluxes for \(R\ll l^\pm\) gives:
\begin{equation}
J_{rad}\approx -2 c_0^+kR
\label{eq:jrad}\end{equation}

\begin{equation}
J_{pos}\approx2 c_0^+v 
\label{eq:posfluxapprox}\end{equation}

Putting these expressions in equations \ref{eq:drdtalt} and
\ref{eq:dxdtalt} gives \begin{equation}
\frac{dR}{dt} \approx \frac{1}{2\Delta c}(J_{in}-2 c_0^+kR),
\label{eq:drdtapproxnodecay}\end{equation}

\[
\frac{dx_0}{dt} \approx \frac{1}{2\Delta c}(2 c_0^+v-J_{in}).
\]

The stable radius, \(R_{stable} \approx J_{in}/2c_0^+k\) is thus
independent of the velocity \(v\) and the droplet will maintain its
position if \(v_{stable}\approx J_{in}/2c_0^+\) . This means that, save
for \(v_{stable}\), the droplet will always move either right or left
and that the movement direction switches at the switching velocity
\(v_{stable}\). It is non-zero due to the self-movement of an active
droplet; recall that an active droplet will move itself up a
concentration gradient. The advection needs to compensate for this
movement, giving rise a non-zero \(v_{stable}\).

We now study this system numerically. As the fluxes on the outside of
the droplet are independent of the velocity, we are in fact studying the
effect of advection on an active droplet, irrespective of its
environment. We plot the stable radius of the droplet in figure
\ref{fig:stableradnodecay} and the corresponding minimum
concentration in \ref{fig:minconnodecay} . We have used the
following parameters: \(D=1, k=0.1, c_0^+=0.9\).

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics{source/figures/pdf/Stable_nodecay.pdf}
        \caption{The stable radius as a function of the velocity \(v\) and
influx \(J_{in}\). The dashed line is the line
\(dx_0/dt=0\).\label{fig:stableradnodecay}}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics{source/figures/pdf/minimumconcentration.pdf}
        \caption{The minimum concentration in a stable droplet as a function of the velocity \(v\) and influx \(J_{in}\). In areas with a low
concentration the effective droplet model is not valid.
        \label{fig:minconnodecay}}
    \end{subfigure}
    \caption{}
\end{figure}

Note that the stable radius of small droplets is independent of the
velocity, but that we do observe some dependence for bigger droplets.
However, concurrently with the size increase is the minimum
concentration decrease, as shown in figure \ref{fig:minconnodecay} .
For very low \(v\) and high \(J_{in}\), the concentration even drops to
0.4 - a concentration corresponding to the dilute well of the free
energy and thus clearly unphysical. Increasing \(v\) raises the minimum
concentration, while also slightly decreasing the radius of the droplet.
To understand this decrease in radius, consider again figure
\ref{fig:concprofile}. Calculating some average concentration
\(\bar{c}=\frac{1}{V}\int c(x)dV\), it is clearly visible in this figure that $\bar{c}$ is
higher for the advected droplet. Estimating the maturation flux as
\(J_{mat}\propto -2Rk\bar{c}\), an advected droplet thus has a higher
maturation flux than a non-advected droplet. The maturation flux needs
to be balanced by the influx \(J_{in}\) for a stable droplet so that
\begin{equation}
J_{in}=2kR\bar{c}.
\label{eq:jmathandwavy}\end{equation}
 Since both \(J_{in}\) and \(k\)
are fixed, \(R\) must decrease and thus advection compacts active
droplets. The superimposed dashed line in figure
\ref{fig:stableradnodecay} corresponds to \(dx_0/dt=0\) and thus
represents the stable droplets for which \(dR/dt=dx_0/dt=0\). Observe
that for small \(v\) it indeed shows a linear dependence between
\(J_{in}\) and \(v\) as predicted, but that for higher \(v\) we do
observe some non-linearity. Due to the low concentrations those areas
are unphysical however. We now investigate the stability of this line by
perturbing \ref{eq:drdtapproxnodecay} around \(R_{stable}\). We
obtain 
\[
\frac{d\delta R}{dt}=-2c_0^+k\delta R.
\] 
Since both \(k>0\) and \(c_0^+>0\), any fluctuations decay; the
steady state is stable. The system we have studied so far is completely
independent of the position in the system as the outside fluxes are
constant. By including decay outside the droplet, i.e. \(a\neq 0\), the
outside fluxes will become dependent on the position of the droplet.

We solve equations \ref{eq:Jmat}, \ref{eq:Jpos},
\ref{eq:leftflux} and \ref{eq:rightflux} numerically by finding
the \(x_0^*\) and \(R^{*}\) for which \(dx_0/dt=dR/dt=0\) inside our
system, i.e. \(0<x_0^*<L\), \(0<R^*<L/2\). Using
\(k=0.3, a=0.1, D=1 ,c_0^-=0.1,c_0^+=0.9, L=5\) , we plot the steady
state radii and positions in figures \ref{fig:rstabledecay} and
\ref{fig:xstabledecay}.


\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics{source/figures/pdf/Rstable.pdf}
        \caption{Steady state radius as a function of \(v\) and \(J_{in}\) made
using \(k=0.3, a=0.1, D=1 ,c_0^-=0.1,c_0^+=0.9, L=5\). Blue areas
correspond to no droplet. Note that the radius increases with increasing $v$.\label{fig:rstabledecay}}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics{source/figures/pdf/Xstable.pdf}
        \caption{Steady state position as a function of \(v\) and \(J_{in}\)
made using \(k=0.3, a=0.1, D=1 ,c_0^-=0.1,c_0^+=0.9, L=5\). Blue areas
correspond to no droplet. Observe that the droplet moves closer to the left of the system for increased $v$.\label{fig:xstabledecay}}
    \end{subfigure}
    \caption{}
\end{figure}

In areas which are blue in both plots no droplet exists in the system.
We identify two causes, each connected to a corresponding `cutoff line'
in the stable position plot. First, by adding decay, we have added
another `exit' for the contents of the system. Thus, for low \(J_{in}\)
and \(v\) a droplet will not exist. This explains the lower left cutoff and
is supported by the fact that this edge corresponds to the line \(R=0\).
The other edge has \(x_0=0\), meaning that the droplet moved past the
edge of the system. In the radius plot we also observe a third cutoff in
the upper left corner. This corresponds to the \(x_0=5\) edge and
represents a droplet at the far end of the system. To satisfy the
no-flux boundary condition, the droplets' radius must go to zero. Hence
this area is shaded blue in the radius plot, but not in the position
plot.

Note that advection increases the droplet radius, contrary to the
no-decay case. Recall that advection decreased the radius because the
outside fluxes were constant. Having added decay to the system, this not
the case anymore. For a droplet at a fixed point \(x_0\), increasing
\(v\) increases the outside flux as less is lost to decay. Although
increasing \(v\) also increases the maturation flux inside the droplet,
the increase of the outside flux is dominant and hence the droplet
radius increases with increasing \(v\). Also observe in figure
\ref{fig:xstabledecay} that increasing \(v\) decreases \(x_0\).
Increasing the flow thus leads to the droplet moving further up that
flow, a very counterintuitive situation. To see why this happens,
consider a droplet of fixed radius \(R\) at position \(x_0\).
Increasing \(v\) increases \(\bar{c}\), which according to equation\ref{eq:jmathandwavy} can only be compensated by a higher influx. In a system with decay, the influx will be higher upstream and hence the droplet moves upstream.

We study the stability of these steady states by plotting \(dx_0/dt\)
and \(dR/dt\) at \(J_{in} = 0.18\) and \(v=0.1\) in figures
\ref{fig:drdtdecay} and \ref{fig:dxdtdecay}.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics{source/figures/pdf/dRdt.pdf}
        \caption{\(dR/dt\) as a function of \(x_0\) and \(R\). The solid black
line denotes the \(dR/dt=0\) and the dashed line \(dx_0/dt=0\). The red
line is the line \(x_0=R\).\label{fig:drdtdecay}}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics{source/figures/pdf/dXdt.pdf}
        \caption{\(dx_0/dt\) as a function of \(x_0\) and \(R\). The solid black
line denotes the \(dR/dt=0\) and the dashed line \(dx_0/dt=0\). The red
line is the line \(x_0=R\).\label{fig:dxdtdecay}}
    \end{subfigure}
    \caption{}
\end{figure}


The solid black lines denote \(dR/dt=0\) and the dashed lines
\(dx_0/dt=0\). The red line is the line \(x_0=R\); a steady state needs
to be above this line, as below this line \(x_0-R<0\), meaning that the
droplets' left interface is outside of the system. We observe that the
line \(dR/dt=0\) is stable, but as \(dx_0/dt=0\) is not, the steady
state is unstable. This plot is typical for all parameters, so we
conclude that all steady states are unstable: the droplet either moves
left or right until it hits the edges of the system. The free droplet
model does not properly describe this situation, as it always has two
interfaces. In reality, when the droplet hits the edges of the system
one of the two interfaces disappears and the droplet becomes like a
wetting layer. We investigate this in the next section.

\hypertarget{droplet-stuck-to-walls}{%
\section{Droplet stuck to walls}\label{droplet-stuck-to-walls}}

In the previous section we showed that a droplet will always move left
or right until it hits the edges of the system, but that this is not properly described by our free droplet model. In this section we
present a slightly modified model to account for this situation. Once the
droplets' interface hits the edge of the system, it ceases to be an
interface between a dense and a dilute phase: rather, the boundary
condition of the droplet must become the boundary condition of the
system. For a droplet on the left of the system we thus have the
boundary conditions \(c(R)=c_0^+\) and \(J(0)=J_{in}\), while for the
droplet on the left we have \(c(L-R)=c_0^+\) and \(J(L)=0\). We present
the behaviour of this modified model in this section, taking a slightly
wider view than before. Instead of assuming the existence of a droplet,
we first investigate when droplets phase separate at the edges of the
system. We will then prove the existence of a stable effective droplet
when such a phase separation should take place and that mass is
conserved. Finally, we present a phase diagram and discuss the
biological connection and implications.

\hypertarget{occurence-of-phase-separation}{%
\subsection{Occurence of phase
separation}\label{occurence-of-phase-separation}}

Consider again the double well free energy with minima at \(c_0^-\) and
\(c_0^+\): \begin{equation}
f(c) = \frac{b}{2\Delta c^2}(c-c_0^-)^2(c-c_0^+)^2
\label{eq:energydensity}\end{equation} This free energy describes a
system phase separating into a dense area with concentration \(c_0^+\)
and a dilute area of concentration \(c_0^-\). Phase separation is usually approached by the quenching of a mixed state, but we have a different system. Specifically, we have an open system which initially does not contain a droplet and experiences an influx from one side. Before a droplet is formed, the entire system is thus in the dilute well of the free energy. Hence we ask at
 at which concentration this initial configuration becomes unstable and a droplet is formed.
 
Thermodynamics show that stability requires \(d^2f/dc^2>0\). In areas where \(d^2f/dc^2<0\), fluctuations keep growing and the system will phase separate. For the free energy density
given by \ref{eq:energydensity}, the area between the two inflection
points (see the vertical lines in figure \ref{fig:freeenergy}) \(c=(c_0^++c_0^-)/2\pm\sqrt{3}(c_0^+-c_0^-)\) is unstable. Thus,
if the concentration in the dilute phase reaches the lower inflection point, a droplet will form. On the other hand, if the concentration in the dense phase becomes lower than upper inflection point, a dilute phase will form.  


To study when this minimum concentration is reached, we consider our
system without a droplet which is thus described by a single
advection-diffusion-decay equation with boundary conditions
\(J(0)=J_{in}\) and \(J(L)=0\). Resulting concentration profiles will
have the highest concentrations at the edges of the system and we thus calculate at
which \(J_{in}\) the concentration reaches the lower inflection point at $x=0$ and $x=L$.
We obtain: 
\begin{equation}
\text{Left:  }J_{in} = - \frac{ 2al((1-\frac{1}{\sqrt{3}}c_0^+)+(1+\frac{1}{\sqrt{3}}c_0^-))}{\frac{vl}{D}-coth(\frac{L}{2l})}
\label{eq:jminleft}
\end{equation}

\begin{equation}
\text{Right: } J_{in} =al((1-\frac{1}{\sqrt{3}}c_0^+)+(1+\frac{1}{\sqrt{3}}c_0^-))\left(e^{-\frac{L}{2l}(\frac{vl}{D}-1)}-e^{-\frac{L}{2l}(\frac{vl}{D}+1)}\right),
\label{eq:jminright}
\end{equation}

where \(l\) is a lengthscale defined as \(l=D/\sqrt{4aD+v^2}\). We plot equations \ref{eq:jminleft} and \ref{eq:jminright} in figure \ref{fig:minJconc} for \(a=0.1, D=1 ,c_0^-=0.1,c_0^+=0.9, L=5\)

\begin{figure}
\hypertarget{fig:minJconc}{%
\centering
\includegraphics[width=0.7\textwidth]{source/figures/pdf/Jmin.pdf}
\caption{The minimum required influx for a given advection speed to form a droplet on the left (blue line) and right (orange line). Plotting parameters are \(a=0.1, D=1 ,c_0^-=0.1,c_0^+=0.9, L=5\).}\label{fig:minJconc}
}
\end{figure}

The blue line shows the minimum \(J_{in}\) for the left side of the
system, while the orange line shows the minimum for the right. We can
recognize four areas in figure \ref{fig:minJconc}. Below both the
blue and the orange line, the concentration never reaches the lower
inflection point and thus no droplets will be formed. In the area below
the orange line but above the blue line, only a droplet on the left is
formed, while exactly the reverse happens on the right side of the plot:
only a droplet on the right is formed. Finally, we note that in the
upper area both droplets can be formed. In this regime, \(J_{in}\) and
\(v\) are high enough for the concentration to reach the inflection
point at both sides of the system.

Note however this analysis is purely based on the free energy density function; we have neglected the $\nabla^2 c$ in the total free energy which penalises gradients in the concentration. Although we do not quantify the effect of including this term, we do observe that penalising gradients significantly raises the free energy of a system without a droplet. Consequently, forming a droplet will be energetically more favourable at lower concentrations, meaning equations \ref{eq:jminleft} and \ref{eq:jminright} are an upper bound for when droplets are formed. Also note that for the validity of our approach, we require a stable droplet in the effective droplet model to exist at and above the influxes in equations \ref{eq:jminleft} and \ref{eq:jminright}. We investigate this is in the next section.


\hypertarget{effective-droplet-2}{%
\subsection{Effective droplet}\label{effective-droplet-2}}

We construct the effective droplet phase diagram for this system by
determining for which \(J^{*}_{in}\) a stable droplet (dR/dt=0) with
radius \(R=0\) exists. For an influx higher than \(J_{in}^*\), a droplet
with \(R>0\) then exists, so that \(J_{in}^*\) is the minimum influx
required for a droplet to exist. For the left and right droplet, we find
the following: \begin{equation}
\text{Left:  }J_{in} = - \frac{ 2a c_0^-l}{\frac{vl}{D}-coth(\frac{L}{2l})}
\label{eq:jminlefted}\end{equation}

\begin{equation}
\text{Right: } J_{in} =ac_0^-l\left(e^{-\frac{L}{2l}(\frac{vl}{D}-1)}-e^{-\frac{L}{2l}(\frac{vl}{D}+1)}\right)
\label{eq:jminrighted}\end{equation}

Note that these equations have the same form as \ref{eq:jminleft}
and \ref{eq:jminright}, save for some prefactor. Defining the
minimum flux as defined by equations \ref{eq:jminleft} and
\ref{eq:jminright} as \(J^{AD}\) and the minimum flux as calculated
by the effective droplet model in equations \ref{eq:jminlefted} and
\ref{eq:jminrighted} as \(J^{ED}\) we obtain the same ratio for both the left and right sides: 
\[
\frac{J^{AD}}{J^{ED}} = \frac{(3-\sqrt{3})c_0^++(3+\sqrt{3})c_0^-}{6c_0^-}
\] 
Note that \(J^{AD}>J^{ED}\) if \(c_0^+>c_0^-\). In other words, the
minimum flux required for a stable droplet is smaller than the minimum
flux required to form a droplet if the concentration in the dense phase
is higher than the concentration in the dilute phase, which it is by
definition. We thus see that a stable droplet with a non-zero radius is
guaranteed to exist if phase separation occurs as determined by
equations \ref{eq:jminleft} and \ref{eq:jminright}. 

A second criterium would be mass conservation: the mass in a separated system
should be similar to the mass in a phase separated system. The mass in the our system without a droplet is:
 \[
\int_0^L c(x)dx = \frac{J_{in}}{a}
\] 
We assume that \(J_{in}\) and \(a\) are such the droplet appears on the left side of the system so that the total mass inside the system is then: \[
\int_0^{R^*}c_{in}(x)dx + \int_{R^*}^Lc_{out}(x)dx
\] 
Where the stable droplet radius \(R^*\) corresponds to
\(dR/dt|_{R=R^*}=0\). Assuming the droplet radius remains small, we can
determine the stable droplet radius and thus perform the integral. For simplification, we assume that $k=a$ and we find: 
\[
\int_0^{R^*}c_{in}(x)dx + \int_{R^*}^Lc_{out}(x)dx=\frac{J_{in}}{a}
\] 
We thus see that mass is conserved when phase separation occurs.

The effective droplet phase diagram predicts an area in which
droplets on both the left and right are stable. We now investigate if
both droplets can coexist, i.e. that the system has a droplet on the
left and the right, using a two droplet model. Defining the radius of the left droplet as $R_1$ and the right droplet as $R_2$, solving the fluxes
for \(dR_1/dt=dR_2/dt=R_1=R_2=0\) yields a minimum influx \(J_{in}\) \[
J_{in} = \frac{c_0^-D}{2l}\left(\frac{vl}{D}+\frac{(1+e^{\frac{L}{l}}-2e^{\frac{-L}{2l}(\frac{vl}{D}-1)})}{(1-e^{\frac{L}{l}})}\right)
\] and if \(l_D,l_v\gg L\) we also obtain a minimum advection velocity
\(v^*\): \begin{equation}
v^*=\frac{aL}{2}
\label{eq:minadv}\end{equation} We plot the corresponding phase diagram
in figure \ref{fig:phasediagramapprox} .

\begin{figure}
\hypertarget{fig:phasediagramapprox}{%
\centering
\includegraphics[width=0.8\textwidth]{source/figures/pdf/phaseapprox.pdf}
\caption{Analytically derived phase diagram of droplets on the edges of the system.}\label{fig:phasediagramapprox}
}
\end{figure}

We observe six configurations and that all these cross at a single point. Note
some sort of symmetry exists: considering the area above the diagonal of
the plot and following it clockwise from the origin, we first observe no
droplets, then a droplet on the left, followed by either a droplet on
the left or right and finally the area where its possible for a single
droplet on the left or right or two droplets simultaneously to exist. Below the
diagonal we observe an analogous trajectory, where instead of a single
droplet on the left we now have a droplet on the right. Considering the
problem concentration profile without the droplet we presented in the previous section this symmetry makes
perfect sense: for low \(v/J_{in}\) the concentration will be highest on
the left hence droplets will be formed on the left. For high
\(v/J_{in}\) exactly the opposite happens and droplets on the right are
favoured. For high enough $v$ and $J_{in}$, droplets on both sides can be formed.

Observe that all phases intersect at a single point. Expanding the crossing
of the minimal fluxes for \(l_D,l_v\gg L\) we obtain \[
v_{cross}=\frac{aL}{2}
\] which is similar to equations \ref{eq:minadv}. This means that the
intersection at this point is a system property and not just a
side-effect of our parameter choice. In figure
\ref{fig:phasediagramnumerical} we plot the results of numerically
solving the equations.

\begin{figure}
\hypertarget{fig:phasediagramnumerical}{%
\centering
\includegraphics[width=0.8\textwidth]{source/figures/pdf/Numericalphase.pdf}
\caption{Numerically determined phase diagram of the droplets in the system. The red solid and dashed lines represent the minimum influx required for
respectively a droplet on the left and right. The black dashed line
corresponds to \(dR_1/dt=0\) in the two droplet model, while the
dotted-dashed line is \(dR_2/dt=0\).}
\label{fig:phasediagramnumerical}
}
\end{figure}

The red solid and dashed lines represent the minimum influx required for
respectively a droplet on the left and right. The black dashed line
corresponds to \(dR_1/dt=0\) in the two droplet model, while the
dotted-dashed line is \(dR_2/dt=0\). Contrary to equation \ref{eq:minadv}, this is not perfectly vertical. This is because \ref{eq:minadv} has been derived assuming that both the left and
right droplet appear at the same time; in reality, one of the droplet
can have a non-zero radius before a second droplet appears. Numerically
we also find that all the configurations are connected through some critical
point. To see why this happens, consider the crossing of the two minimum influxes as determined by equations \ref{eq:jminleft} and \ref{eq:jminright}. At this point, the concentration profile is such that the concentration on each side of the system is similar: \(dR/dt=R=0\) on
both sides. By changing \(J_{in}\) or \(v\) any of the configurations can be
reached from this point and hence all the lines cross at a single point.

\hypertarget{biological-implications}{%
\section{Biological implications}\label{biological-implications}}

We now compare the behaviour of our model to biological observations. First
off, our model predicts that the only stable position for the droplet is
at the edges of the system. Biologically, stacks are either located at
the ribbon or at the ERES, thus matching our model. Furthermore, this
position is dependent on the state of the microtubules: when the microtubules are depolymerised, the ribbon breaks up and the stacks co-localise with the
ERES. Our free droplet models predicts similar behaviour, with the
switching happening at a finite advection velocity. The model in which the droplets are stuck to the edges of the system also shows such an advection-dependent transition, but also shows the possibility of having droplets at both the ERES and the Ribbon; something which is not observed in real cells. Another small but not unimportant detail is that the 
droplet size in our model is dependent on the amount of trafficking. Similar observations have been made for the Golgi\textsuperscript{50}.

We now make a speculative connection to the internal maturation of the Golgi.  We have shown that an active droplet is able to move itself up a concentration gradient. This movement is generated by imbalanced fluxes: the droplet grows on one side, but shrinks on the other, akin to tread-milling. In the cisternal maturation model, the Golgi grows on one side by vesicles forming a cis compartment, whereas the opposite happens on the trans side. This similar to how an active droplet moves and cisternal maturation could thus be the process by which the stack moves from its position in the ribbon to the ERES.



\hypertarget{conclusion-1}{%
\chapter{Conclusion}\label{conclusion-1}}

In this thesis we have investigated the connection between the intracellular transport and the Golgi apparatus. In the first part of this thesis we analysed the experimental data generated by the RUSH technique and investigated if intracellular transport could be explained by an advection-diffusion model. To do so, we developed a new technique based on image gradients and evaluated a second method based on neural networks - Physics Informed Neural Networks. Unfortunately, neither of the two could give a definite answer. Our home-made method returned diffusion and advection fields which showed a non-random pattern, but did not betray any coherent underlying structure. The PINN seemed to outperform our image gradient method, but led to a clearly incorrect answer when allowed to infer a varying coefficient field. We attribute the failure of both methods mostly to the quality and quantity of the data and thus present several recommendations to experimentalists. 

To properly quantify fluorescence microscopy data, the entire setup should be rethought with this in mind. As such, above all a choice of quantification method must be made. Assuming they wish to use one of the methods we have presented, a calibration should be performed to learn the mapping of the fluorescence intensity to the concentration. Taking bright field images should also be considered, as these can be used for segmentation. Finally, the image acquisition procedure can be improved by ensuring a steady frame-rate and increasing it. Although increasing the frame rate increases photo
bleaching, the last 200 frames of the roughly 300 frame movie of the ManII trafficking contain almost no information about the intracellular transport. By focusing the image acquisition time, more useful data can be gathered.

We also foresee several directions to improve our image-gradient method. The
most gains can be made in the step where the derivatives are calculated.
The Sobel filter we have applied is an extremely simple approximation and improving this step would vastly improve results. The fitting method we have applied - least squares - , is also rather basic and known to be sensitive to outliers and noise. Since the image gradients turn the data into a generic set of features, any fitting procedure could be used. In selecting a different fitting procedure, care should be taken to select a method which can properly handle noisy data with outliers.

We have proven that it is possible to infer coefficient
fields with physics informed neural networks. Despite the failure of PINNs applied to the RUSH data, we believe PINNs show great promise and we propose several avenues to improve their performance. Implementing Bayesian neural networks would yield a distribution instead of a single value as an output. We believe this to be of prime importance, as the results of PINNs are not robust yet. Secondly, fitting using coefficient fields is a very hard problem in which it is likely the network will get stuck in incorrect local minima. To remedy this, we propose a `boots-trapping' technique,
in which one would first assume the coefficients to be locally constant,
similar to the sliding window technique of the image-gradient method.
Using this as a starting point, in the second step the fully spatially
varying expressions are then used to improve the first step estimate. 

In the second part of the thesis we have investigated if the Golgi could be described as a phase separated droplet in a diffusive-advective environment. In the process, we also studied the effect of advection on active droplets and found that it compacts them. We showed that droplets can indeed phase separate in such an environment and switch position between the edges of the system depending on the magnitude of advection. Active droplets move by growing on one side and shrinking on the other and we thus speculate that cisternal maturation can actually be responsible for moving the stacks from their location in the ribbon to an ERES when the microtubules are depolymerised. However, our model also showed the possibility for two droplets to exist simultaneously in the ribbon and at the ERES - a situation not observed in cells. We also note that our model allowed several stable configurations to exist at a given parameter set. More work is required to investigate if any of these states are preferred over one another, probably through simulations or numerically solving the complete Cahn-Hilliard equation. We have also modelled the Golgi as a droplet of immature cargo, including a conversion term but otherwise completely neglecting mature cargo. Future work should incorporate this by for example switching to a two-component model. Furthermore, we note that dynamics could be added to the model, as the advection-diffusion equation has a time dependent solution through Greens' functions. 

\section{Afterword}

When people ask me why I did my masters project in Paris at Institut Curie I always answer it's sort of an escalated joke. When Remy told me he was going to Institut Curie for his post-doc we discussed the research done there and it seemed really interesting, so when he jokingly asked me why I wouldn't do my masters project there I thought, 'why not?' and a few weeks later I was boarding a TGV to Paris to discuss possible projects with Pierre. Now, having lived in Paris for nearly a year, I haven't regretted the decision one second. Well, except for when I'm filling in another form for HR - I seem to have gotten the full 'French treatment'. So Remy, I guess I owe you a thanks for that joke. We'll get to that PhD later.  

The scientific world is a small place. Having decided on a project with Pierre as my supervisor, I thought Kees would make for a good supervisor in the Netherlands and upon telling him about my project he responded that he knew Pierre from back in the day. Pierre and Kees, thanks for having given me the freedom to follow what I found interesting (hence the chapter on neural networks), but also asking the right questions to make me realise that most of my crazy ideas wouldn't work. Your advice and guidance has kept this thing on track. 

I'll keep working with Pierre and Remy as I'm starting my PhD here in Paris in January. We'll be working on applying physics informed neural networks to biophysical problems, specifically the actin cortex. The last year I've found out that I really enjoy working on the interface of physics, biology and computer science, so that's exactly what I'm going to do the next three years. 

My PhD is not going to be at Curie, meaning that I'm not going to be seeing the people of UMR168 everyday anymore. And that's making me a little bit sad, since I can't put into words how much fun the last year has been. Allison, Efe, Amit, Jean-Patrick, Anne-Marie, Thibaut, Michael, Joanna, Julienne, Maj (yes, even you), and whoever else I've forgotten (sorry about that), thanks. 

Despite the fun I've had and all the different things I've learned (who knew biology could be so complex?), it wasn't always easy. About six weeks into my project a man whom I might best describe as a second father to me unexpectedly passed away. He always pushed me to not bitch and work harder, telling me how he'd been stupid and failed. Always reminding me he'd be there at my graduation. But he won't, and so I dedicate this to him. 

\emph{Jan, deze is voor jou.}


\footnotesize

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-emr_journeys_2009}{}%
1. Emr, S. \emph{et al.} Journeys through the Golgi---taking stock in a
new era. \emph{The Journal of Cell Biology} \textbf{187,} 449--453
(2009).

\leavevmode\hypertarget{ref-tang_cell_2013}{}%
2. Tang, D. \& Wang, Y. Cell cycle regulation of Golgi membrane
dynamics. \emph{Trends in Cell Biology} \textbf{23,} 296--304 (2013).

\leavevmode\hypertarget{ref-rothman_future_2010}{}%
3. Rothman, J. E. The Future of Golgi Research. \emph{Molecular Biology
of the Cell} \textbf{21,} 3776--3780 (2010).

\leavevmode\hypertarget{ref-gosavi_function_2017}{}%
4. Gosavi, P. \& Gleeson, P. A. The Function of the Golgi Ribbon
Structure - An Enduring Mystery Unfolds! \emph{BioEssays} \textbf{39,}
1700063 (2017).

\leavevmode\hypertarget{ref-budnik_er_2009}{}%
5. Budnik, A. \& Stephens, D. J. ER exit sites - Localization and
control of COPII vesicle formation. \emph{FEBS Letters} \textbf{583,}
3796--3803 (2009).

\leavevmode\hypertarget{ref-bressloff_stochastic_2013}{}%
6. Bressloff, P. C. \& Newby, J. M. Stochastic models of intracellular
transport. \emph{Reviews of Modern Physics} \textbf{85,} 135--196
(2013).

\leavevmode\hypertarget{ref-wei_unraveling_2010}{}%
7. Wei, J.-H. \& Seemann, J. Unraveling the Golgi Ribbon. \emph{Traffic}
\textbf{11,} 1391--1400 (2010).

\leavevmode\hypertarget{ref-ronchi_positive_2014}{}%
8. Ronchi, P., Tischer, C., Acehan, D. \& Pepperkok, R. Positive
feedback between Golgi membranes, microtubules and ER exit sites directs
de novo biogenesis of the Golgi. \emph{Journal of Cell Science}
\textbf{127,} 4620--4633 (2014).

\leavevmode\hypertarget{ref-newby_quasi-steady_2010}{}%
9. Newby, J. M. \& Bressloff, P. C. Quasi-steady State Reduction of
Molecular Motor-Based Models of Directed Intermittent Search.
\emph{Bulletin of Mathematical Biology} \textbf{72,} 1840--1866 (2010).

\leavevmode\hypertarget{ref-newby_random_2010}{}%
10. Newby, J. \& Bressloff, P. C. Random intermittent search and the
tug-of-war model of motor-driven transport. \emph{Journal of Statistical
Mechanics: Theory and Experiment} \textbf{2010,} P04014 (2010).

\leavevmode\hypertarget{ref-alberts_molecular_nodate}{}%
11. Alberts. \emph{Molecular biology of the Cell}.

\leavevmode\hypertarget{ref-staehelin_nanoscale_2008}{}%
12. Staehelin, L. A. \& Kang, B.-H. Nanoscale Architecture of
Endoplasmic Reticulum Export Sites and of Golgi Membranes as Determined
by Electron Tomography. \emph{PLANT PHYSIOLOGY} \textbf{147,} 1454--1468
(2008).

\leavevmode\hypertarget{ref-griffiths_rubisco_nodate}{}%
13. Griffiths, H. Rubisco is said to be both the most important enzyme
on Earth and surprisingly inefficient. Yet an understanding of the
reaction by which it fixes CO2 suggests that evolution has made the best
of a bad job. 2

\leavevmode\hypertarget{ref-glick_models_2011}{}%
14. Glick, B. S. \& Luini, A. Models for Golgi Traffic: A Critical
Assessment. \emph{Cold Spring Harbor Perspectives in Biology}
\textbf{3,} a005215--a005215 (2011).

\leavevmode\hypertarget{ref-hirschberg_kinetic_1998}{}%
15. Hirschberg, K. \emph{et al.} Kinetic Analysis of Secretory Protein
Traffic and Characterization of Golgi to Plasma Membrane Transport
Intermediates in Living Cells. \emph{The Journal of Cell Biology}
\textbf{143,} 1485--1503 (1998).

\leavevmode\hypertarget{ref-boncompain_synchronization_2012}{}%
16. Boncompain, G. \emph{et al.} Synchronization of secretory protein
traffic in populations of cells. \emph{Nature Methods} \textbf{9,}
493--498 (2012).

\leavevmode\hypertarget{ref-holcman_modeling_2007}{}%
17. Holcman, D. Modeling DNA and Virus Trafficking in the Cell
Cytoplasm. \emph{Journal of Statistical Physics} \textbf{127,} 471--494
(2007).

\leavevmode\hypertarget{ref-lagache_effective_2008}{}%
18. Lagache, T. \& Holcman, D. Effective Motion of a Virus Trafficking
Inside a Biological Cell. \emph{SIAM Journal on Applied Mathematics}
\textbf{68,} 1146--1167 (2008).

\leavevmode\hypertarget{ref-dinh_model_2005}{}%
19. Dinh, A.-T., Theofanous, T. \& Mitragotri, S. A Model for
Intracellular Trafficking of Adenoviral Vectors. \emph{Biophysical
Journal} \textbf{89,} 1574--1588 (2005).

\leavevmode\hypertarget{ref-brandenburg_virus_2007}{}%
20. Brandenburg, B. \& Zhuang, X. Virus trafficking -- learning from
single-virus tracking. \emph{Nature Reviews Microbiology} \textbf{5,}
197--208 (2007).

\leavevmode\hypertarget{ref-zwicker_growth_2017}{}%
21. Zwicker, D., Seyboldt, R., Weber, C. A., Hyman, A. A. \& Jülicher,
F. Growth and division of active droplets provides a model for
protocells. \emph{Nature Physics} \textbf{13,} 408--413 (2017).

\leavevmode\hypertarget{ref-hyman_liquid-liquid_2014}{}%
22. Hyman, A. A., Weber, C. A. \& Jülicher, F. Liquid-Liquid Phase
Separation in Biology. \emph{Annual Review of Cell and Developmental
Biology} \textbf{30,} 39--58 (2014).

\leavevmode\hypertarget{ref-zwicker_centrosomes_2014}{}%
23. Zwicker, D., Decker, M., Jaensch, S., Hyman, A. A. \& Jülicher, F.
Centrosomes are autocatalytic droplets of pericentriolar material
organized by centrioles. \emph{Proceedings of the National Academy of
Sciences} \textbf{111,} E2636--E2645 (2014).

\leavevmode\hypertarget{ref-de_vos_seeing_2016}{}%
24. Sbalzarini, I. F. Seeing Is Believing: Quantifying Is Convincing:
Computational Image Analysis in Biology. in \emph{Focus on Bio-Image
Informatics} (eds. De Vos, W. H., Munck, S. \& Timmermans, J.-P.)
\textbf{219,} 1--39 (Springer International Publishing, 2016).

\leavevmode\hypertarget{ref-hutchison_computational_2014}{}%
25. Chen, K.-C., Qiu, M., Kovacevic, J. \& Yang, G. Computational Image
Modeling for Characterization and Analysis of Intracellular Cargo
Transport. in \emph{Computational Modeling of Objects Presented in
Images. Fundamentals, Methods, and Applications} (eds. Hutchison, D. et
al.) \textbf{8641,} 292--303 (Springer International Publishing, 2014).

\leavevmode\hypertarget{ref-lee_image-based_2015}{}%
26. Lee, H.-C. \& Yang, G. An image-based computational method for
characterizing whole-cell scale spatiotemporal dynamics of intracellular
transport. in \emph{2015 IEEE 12th International Symposium on Biomedical
Imaging (ISBI)} 699--702 (IEEE, 2015).
doi:\href{https://doi.org/10.1109/ISBI.2015.7163969}{10.1109/ISBI.2015.7163969}

\leavevmode\hypertarget{ref-yang_bioimage_2013}{}%
27. Yang, G. Bioimage informatics for understanding spatiotemporal
dynamics of cellular processes: Spatiotemporal dynamics of cellular
processes. \emph{Wiley Interdisciplinary Reviews: Systems Biology and
Medicine} \textbf{5,} 367--380 (2013).

\leavevmode\hypertarget{ref-hebert_spatiotemporal_2005}{}%
28. Hebert, B., Costantino, S. \& Wiseman, P. W. Spatiotemporal Image
Correlation Spectroscopy (STICS) Theory, Verification, and Application
to Protein Velocity Mapping in Living CHO Cells. \emph{Biophysical
Journal} \textbf{88,} 3601--3614 (2005).

\leavevmode\hypertarget{ref-kisley_characterization_2015}{}%
29. Kisley, L. \emph{et al.} Characterization of Porous Materials by
Fluorescence Correlation Spectroscopy Super-resolution Optical
Fluctuation Imaging. \emph{ACS Nano} \textbf{9,} 9158--9166 (2015).

\leavevmode\hypertarget{ref-semrau_particle_2007}{}%
30. Semrau, S. \& Schmidt, T. Particle Image Correlation Spectroscopy
(PICS): Retrieving Nanometer-Scale Correlations from High-Density
Single-Molecule Position Data. \emph{Biophysical Journal} \textbf{92,}
613--621 (2007).

\leavevmode\hypertarget{ref-raissi_physics_2017}{}%
31. Raissi, M., Perdikaris, P. \& Karniadakis, G. E. Physics Informed
Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial
Differential Equations. \emph{arXiv:1711.10566 {[}cs, math, stat{]}}
(2017).

\leavevmode\hypertarget{ref-barron_performance_1994}{}%
32. Barron, J. L., Fleet, D. J. \& Beauchemin, S. S. Performance of
Optical Flow Techniques. 60 (1994).

\leavevmode\hypertarget{ref-dong_motion_2006}{}%
33. Dong, G., Baskin, T. I. \& Palaniappan, K. Motion Flow Estimation
from Image Sequences with Applications to Biological Growth and
Motility. in \emph{2006 International Conference on Image Processing}
1245--1248 (IEEE, 2006).
doi:\href{https://doi.org/10.1109/ICIP.2006.312551}{10.1109/ICIP.2006.312551}

\leavevmode\hypertarget{ref-vig_quantification_2016}{}%
34. Vig, D. K., Hamby, A. E. \& Wolgemuth, C. W. On the Quantification
of Cellular Velocity Fields. \emph{Biophysical Journal} \textbf{110,}
1469--1475 (2016).

\leavevmode\hypertarget{ref-garcia_robust_2010}{}%
35. Garcia, D. Robust smoothing of gridded data in one and higher
dimensions with missing values. \emph{Computational Statistics \& Data
Analysis} \textbf{54,} 1167--1178 (2010).

\leavevmode\hypertarget{ref-zimon_novel_2016}{}%
36. Zimoń, M., Reese, J. \& Emerson, D. A novel coupling of noise
reduction algorithms for particle flow simulations. \emph{Journal of
Computational Physics} \textbf{321,} 169--190 (2016).

\leavevmode\hypertarget{ref-zimon_evaluation_2016}{}%
37. Zimoń, M. \emph{et al.} An evaluation of noise reduction algorithms
for particle-based fluid simulations in multi-scale applications.
\emph{Journal of Computational Physics} \textbf{325,} 380--394 (2016).

\leavevmode\hypertarget{ref-grinberg_analyzing_2009}{}%
38. Grinberg, L., Yakhot, A. \& Karniadakis, G. E. Analyzing Transient
Turbulence in a Stenosed Carotid Artery by Proper Orthogonal
Decomposition. \emph{Annals of Biomedical Engineering} \textbf{37,}
2200--2217 (2009).

\leavevmode\hypertarget{ref-grinberg_proper_2012}{}%
39. Grinberg, L. Proper orthogonal decomposition of atomistic flow
simulations. \emph{Journal of Computational Physics} \textbf{231,}
5542--5556 (2012).

\leavevmode\hypertarget{ref-bruno_numerical_2012}{}%
40. Bruno, O. \& Hoch, D. Numerical Differentiation of Approximated
Functions with Limited Order-of-Accuracy Deterioration. \emph{SIAM
Journal on Numerical Analysis} \textbf{50,} 1581--1603 (2012).

\leavevmode\hypertarget{ref-knowles_methods_nodate}{}%
41. Knowles, I. \& Renka, R. J. METHODS FOR NUMERICAL DIFFERENTIATION OF
NOISY DATA. 12

\leavevmode\hypertarget{ref-rizk_segmentation_2014}{}%
42. Rizk, A. \emph{et al.} Segmentation and quantification of
subcellular structures in fluorescence microscopy images using Squassh.
\emph{Nature Protocols} \textbf{9,} 586--596 (2014).

\leavevmode\hypertarget{ref-karpatne_physics-guided_2017}{}%
43. Karpatne, A., Watkins, W., Read, J. \& Kumar, V. Physics-guided
Neural Networks (PGNN): An Application in Lake Temperature Modeling.
\emph{arXiv:1710.11431 {[}physics, stat{]}} (2017).

\leavevmode\hypertarget{ref-sharma_weakly-supervised_2018}{}%
44. Sharma, R., Farimani, A. B., Gomes, J., Eastman, P. \& Pande, V.
Weakly-Supervised Deep Learning of Heat Transport via Physics Informed
Loss. \emph{arXiv:1807.11374 {[}cs, stat{]}} (2018).

\leavevmode\hypertarget{ref-pun_physically-informed_2018}{}%
45. Pun, G. P. P., Batra, R., Ramprasad, R. \& Mishin, Y.
Physically-informed artificial neural networks for atomistic modeling of
materials. \emph{arXiv:1808.01696 {[}cond-mat{]}} (2018).

\leavevmode\hypertarget{ref-raissi_physics_2017-1}{}%
46. Raissi, M., Perdikaris, P. \& Karniadakis, G. E. Physics Informed
Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial
Differential Equations. \emph{arXiv:1711.10561 {[}cs, math, stat{]}}
(2017).

\leavevmode\hypertarget{ref-bray_theory_2002}{}%
47. Bray, A. J. Theory of phase-ordering kinetics. \emph{Advances in
Physics} \textbf{51,} 481--587 (2002).

\leavevmode\hypertarget{ref-weber_physics_2018}{}%
48. Weber, C. A., Zwicker, D., Jülicher, F. \& Lee, C. F. Physics of
Active Emulsions. \emph{arXiv:1806.09552 {[}cond-mat{]}} (2018).

\leavevmode\hypertarget{ref-zwicker_suppression_2015}{}%
49. Zwicker, D., Hyman, A. A. \& Jülicher, F. Suppression of Ostwald
ripening in active emulsions. \emph{Physical Review E} \textbf{92,}
(2015).

\leavevmode\hypertarget{ref-sengupta_control_2011}{}%
50. Sengupta, D. \& Linstedt, A. D. Control of Organelle Size: The Golgi
Complex. \emph{Annual Review of Cell and Developmental Biology}
\textbf{27,} 57--77 (2011).

\leavevmode\hypertarget{ref-weber_droplet_2017}{}%
51. Weber, C. A., Lee, C. F. \& Jülicher, F. Droplet ripening in
concentration gradients. \emph{New Journal of Physics} \textbf{19,}
053021 (2017).

\end{document}
